{"cells":[{"cell_type":"markdown","metadata":{"id":"X1rhtOAceoHn"},"source":["# T5-ParaDetox Pipeline with DecompX Reranking\n","\n","This notebook combines:\n","- **T5-base** fine-tuned on ParaDetox for detoxification\n","- **DecompX reranking** to select the least toxic candidate from multiple generations\n","\n","## Pipeline\n","\n","1. Generate `num_candidates` detoxified texts per input using T5 sampling\n","2. Score each candidate using DecompX toxicity attribution (RoBERTa-based)\n","3. Select candidate with lowest toxicity score\n","4. Evaluate with BLEU, BERTScore, MeaningBERT, Perplexity, Toxicity\n","\n","---\n","\n","## `detoxify()` API\n","\n","```python\n","def detoxify(\n","    data_type: str = \"paradetox\",\n","    output_folder: str = \"T5_w_DecompX-Reranking\",\n","    batch_size: int = 8,\n","    max_length: int = 128,\n","    num_examples: int = 100,\n","    num_candidates: int = 10,\n","    temperature: float = 1.0,\n","    top_k: int = 50,\n","    top_p: float = 0.95,\n","    overwrite_gen: bool = False,\n","    run_eval: bool = True,\n","    overwrite_eval: bool = False,\n","    echo: bool = False,\n",")\n","```\n","\n","### Key Arguments\n","\n","- `data_type`: Dataset key (paradetox, microagressions_test, sbf_test, dynabench_test, jigsaw_toxic, appdia_original, appdia_discourse)\n","- `output_folder`: Folder under `data/model_outputs/` for results\n","- `num_candidates`: Number of candidates to generate per input for reranking\n","- `temperature`: Sampling temperature for diversity (higher = more diverse)\n","- `echo`: If True, print example inputs, candidates, and outputs"]},{"cell_type":"markdown","metadata":{"id":"bYc9JrJaeoHp"},"source":["## Setup"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4W_N0aereoHp","executionInfo":{"status":"ok","timestamp":1764843405675,"user_tz":480,"elapsed":39736,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"a7cc48f6-d8d0-4302-a3c9-0d988cfe40fe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","PROJECT_BASE: /content/drive/MyDrive/w266 - Project\n","XDETOX_DIR: /content/drive/MyDrive/w266 - Project/XDetox -> True\n","T5_CHECKPOINT: /content/drive/MyDrive/w266 - Project/t5-base-detox-model\n","TRANSFORMERS_CACHE: /content/drive/MyDrive/w266 - Project/XDetox/cache\n","CUDA available: True\n","GPU: Tesla T4\n"]}],"source":["#@title Mount Drive, imports & locate XDetox\n","from google.colab import drive; drive.mount('/content/drive')\n","\n","import os, sys, torch\n","\n","# Base paths (adjust if needed)\n","PROJECT_BASE = \"/content/drive/MyDrive/w266 - Project\"\n","XDETOX_DIR   = os.path.join(PROJECT_BASE, \"XDetox\")\n","T5_CHECKPOINT = os.path.join(PROJECT_BASE, \"t5-base-detox-model\")\n","\n","print(\"PROJECT_BASE:\", PROJECT_BASE)\n","print(\"XDETOX_DIR:\", XDETOX_DIR, \"->\", os.path.isdir(XDETOX_DIR))\n","print(\"T5_CHECKPOINT:\", T5_CHECKPOINT)\n","\n","assert os.path.isdir(XDETOX_DIR), f\"XDETOX_DIR does not exist: {XDETOX_DIR}\"\n","\n","# Runtime setup (paths, cache, GPU)\n","HF_CACHE = os.path.join(XDETOX_DIR, \"cache\")\n","os.makedirs(HF_CACHE, exist_ok=True)\n","os.environ[\"TRANSFORMERS_CACHE\"] = HF_CACHE\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\n","if XDETOX_DIR not in sys.path:\n","    sys.path.append(XDETOX_DIR)\n","\n","print(\"TRANSFORMERS_CACHE:\", HF_CACHE)\n","print(\"CUDA available:\", torch.cuda.is_available())\n","if torch.cuda.is_available():\n","    print(\"GPU:\", torch.cuda.get_device_name(0))"]},{"cell_type":"code","source":["# Verify XDetox repo layout\n","for d in [\"rewrite\", \"evaluation\", \"datasets\", \"data\"]:\n","    assert os.path.isdir(os.path.join(XDETOX_DIR, d)), f\"Missing folder: {d}\"\n","print(\"Repo folders OK.\")\n","\n","REPO = XDETOX_DIR\n","DATASET_BASE = REPO"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L1kyTe1yzOgQ","executionInfo":{"status":"ok","timestamp":1764843405720,"user_tz":480,"elapsed":17,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"750c2e2f-63d0-44ed-fe87-72cfcca5a777"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Repo folders OK.\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QUMJhQb_eoHq","executionInfo":{"status":"ok","timestamp":1764843440847,"user_tz":480,"elapsed":35125,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"756442d3-c825-40b4-aaf2-c69c53627a0c"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.3.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["#@title Install dependencies (aligned with LLM DecompX pipeline)\n","!pip -q install --upgrade pip setuptools wheel\n","!pip -q install \"transformers==4.41.2\" \"tokenizers==0.19.1\" \\\n","                \"datasets==2.19.0\" \"evaluate==0.4.1\" \\\n","                \"sacrebleu==2.4.1\" sacremoses ftfy nltk matplotlib pandas jedi \\\n","                sentencepiece\n","!pip -q install bert-score"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"NGDUUoabeoHr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764843451323,"user_tz":480,"elapsed":10470,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"f75580f1-d2c3-48e5-c949-0834b80a156a"},"outputs":[{"output_type":"stream","name":"stdout","text":["NLTK ready\n"]}],"source":["#@title NLTK data\n","import nltk\n","nltk.download(\"punkt\", quiet=True)\n","try:\n","    nltk.download(\"punkt_tab\", quiet=True)\n","except Exception:\n","    pass\n","print(\"NLTK ready\")"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"784CUqYSeoHr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764843462361,"user_tz":480,"elapsed":11032,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"5f74d9a8-eb03-4a8a-a08d-d59b7928d5a1"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Libraries imported\n"]}],"source":["#@title Imports\n","import glob, re, json, shutil, math\n","import numpy as np\n","import pandas as pd\n","from tqdm.auto import tqdm\n","from pathlib import Path\n","from subprocess import run, PIPE\n","from typing import List, Tuple\n","\n","from transformers import (\n","    T5Tokenizer,\n","    T5ForConditionalGeneration,\n",")\n","# DecompX Masker (same alias as LLM pipeline)\n","from rewrite.mask_orig import Masker as Masker_single\n","\n","# DecompX compatibility fixes for newer Transformers versions\n","import transformers.modeling_utils as modeling_utils\n","\n","# 1) apply_chunking_to_forward moved to transformers.pytorch_utils\n","try:\n","    from transformers.modeling_utils import apply_chunking_to_forward\n","except ImportError:\n","    from transformers.pytorch_utils import apply_chunking_to_forward\n","    modeling_utils.apply_chunking_to_forward = apply_chunking_to_forward\n","\n","# 2) find_pruneable_heads_and_indices may move out of modeling_utils\n","try:\n","    from transformers.modeling_utils import find_pruneable_heads_and_indices\n","except ImportError:\n","    try:\n","        from transformers.models.bert.modeling_bert import find_pruneable_heads_and_indices\n","        modeling_utils.find_pruneable_heads_and_indices = find_pruneable_heads_and_indices\n","    except ImportError:\n","        def find_pruneable_heads_and_indices(*args, **kwargs):\n","            raise NotImplementedError(\"find_pruneable_heads_and_indices is not available\")\n","        modeling_utils.find_pruneable_heads_and_indices = find_pruneable_heads_and_indices\n","\n","# 3) prune_linear_layer may also move\n","try:\n","    from transformers.modeling_utils import prune_linear_layer\n","except ImportError:\n","    try:\n","        from transformers.models.bert.modeling_bert import prune_linear_layer\n","        modeling_utils.prune_linear_layer = prune_linear_layer\n","    except ImportError:\n","        def prune_linear_layer(*args, **kwargs):\n","            raise NotImplementedError(\"prune_linear_layer is not available\")\n","        modeling_utils.prune_linear_layer = prune_linear_layer\n","\n","print(\"Libraries imported\")"]},{"cell_type":"markdown","metadata":{"id":"mJ6WLuymeoHr"},"source":["## Dataset Configuration"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"I6TEERFXeoHr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764843462395,"user_tz":480,"elapsed":31,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"68abf346-76cb-4031-b3df-acb849b8baa4"},"outputs":[{"output_type":"stream","name":"stdout","text":["7 datasets configured: paradetox, microagressions_test, sbf_test, dynabench_test, jigsaw_toxic, appdia_original, appdia_discourse\n"]}],"source":["#@title Data configs (matching XDetox datasets)\n","data_configs = {\n","    \"paradetox\": {\n","        \"data_path\": \"./datasets/paradetox/test_toxic_parallel.txt\",\n","        \"format\": \"txt\",\n","    },\n","    \"microagressions_test\": {\n","        \"data_path\": \"./datasets/microagressions/test.csv\",\n","        \"format\": \"csv\",\n","    },\n","    \"sbf_test\": {\n","        \"data_path\": \"./datasets/sbf/sbftst.csv\",\n","        \"format\": \"csv\",\n","    },\n","    \"dynabench_test\": {\n","        \"data_path\": \"./datasets/dynabench/db_test.csv\",\n","        \"format\": \"csv\",\n","    },\n","    \"jigsaw_toxic\": {\n","        \"data_path\": \"./datasets/jigsaw_full_30/test_10k_toxic.txt\",\n","        \"format\": \"txt\",\n","    },\n","    \"appdia_original\": {\n","        \"data_path\": \"./datasets/appdia/original-annotated-data/original-test.tsv\",\n","        \"format\": \"tsv\",\n","    },\n","    \"appdia_discourse\": {\n","        \"data_path\": \"./datasets/appdia/discourse-augmented-data/discourse-test.tsv\",\n","        \"format\": \"tsv\",\n","    },\n","}\n","print(f\"{len(data_configs)} datasets configured:\", \", \".join(data_configs.keys()))"]},{"cell_type":"markdown","metadata":{"id":"fExrd6_JeoHs"},"source":["## Helper Functions"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"qNg4xbyHeoHs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764843462396,"user_tz":480,"elapsed":13,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"ff4e14d9-5423-4df0-c0fb-93fe7124e3cc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Helper functions loaded\n"]}],"source":["#@title Helper functions\n","\n","def _ensure_dir(p: str):\n","    Path(p).mkdir(parents=True, exist_ok=True)\n","\n","def load_test_data(data_type: str, num_examples: int = None) -> List[str]:\n","    \"\"\"\n","    Load test data from .txt / .csv / .tsv.\n","    Returns a list of toxic texts as strings.\n","    \"\"\"\n","    if data_type not in data_configs:\n","        raise ValueError(f\"Unknown data_type: {data_type}\")\n","\n","    cfg = data_configs[data_type]\n","    data_path = os.path.join(DATASET_BASE, cfg[\"data_path\"].lstrip(\"./\"))\n","\n","    texts = []\n","\n","    if cfg[\"format\"] == \"txt\":\n","        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n","            texts = [line.strip() for line in f if line.strip()]\n","\n","    elif cfg[\"format\"] == \"csv\":\n","        df = pd.read_csv(data_path)\n","        if \"text\" in df.columns:\n","            texts = df[\"text\"].tolist()\n","        elif \"toxic\" in df.columns:\n","            texts = df[\"toxic\"].tolist()\n","        else:\n","            texts = df.iloc[:, 0].tolist()\n","\n","    elif cfg[\"format\"] == \"tsv\":\n","        df = pd.read_csv(data_path, sep=\"\\t\")\n","        if \"text\" in df.columns:\n","            texts = df[\"text\"].tolist()\n","        else:\n","            texts = df.iloc[:, 0].tolist()\n","\n","    cleaned = []\n","    for t in texts:\n","        if pd.isna(t):\n","            continue\n","        s = str(t).strip()\n","        if s:\n","            cleaned.append(s)\n","\n","    if num_examples and num_examples > 0:\n","        cleaned = cleaned[:num_examples]\n","\n","    return cleaned\n","\n","def _safe_float(x):\n","    try:\n","        return float(x)\n","    except Exception:\n","        return float(\"nan\")\n","\n","def _read_stats_file(path: str) -> dict:\n","    out = {}\n","    with open(path, \"r\") as f:\n","        for line in f:\n","            if \":\" not in line:\n","                continue\n","            k, v = line.strip().split(\": \", 1)\n","            k = k.replace(\"(skipped)\", \"\").strip().lower()\n","            out[k] = _safe_float(v)\n","    return out\n","\n","print(\"Helper functions loaded\")"]},{"cell_type":"markdown","metadata":{"id":"1alT9KsbeoHs"},"source":["## T5 Model Loading"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"t1drYIvieoHs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764843509801,"user_tz":480,"elapsed":47409,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"f02a413d-7764-4692-c9ae-56389843d141"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading T5 model from /content/drive/MyDrive/w266 - Project/t5-base-detox-model...\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"output_type":"stream","name":"stdout","text":["T5 model loaded on cuda\n"]}],"source":["#@title Load T5 model (ParaDetox)\n","print(f\"Loading T5 model from {T5_CHECKPOINT}...\")\n","\n","t5_tokenizer = T5Tokenizer.from_pretrained(T5_CHECKPOINT)\n","t5_model = T5ForConditionalGeneration.from_pretrained(T5_CHECKPOINT)\n","t5_model.eval()\n","\n","DEVICE_T5 = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","t5_model.to(DEVICE_T5)\n","\n","print(f\"T5 model loaded on {DEVICE_T5}\")"]},{"cell_type":"code","source":["#@title T5 multi-candidate generation\n","\n","def t5_generate_candidates(\n","    text: str,\n","    model: T5ForConditionalGeneration,\n","    tokenizer: T5Tokenizer,\n","    num_candidates: int,\n","    temperature: float = 1.0,\n","    top_k: int = 50,\n","    top_p: float = 0.95,\n","    max_length: int = 128,\n","    device: torch.device = DEVICE_T5,\n",") -> List[str]:\n","    \"\"\"\n","    Generate num_candidates outputs for a single input.\n","    \"\"\"\n","    input_text = f\"detoxify: {text}\"\n","    input_ids = tokenizer.encode(\n","        input_text,\n","        return_tensors=\"pt\",\n","        max_length=max_length,\n","        truncation=True,\n","    ).to(device)\n","\n","    with torch.no_grad():\n","        outputs = model.generate(\n","            input_ids,\n","            max_length=max_length,\n","            num_return_sequences=num_candidates,\n","            do_sample=True,\n","            temperature=temperature,\n","            top_k=top_k,\n","            top_p=top_p,\n","            no_repeat_ngram_size=2,\n","        )\n","\n","    return [tokenizer.decode(out, skip_special_tokens=True) for out in outputs]\n","\n","def t5_generate_candidates_batch(\n","    texts: List[str],\n","    model: T5ForConditionalGeneration,\n","    tokenizer: T5Tokenizer,\n","    num_candidates: int,\n","    temperature: float = 1.0,\n","    top_k: int = 50,\n","    top_p: float = 0.95,\n","    max_length: int = 128,\n","    batch_size: int = 8,\n","    device: torch.device = DEVICE_T5,\n",") -> List[List[str]]:\n","    \"\"\"\n","    Batch generation of candidates for many inputs.\n","    \"\"\"\n","    all_candidates: List[List[str]] = []\n","    for i in tqdm(range(0, len(texts), batch_size), desc=\"T5 Generation\"):\n","        batch_texts = texts[i:i + batch_size]\n","        prompts = [f\"detoxify: {t}\" for t in batch_texts]\n","\n","        enc = tokenizer(\n","            prompts,\n","            return_tensors=\"pt\",\n","            max_length=max_length,\n","            truncation=True,\n","            padding=True,\n","        ).to(device)\n","\n","        with torch.no_grad():\n","            outputs = model.generate(\n","                **enc,\n","                max_length=max_length,\n","                num_return_sequences=num_candidates,\n","                do_sample=True,\n","                temperature=temperature,\n","                top_k=top_k,\n","                top_p=top_p,\n","                no_repeat_ngram_size=2,\n","            )\n","\n","        # outputs: shape [B * num_candidates, seq_len]\n","        decoded = [tokenizer.decode(o, skip_special_tokens=True) for o in outputs]\n","        B = len(batch_texts)\n","        for b in range(B):\n","            start = b * num_candidates\n","            end = (b + 1) * num_candidates\n","            all_candidates.append(decoded[start:end])\n","\n","    return all_candidates\n","\n","# Quick sanity check\n","test_text = \"This is a stupid idea\"\n","test_cands = t5_generate_candidates(test_text, t5_model, t5_tokenizer, num_candidates=3, device=DEVICE_T5)\n","print(f\"Input: {test_text}\")\n","for i, c in enumerate(test_cands):\n","    print(f\"  cand[{i}]: {c}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nax9JtmYUrEs","executionInfo":{"status":"ok","timestamp":1764843516603,"user_tz":480,"elapsed":6805,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"2cb51db4-c4e0-483d-c927-f3d2963bc2b4"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Input: This is a stupid idea\n","  cand[0]: This is a bad idea\n","  cand[1]: This is a bad idea\n","  cand[2]: This is a bad idea\n"]}]},{"cell_type":"markdown","source":["## DecompX reranking (using Masker, mask-count-based score)"],"metadata":{"id":"_B_75HaSU2dO"}},{"cell_type":"code","execution_count":11,"metadata":{"id":"8LtjFBnaeoHs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764843516623,"user_tz":480,"elapsed":13,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"26b60f6d-9813-4e89-840d-2743e40e5d9f"},"outputs":[{"output_type":"stream","name":"stdout","text":["DecompX reranking functions loaded\n"]}],"source":["#@title DecompX-based reranking helpers\n","\n","def _decompx_mask_texts(\n","    texts: List[str],\n","    threshold: float = 0.20,\n","    batch_size: int = 16,\n",") -> List[str]:\n","    \"\"\"\n","    Run DecompX Masker on a list of texts and return masked versions.\n","    Same as in LLM DecompX pipeline.\n","    \"\"\"\n","    if not texts:\n","        return []\n","\n","    masker = Masker_single()\n","    masked_all = []\n","    for i in tqdm(range(0, len(texts), batch_size),\n","                  desc=\"DecompX masking for reranking\", leave=False):\n","        batch = texts[i:i + batch_size]\n","        batch_out = masker.process_text(sentence=batch, threshold=threshold)\n","        masked_all.extend(batch_out)\n","    cleaned = [\n","        m.replace(\"<s>\", \"\").replace(\"</s>\", \"\").strip()\n","        for m in masked_all\n","    ]\n","    masker.release_model()\n","    return cleaned\n","\n","def _decompx_toxicity_scores(\n","    texts: List[str],\n","    threshold: float = 0.20,\n","    batch_size: int = 16,\n",") -> np.ndarray:\n","    \"\"\"\n","    Score texts by DecompX 'toxicity':\n","\n","      score = (# of <mask> tokens DecompX inserts) / (# tokens)\n","\n","    Lower score => less DecompX-toxic.\n","    \"\"\"\n","    if not texts:\n","        return np.zeros((0,), dtype=float)\n","\n","    masked = _decompx_mask_texts(texts, threshold=threshold, batch_size=batch_size)\n","    scores = []\n","    for m in masked:\n","        num_masks = len(re.findall(r\"<mask>\", m))\n","        tokens = m.split()\n","        length = max(len(tokens), 1)\n","        scores.append(num_masks / length)\n","    return np.asarray(scores, dtype=float)\n","\n","def rerank_candidates_decompx(\n","    sources: List[str],\n","    candidates: List[List[str]],\n","    threshold: float = 0.20,\n","    batch_size_mask: int = 16,\n","):\n","    \"\"\"\n","    DecompX-based reranking (same as LLM pipeline):\n","\n","      - Flatten candidates\n","      - Score each candidate with DecompX\n","      - For each source, choose candidate with lowest score\n","\n","    Returns:\n","      best_idx: np.ndarray (N,) chosen candidate index per source\n","      details: dict with 'score' matrix shape [N, C]\n","    \"\"\"\n","    N = len(sources)\n","    assert len(candidates) == N, \"candidates length mismatch\"\n","\n","    if N == 0:\n","        return np.array([], dtype=int), {}\n","\n","    C_list = [len(c) for c in candidates]\n","    assert len(set(C_list)) == 1, \"All inputs must have same num_candidates\"\n","    C = C_list[0]\n","    if C == 0:\n","        raise ValueError(\"num_candidates must be >= 1\")\n","\n","    flat_cands = []\n","    flat_src_idx = []\n","    for i, cand_list in enumerate(candidates):\n","        for cand in cand_list:\n","            flat_cands.append(cand)\n","            flat_src_idx.append(i)\n","    flat_src_idx = np.array(flat_src_idx, dtype=int)\n","\n","    scores = _decompx_toxicity_scores(\n","        flat_cands,\n","        threshold=threshold,\n","        batch_size=batch_size_mask,\n","    )  # [N*C]\n","\n","    scores2 = scores.reshape(N, C)\n","    best_idx = np.argmin(scores2, axis=1)\n","\n","    details = {\n","        \"score\": scores2,\n","    }\n","    return best_idx, details\n","\n","print(\"DecompX reranking functions loaded\")"]},{"cell_type":"markdown","metadata":{"id":"7EBMYfvEeoHs"},"source":["## Evaluation models and metrics"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"NvlGUHVZeoHs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764843516645,"user_tz":480,"elapsed":19,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"dae43ed0-db88-4a5b-decc-260c68b1ca86"},"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluation helpers defined\n"]}],"source":["#@title Evaluation helpers (evaluate_all.py with MeaningBERT + toxicity)\n","\n","def _eval_with_toxicity(base_path,\n","                        overwrite_eval: bool = False,\n","                        skip_ref: bool = False,\n","                        tox_threshold: float = 0.5,\n","                        tox_batch_size: int = 32):\n","    \"\"\"\n","    Call evaluation.evaluate_all on each run folder in base_path.\n","    Same pattern as LLM DecompX pipeline.\n","    \"\"\"\n","    import sys as _sys\n","    for folder in os.listdir(base_path):\n","        gen_dir = os.path.join(base_path, folder)\n","        if not os.path.isdir(gen_dir):\n","            continue\n","        orig_path = os.path.join(gen_dir, \"orig.txt\")\n","        gen_path  = os.path.join(gen_dir, \"gen.txt\")\n","        out_stats = os.path.join(gen_dir, \"gen_stats.txt\")\n","        if not (os.path.exists(orig_path) and os.path.exists(gen_path)):\n","            continue\n","        if os.path.exists(out_stats) and not overwrite_eval:\n","            continue\n","\n","        env = os.environ.copy()\n","        env[\"PYTHONPATH\"] = REPO + (\n","            \":\" + env.get(\"PYTHONPATH\", \"\") if env.get(\"PYTHONPATH\") else \"\"\n","        )\n","        cmd = [\n","            _sys.executable, \"-m\", \"evaluation.evaluate_all\",\n","            \"--orig_path\", orig_path,\n","            \"--gen_path\",  gen_path,\n","            \"--tox_threshold\", str(tox_threshold),\n","            \"--tox_batch_size\", str(tox_batch_size),\n","        ]\n","        if skip_ref:\n","            cmd.append(\"--skip_ref\")\n","        print(\"Eval:\", \" \".join(cmd))\n","        res = run(cmd, cwd=REPO, env=env, stdout=PIPE, stderr=PIPE, text=True)\n","        if res.returncode != 0:\n","            print(res.stdout)\n","            print(res.stderr)\n","            res.check_returncode()\n","\n","def _aggregate_eval_csv(output_folder: str,\n","                        data_type: str,\n","                        base_out_dir: str):\n","    \"\"\"\n","    Aggregate eval metrics for T5 ParaDetox + DecompX reranking.\n","\n","    Layout (absolute base_out_dir):\n","      base_out_dir/\n","        └── {data_type}/\n","            └── T5_DecompX/\n","                └── {run_folder}/\n","                    └── gen_stats.txt\n","\n","    threshold column is kept as a label (=0.20) for compatibility.\n","    \"\"\"\n","    rows = []\n","\n","    rerank_dir = \"T5_DecompX\"\n","    base_path  = os.path.join(base_out_dir, data_type, rerank_dir)\n","    if not os.path.isdir(base_path):\n","        print(\"No evaluation directory found:\", base_path)\n","        return\n","\n","    for folder in os.listdir(base_path):\n","        gen_dir    = os.path.join(base_path, folder)\n","        stats_path = os.path.join(gen_dir, \"gen_stats.txt\")\n","        if not os.path.exists(stats_path):\n","            continue\n","        s = _read_stats_file(stats_path)\n","        rows.append({\n","            \"threshold\":       0.20,  # label only, matches LLM pipeline style\n","            \"folder\":          folder,\n","            \"bertscore\":       s.get(\"bertscore\", np.nan),\n","            \"meaningbert\":     s.get(\"meaningbert\", np.nan),\n","            \"bleu4\":           s.get(\"bleu4\", np.nan),\n","            \"perplexity_gen\":  s.get(\"perplexity gen\", np.nan),\n","            \"perplexity_orig\": s.get(\"perplexity orig\", np.nan),\n","            \"toxicity_gen\":    s.get(\"toxicity gen\", np.nan),\n","            \"toxicity_orig\":   s.get(\"toxicity orig\", np.nan),\n","        })\n","\n","    if rows:\n","        cols = [\n","            \"threshold\", \"folder\",\n","            \"bertscore\", \"meaningbert\", \"bleu4\",\n","            \"perplexity_gen\", \"perplexity_orig\",\n","            \"toxicity_gen\", \"toxicity_orig\",\n","        ]\n","        df = pd.DataFrame(rows)\n","        df = df[cols]\n","        out_csv = os.path.join(base_out_dir, data_type, f\"{data_type}.csv\")\n","        _ensure_dir(os.path.dirname(out_csv))\n","        df.to_csv(out_csv, index=False)\n","        print(\"Wrote summary CSV:\", out_csv)\n","    else:\n","        print(\"No evaluation files found to summarize.\")\n","\n","print(\"Evaluation helpers defined\")"]},{"cell_type":"code","source":["#@title Helpers for folder naming\n","\n","def _build_run_folder_name_t5_decompx(\n","    num_candidates: int,\n","    max_length: int,\n","    temperature: float,\n","    top_k: int,\n","    top_p: float,\n","    decompx_threshold: float,\n",") -> str:\n","    \"\"\"\n","    Build a folder name encoding T5 + DecompX hyperparameters.\n","    Parallel to LLM DecompX folder naming.\n","    \"\"\"\n","    return (\n","        f\"t5_nc{num_candidates}_maxlen{max_length}_\"\n","        f\"temp{temperature}_topk{top_k}_topp{top_p}_\"\n","        f\"dxth{decompx_threshold}\"\n","    )"],"metadata":{"id":"OSX30Hxh0DoS","executionInfo":{"status":"ok","timestamp":1764843516709,"user_tz":480,"elapsed":61,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","execution_count":14,"metadata":{"id":"cvnm89jUeoHt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764843516712,"user_tz":480,"elapsed":21,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"d7ff3ee8-037f-4b0d-983d-3c8b65f28c1d"},"outputs":[{"output_type":"stream","name":"stdout","text":["detoxify() defined\n"]}],"source":["#@title detoxify() — T5 + DecompX reranking + evaluate_all\n","\n","def detoxify(\n","    data_type: str = \"paradetox\",\n","    output_folder: str = \"T5_w_DecompX-Reranking_Pipeline\",\n","    echo: bool = False,\n","    num_examples: int = 1000,\n","    batch_size: int = 8,\n","    num_candidates: int = 10,\n","    max_length: int = 128,\n","    temperature: float = 1.0,\n","    top_k: int = 50,\n","    top_p: float = 0.95,\n","    # DecompX\n","    decompx_threshold: float = 0.20,\n","    decompx_batch_size: int = 16,\n","    # generation + eval flags\n","    overwrite_gen: bool = False,\n","    run_eval: bool = True,\n","    overwrite_eval: bool = False,\n","    skip_ref_eval: bool = False,\n","):\n","    \"\"\"\n","    T5 ParaDetox + DecompX reranking + evaluate_all.py\n","\n","    Steps:\n","      1. Load toxic inputs.\n","      2. Generate num_candidates candidates per input with T5.\n","      3. Rerank candidates with DecompX (mask-count ratio).\n","      4. Save orig.txt and gen.txt under:\n","         data/model_outputs/{output_folder}/{data_type}/T5_DecompX/{run_folder}/\n","      5. Run evaluation via evaluation.evaluate_all.py.\n","      6. Aggregate per-dataset CSV like LLM DecompX pipeline.\n","    \"\"\"\n","    assert data_type in data_configs, f\"Unknown data_type: {data_type}\"\n","\n","    # Base output relative to repo (same style as LLM pipeline)\n","    base_out_rel = os.path.join(\"data\", \"model_outputs\", output_folder)\n","    base_out_abs = os.path.join(REPO, base_out_rel)\n","    _ensure_dir(base_out_abs)\n","\n","    # Load data\n","    print(\"=\" * 80)\n","    print(f\"[{data_type}] Loading data...\")\n","    orig_texts = load_test_data(data_type, num_examples)\n","    print(f\"  Loaded {len(orig_texts)} examples\")\n","\n","    if echo:\n","        print(\"\\n[echo] Example inputs (first up to 3):\")\n","        for i, s in enumerate(orig_texts[:3]):\n","            print(f\"  input[{i}]: {s}\")\n","        print(f\"\\n[echo] DecompX threshold: {decompx_threshold}\")\n","        print(f\"[echo] num_candidates per input: {num_candidates}\")\n","\n","    # Directory for this pipeline (T5 + DecompX)\n","    rerank_dir = \"T5_DecompX\"\n","    cur_rel = os.path.join(base_out_rel, data_type, rerank_dir)\n","    cur_abs = os.path.join(REPO, cur_rel)\n","    _ensure_dir(cur_abs)\n","\n","    # Run-folder name for current hyperparameters\n","    run_folder = _build_run_folder_name_t5_decompx(\n","        num_candidates=num_candidates,\n","        max_length=max_length,\n","        temperature=temperature,\n","        top_k=top_k,\n","        top_p=top_p,\n","        decompx_threshold=decompx_threshold,\n","    )\n","    final_abs = os.path.join(cur_abs, run_folder)\n","    _ensure_dir(final_abs)\n","\n","    orig_path  = os.path.join(final_abs, \"orig.txt\")\n","    gen_path   = os.path.join(final_abs, \"gen.txt\")\n","    stats_path = os.path.join(final_abs, \"gen_stats.txt\")\n","\n","    # Generate or reuse outputs\n","    if overwrite_gen or not os.path.exists(gen_path):\n","        print(\"  Generating T5 candidates...\")\n","        all_candidates = t5_generate_candidates_batch(\n","            texts=orig_texts,\n","            model=t5_model,\n","            tokenizer=t5_tokenizer,\n","            num_candidates=num_candidates,\n","            temperature=temperature,\n","            top_k=top_k,\n","            top_p=top_p,\n","            max_length=max_length,\n","            batch_size=batch_size,\n","            device=DEVICE_T5,\n","        )\n","\n","        if echo and all_candidates:\n","            print(\"\\n[echo] Example candidates for input[0]:\")\n","            for j, c in enumerate(all_candidates[0][:3]):\n","                print(f\"    cand[{j}]: {c}\")\n","\n","        print(f\"  DecompX reranking (threshold={decompx_threshold:.2f})...\")\n","        best_idx, details = rerank_candidates_decompx(\n","            sources=orig_texts,\n","            candidates=all_candidates,\n","            threshold=decompx_threshold,\n","            batch_size_mask=decompx_batch_size,\n","        )\n","        best_generations = [\n","            all_candidates[i][best_idx[i]] for i in range(len(orig_texts))\n","        ]\n","\n","        if echo:\n","            print(\"\\n[echo] Selected outputs (first up to 3):\")\n","            for i, g in enumerate(best_generations[:3]):\n","                print(f\"  output[{i}]: {g}\")\n","\n","        # Save orig and gen\n","        with open(orig_path, \"w\") as f:\n","            for t in orig_texts:\n","                f.write(re.sub(r\"\\s+\", \" \", t).strip() + \"\\n\")\n","        with open(gen_path, \"w\") as f:\n","            for t in best_generations:\n","                f.write(re.sub(r\"\\s+\", \" \", t).strip() + \"\\n\")\n","\n","        print(\"  Saved orig/gen to:\", final_abs)\n","    else:\n","        print(\"  Reusing existing orig/gen from:\", final_abs)\n","        with open(orig_path, \"r\") as f:\n","            orig_texts = [l.strip() for l in f]\n","        with open(gen_path, \"r\") as f:\n","            best_generations = [l.strip() for l in f]\n","        print(f\"  Loaded {len(best_generations)} generated examples\")\n","\n","    # Evaluation via evaluation/evaluate_all.py\n","    metrics = None\n","    if run_eval:\n","        base_path = os.path.join(base_out_abs, data_type, rerank_dir)\n","        _eval_with_toxicity(\n","            base_path,\n","            overwrite_eval=overwrite_eval,\n","            skip_ref=skip_ref_eval,\n","            tox_threshold=0.5,\n","            tox_batch_size=32,\n","        )\n","        _aggregate_eval_csv(\n","            output_folder,\n","            data_type,\n","            os.path.join(REPO, \"data\", \"model_outputs\", output_folder),\n","        )\n","\n","        if os.path.exists(stats_path):\n","            metrics = _read_stats_file(stats_path)\n","            if echo:\n","                print(\"\\n[echo] Evaluation metrics for this run:\")\n","                for k, v in metrics.items():\n","                    if isinstance(v, float) and math.isnan(v):\n","                        continue\n","                    print(f\"  {k}: {v:.4f}\")\n","        else:\n","            print(\"  gen_stats.txt not found for this run; no metrics to print.\")\n","\n","    print(\"=\" * 80)\n","    return metrics\n","\n","print(\"detoxify() defined\")"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"KYhL4PZoeoHt","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["d9fa30fb011c40f0a5ff0ed73563cf8e","68e7ed1395a2451da4bab619de34e553","5320369743274bb3923ff70e7b6e4851","f71fb6720b724e7db15968ae2b1b289e","97a943e8f0d448f1b00a8dff35d95c2f","181dded5a16d48c688e0302049ab7315","9ae114b12c354618b84cec044810276b","fbe5387e200a4b68a03557f215aaf9fd","a47ceb8cb5c54753bbbc063e2a0c2edc","7a9f5ee14726400aa28be267b2af989a","6f2d9b4364284d85ae8e138e8001d000","3a6bef4c7e7f4036b887e8a1cf4a1daa","0ccce364a1564d0cbc5b693848a9ed43","3b6fd9b7cd2649e48c63e568e32ba5ba","9e7a558bc7784fe18157d95acde90162","d4fcfb7905534e85a232dae9d6371bd4","f0d20aeed9bc4213a8851a96792c17bd","ce16ba3c4baf46228ca5841c146b7b66","a14045c3c03f4858bf2231dd14fc743a","6e9a73b3a6f24315bc3aed9e4cbc6353","6e8bbd04a5cf407ea246f61a62fb4609","3bf27dfd1b3b4b1da19aa3d632b38536"]},"executionInfo":{"status":"ok","timestamp":1764844526287,"user_tz":480,"elapsed":1009580,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"b4ec0927-f396-4f79-e708-785ce138f72b"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","[paradetox] Loading data...\n","  Loaded 671 examples\n","\n","[echo] Example inputs (first up to 3):\n","  input[0]: . or the loud ass one - thousand ton beast roaring towards you howling its horn .\n","  input[1]: mandated  and \" right fucking now \" would be good .\n","  input[2]: * neither * of my coworkers gave a shit when it came time to ditch mitch . ugh .\n","\n","[echo] DecompX threshold: 0.2\n","[echo] num_candidates per input: 10\n","  Generating T5 candidates...\n"]},{"output_type":"display_data","data":{"text/plain":["T5 Generation:   0%|          | 0/84 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9fa30fb011c40f0a5ff0ed73563cf8e"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","[echo] Example candidates for input[0]:\n","    cand[0]: . or the one thousand ton beast roaring towards you howling its horn\n","    cand[1]: or the one thousand ton beast roaring toward you howling its horn\n","    cand[2]: or the one thousand ton beast roaring towards you howling its horn\n","  DecompX reranking (threshold=0.20)...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n","Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n","You are not authenticated with the Hugging Face Hub in this notebook.\n","If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n","  warnings.warn(\n","Some weights of the model checkpoint at s-nlp/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"display_data","data":{"text/plain":["DecompX masking for reranking:   0%|          | 0/420 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a6bef4c7e7f4036b887e8a1cf4a1daa"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:1052: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","[echo] Selected outputs (first up to 3):\n","  output[0]: .or the one - thousand ton beast roaring towards you howling its horn\n","  output[1]: Mandatory and \"right now\" would be good\n","  output[2]: neither * of my coworkers gave a damn when it came time to ditch mitch . ugh\n","  Saved orig/gen to: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/T5_w_DecompX-Reranking_Pipeline - KB/paradetox/T5_DecompX/t5_nc10_maxlen128_temp1.0_topk50_topp0.95_dxth0.2\n","Eval: /usr/bin/python3 -m evaluation.evaluate_all --orig_path /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/T5_w_DecompX-Reranking_Pipeline - KB/paradetox/T5_DecompX/t5_nc10_maxlen128_temp1.0_topk50_topp0.95_dxth0.2/orig.txt --gen_path /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/T5_w_DecompX-Reranking_Pipeline - KB/paradetox/T5_DecompX/t5_nc10_maxlen128_temp1.0_topk50_topp0.95_dxth0.2/gen.txt --tox_threshold 0.5 --tox_batch_size 32\n","Wrote summary CSV: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/T5_w_DecompX-Reranking_Pipeline - KB/paradetox/paradetox.csv\n","\n","[echo] Evaluation metrics for this run:\n","  bertscore: 0.9473\n","  meaningbert: 71.4820\n","  bleu4: 88.2330\n","  toxicity gen: 0.2077\n","  perplexity gen: 235.2200\n","  toxicity orig: 0.9253\n","  perplexity orig: 273.7500\n","  percent toxic gen: 0.2042\n","  percent toxic ref: 0.9285\n","================================================================================\n","\n","Paradetox metrics for this run:\n","  bertscore: 0.9473\n","  meaningbert: 71.4820\n","  bleu4: 88.2330\n","  toxicity gen: 0.2077\n","  perplexity gen: 235.2200\n","  toxicity orig: 0.9253\n","  perplexity orig: 273.7500\n","  percent toxic gen: 0.2042\n","  percent toxic ref: 0.9285\n"]}],"source":["#@title Example run on ParaDetox\n","metrics_paradetox = detoxify(\n","    data_type=\"paradetox\",\n","    output_folder=\"T5_w_DecompX-Reranking_Pipeline - KB\",\n","    echo=True,\n","    num_examples=1000,\n","    batch_size=8,\n","    num_candidates=10,\n","    max_length=128,\n","    temperature=1.0,\n","    top_k=50,\n","    top_p=0.95,\n","    decompx_threshold=0.20,\n","    decompx_batch_size=16,\n","    overwrite_gen=True,\n","    run_eval=True,\n","    overwrite_eval=True,\n","    skip_ref_eval=False,\n",")\n","\n","print(\"\\nParadetox metrics for this run:\")\n","if metrics_paradetox:\n","    for k, v in metrics_paradetox.items():\n","        if isinstance(v, float) and math.isnan(v):\n","            continue\n","        print(f\"  {k}: {v:.4f}\")"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"wSlQBk1AeoHt","executionInfo":{"status":"ok","timestamp":1764844526307,"user_tz":480,"elapsed":12,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"outputs":[],"source":["# #@title Run on multiple datasets\n","\n","# datasets_to_eval = [\"paradetox\", \"microagressions_test\", \"sbf_test\", \"dynabench_test\"]\n","# num_examples = 200\n","# output_folder = \"T5_w_DecompX-Reranking_Pipeline\"\n","\n","# all_results = {}\n","\n","# print(\"\\n\" + \"=\" * 80)\n","# print(\"T5-PARADETOX + DECOMPX RERANKING PIPELINE (evaluate_all)\")\n","# print(\"=\" * 80)\n","\n","# for dataset_name in datasets_to_eval:\n","#     try:\n","#         results = detoxify(\n","#             data_type=dataset_name,\n","#             output_folder=output_folder,\n","#             echo=False,\n","#             num_examples=num_examples,\n","#             batch_size=8,\n","#             num_candidates=10,\n","#             max_length=128,\n","#             temperature=1.0,\n","#             top_k=50,\n","#             top_p=0.95,\n","#             decompx_threshold=0.20,\n","#             decompx_batch_size=16,\n","#             overwrite_gen=False,\n","#             run_eval=True,\n","#             overwrite_eval=False,\n","#             skip_ref_eval=False,\n","#         )\n","#         if results:\n","#             all_results[dataset_name] = results\n","#             print(f\"  {dataset_name}: done\")\n","#     except Exception as e:\n","#         print(f\"  Error on {dataset_name}: {e}\")\n","#         import traceback\n","#         traceback.print_exc()\n","#         continue\n","\n","# print(\"\\n\" + \"=\" * 80)\n","\n","# # Optional short summary of this batch of runs\n","# if all_results:\n","#     rows = []\n","#     for dataset_name, results in all_results.items():\n","#         row = {\"dataset\": dataset_name}\n","#         row.update(results)\n","#         rows.append(row)\n","\n","#     df = pd.DataFrame(rows)\n","\n","#     # Map keys with spaces to snake_case for convenience\n","#     rename_map = {\n","#         \"perplexity gen\": \"perplexity_gen\",\n","#         \"perplexity orig\": \"perplexity_orig\",\n","#         \"toxicity gen\": \"toxicity_gen\",\n","#         \"toxicity orig\": \"toxicity_orig\",\n","#     }\n","#     df = df.rename(columns=rename_map)\n","\n","#     col_order = [\n","#         \"dataset\",\n","#         \"bertscore\",\n","#         \"meaningbert\",\n","#         \"bleu4\",\n","#         \"perplexity_gen\",\n","#         \"perplexity_orig\",\n","#         \"toxicity_gen\",\n","#         \"toxicity_orig\",\n","#     ]\n","#     df = df[[c for c in col_order if c in df.columns]]\n","\n","#     summary_csv = os.path.join(\n","#         XDETOX_DIR,\n","#         \"data\",\n","#         \"model_outputs\",\n","#         output_folder,\n","#         \"t5_decompx_summary_latest_run.csv\",\n","#     )\n","#     _ensure_dir(os.path.dirname(summary_csv))\n","#     df.to_csv(summary_csv, index=False)\n","#     print(f\"Saved summary of this run to {summary_csv}\\n\")\n","#     print(df.to_string(index=False))\n","# else:\n","#     print(\"No per-run metrics collected (per-dataset CSVs still written under data/model_outputs).\")\n"]},{"cell_type":"code","source":[],"metadata":{"id":"JfCJf7L7l_0K","executionInfo":{"status":"ok","timestamp":1764844526314,"user_tz":480,"elapsed":3,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"execution_count":16,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"d9fa30fb011c40f0a5ff0ed73563cf8e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_68e7ed1395a2451da4bab619de34e553","IPY_MODEL_5320369743274bb3923ff70e7b6e4851","IPY_MODEL_f71fb6720b724e7db15968ae2b1b289e"],"layout":"IPY_MODEL_97a943e8f0d448f1b00a8dff35d95c2f"}},"68e7ed1395a2451da4bab619de34e553":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_181dded5a16d48c688e0302049ab7315","placeholder":"​","style":"IPY_MODEL_9ae114b12c354618b84cec044810276b","value":"T5 Generation: 100%"}},"5320369743274bb3923ff70e7b6e4851":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fbe5387e200a4b68a03557f215aaf9fd","max":84,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a47ceb8cb5c54753bbbc063e2a0c2edc","value":84}},"f71fb6720b724e7db15968ae2b1b289e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a9f5ee14726400aa28be267b2af989a","placeholder":"​","style":"IPY_MODEL_6f2d9b4364284d85ae8e138e8001d000","value":" 84/84 [01:33&lt;00:00,  1.06it/s]"}},"97a943e8f0d448f1b00a8dff35d95c2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"181dded5a16d48c688e0302049ab7315":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ae114b12c354618b84cec044810276b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fbe5387e200a4b68a03557f215aaf9fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a47ceb8cb5c54753bbbc063e2a0c2edc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7a9f5ee14726400aa28be267b2af989a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f2d9b4364284d85ae8e138e8001d000":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3a6bef4c7e7f4036b887e8a1cf4a1daa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0ccce364a1564d0cbc5b693848a9ed43","IPY_MODEL_3b6fd9b7cd2649e48c63e568e32ba5ba","IPY_MODEL_9e7a558bc7784fe18157d95acde90162"],"layout":"IPY_MODEL_d4fcfb7905534e85a232dae9d6371bd4"}},"0ccce364a1564d0cbc5b693848a9ed43":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f0d20aeed9bc4213a8851a96792c17bd","placeholder":"​","style":"IPY_MODEL_ce16ba3c4baf46228ca5841c146b7b66","value":"DecompX masking for reranking: 100%"}},"3b6fd9b7cd2649e48c63e568e32ba5ba":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_a14045c3c03f4858bf2231dd14fc743a","max":420,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6e9a73b3a6f24315bc3aed9e4cbc6353","value":420}},"9e7a558bc7784fe18157d95acde90162":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e8bbd04a5cf407ea246f61a62fb4609","placeholder":"​","style":"IPY_MODEL_3bf27dfd1b3b4b1da19aa3d632b38536","value":" 419/420 [02:40&lt;00:00,  4.31it/s]"}},"d4fcfb7905534e85a232dae9d6371bd4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"f0d20aeed9bc4213a8851a96792c17bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce16ba3c4baf46228ca5841c146b7b66":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a14045c3c03f4858bf2231dd14fc743a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e9a73b3a6f24315bc3aed9e4cbc6353":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6e8bbd04a5cf407ea246f61a62fb4609":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bf27dfd1b3b4b1da19aa3d632b38536":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
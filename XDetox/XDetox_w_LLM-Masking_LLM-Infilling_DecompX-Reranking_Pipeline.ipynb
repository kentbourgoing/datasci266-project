{"cells":[{"cell_type":"markdown","id":"82d4549b","metadata":{"id":"82d4549b"},"source":["# XDetox with LLM Masking, LLM Infilling, and DecompX Reranking\n","\n","This notebook runs an XDetox pipeline with:\n","\n","1. **LLM masking** using Mistral-7B-Instruct (`mistralai/Mistral-7B-Instruct-v0.2`), which detects toxic spans and replaces them with `<mask>`.\n","2. **LLM infilling** (same Mistral model) that fills the `<mask>` tokens with safe alternatives while keeping the rest of the sentence almost unchanged.\n","3. **DecompX reranking** of multiple LLM candidates per input, using token-level importance scores for toxicity from DecompX.\n","\n","The goal is to pick, for each toxic input sentence, **one best detoxified candidate** that is:\n","\n","* As **non-toxic** as possible.\n","* As **semantically close** as possible to the original.\n","* As **fluent and natural** as possible.\n","\n","The main differences from other pipelines are:\n","\n","* Both **masking** and **infilling** are done by an LLM (Mistral-7B-Instruct).\n","* There is **no MaRCo / BART generation**.\n","* **DecompX is used only for reranking**, not for masking.\n","\n","---\n","\n","## Scoring: DecompX Reranking\n","\n","Let $s_j$ be a candidate detoxified sentence, and let $t_{i,j}$ be its tokens.\n","DecompX gives an **importance score** $\\text{Importance}(t_{i,j})$ for each token, which measures how much that token contributes to predicted toxicity.\n","\n","For each candidate $s_j$, DecompX reranking computes a **toxicity score** as the sum of token importances:\n","\n","$$\n","\\text{Score}(s_j) = \\sum_{i=1}^{N_j} \\text{Importance}(t_{i,j})\n","$$\n","\n","where $N_j$ is the number of tokens in $s_j$.\n","\n","The final chosen sentence $s^*$ is:\n","\n","$$\n","s^* = \\arg\\min_{s_j} \\left( \\sum_{i=1}^{N_j} \\text{Importance}(t_{i,j}) \\right)\n","$$\n","\n","In words:\n","\n","* DecompX is applied to **each candidate sentence**.\n","* For each candidate, we **aggregate token-level toxicity importance**.\n","* The candidate with the **lowest total importance** (lowest contribution to toxicity) is selected.\n","\n","The notebook uses a **DecompX threshold** `decompx_threshold` that controls which tokens are considered toxic enough to be masked / receive high importance. A lower threshold is more sensitive and may highlight more tokens, while a higher threshold is stricter.\n","\n","---\n","\n","## LLM Masking (Mistral-7B-Instruct)\n","\n","### Prompted masking behavior\n","\n","Masking is done by a chat-style LLM:\n","\n","* Model: `mistralai/Mistral-7B-Instruct-v0.2`.\n","* The LLM is instructed to:\n","\n","  * **Identify toxic, offensive, or profane words or short phrases**.\n","  * Replace **each toxic span** with a **single `<mask>` token**.\n","  * Allow **multiple `<mask>` tokens** in one sentence (one per toxic span).\n","  * If multiple neighboring words are toxic, **collapse them into one `<mask>`**\n","    (no `<mask> <mask> ...` runs).\n","  * Keep **all non-toxic words and punctuation unchanged**.\n","  * **Not** paraphrase, summarize, or reorder the sentence.\n","  * Return the masked sentence **inside exactly one pair of brackets**:\n","\n","    ```text\n","    [This is a <mask> example.]\n","    ```\n","\n","### Post-processing of LLM masks\n","\n","Because LLM output can be noisy, the notebook cleans the raw masked output:\n","\n","1. **Extract the bracket content**:\n","\n","   * Read the first `[ ... ]` block if present.\n","   * If there is `[` but no `]`, take everything after the first `[` as the sentence.\n","   * If there are no brackets, fall back to the full string.\n","\n","2. **Strip outer brackets** that may remain.\n","\n","3. **Normalize whitespace** (collapse repeated spaces).\n","\n","4. **Normalize `<mask>` tokens**:\n","\n","   * Variants like `<Mask>`, `<MASK>`, `< mask >` are normalized to `<mask>`.\n","\n","5. **Collapse runs of `<mask>`**:\n","\n","   * Any sequence `<mask> <mask> <mask>` becomes a single `<mask>`.\n","\n","6. If cleaning yields an empty string, fall back to the original masked text.\n","\n","All cleaned, LLM-masked sentences are written to:\n","\n","* `data/model_outputs/{output_folder}/{data_type}/LLM_Mask_LLM_DecompX/masked_inputs.txt`\n","\n","and reused across later runs with the same `output_folder` and `data_type`.\n","\n","---\n","\n","## LLM Infilling (Mistral-7B-Instruct)\n","\n","After masking, detoxification is also done by Mistral-7B-Instruct via **infilling**.\n","\n","For each example, the LLM sees two inputs:\n","\n","1. **Toxic Sentence**: the original toxic sentence.\n","2. **Masked Sentence**: the same sentence, where toxic spans have been replaced with `<mask>`.\n","\n","The infilling prompt instructs the LLM to:\n","\n","* Treat the **Masked Sentence** as a **template**.\n","* For each `<mask>` token:\n","\n","  * Insert a **short, non-toxic word or phrase** that fits the context.\n","  * Preserve the meaning and intent of the **Toxic Sentence** as much as possible.\n","* Keep **all non-masked text unchanged**, except for small edits needed for grammar or agreement.\n","* Keep the **language the same** (no translation).\n","* Output only the final detoxified sentence in **one pair of brackets**:\n","\n","  ```text\n","  [Detoxified sentence here.]\n","  ```\n","\n","For each `(source, masked)` pair, the notebook:\n","\n","1. Builds a prompt that includes both **Toxic Sentence** and **Masked Sentence** plus a few-shot example.\n","2. Calls `generate(...)` with:\n","\n","   * `num_return_sequences = num_candidates`,\n","   * `do_sample = llm_sample`,\n","   * `temperature = llm_temperature` (if sampling),\n","   * `top_p = llm_top_p`,\n","   * `max_new_tokens = llm_max_new_tokens`.\n","3. Decodes only the **new tokens** after the prompt.\n","4. Extracts the text inside `[ ... ]` and cleans it:\n","\n","   * Remove outer brackets,\n","   * Normalize whitespace,\n","   * Remove any leftover `<mask>` tokens.\n","\n","If cleaning produces an empty string, the pipeline falls back to the original toxic input for that candidate.\n","\n","This produces a list of candidates:\n","\n","* `candidates[i]` is a list of length `num_candidates` for input $i$.\n","\n","---\n","\n","## DecompX Reranking of LLM Candidates\n","\n","Once we have LLM-infilling candidates, we apply **DecompX reranking**:\n","\n","1. **Flatten** all candidates into one list while tracking which input they belong to.\n","\n","2. For each candidate, run DecompX to obtain token-level importance scores for toxicity.\n","\n","3. For each candidate $s_j$, compute:\n","\n","   $$\n","   \\text{Score}(s_j) = \\sum_{i=1}^{N_j} \\text{Importance}(t_{i,j})\n","   $$\n","\n","   where $N_j$ is the number of tokens in the candidate.\n","\n","4. For each input sentence, collect the scores of all its candidates and choose the candidate with the **lowest DecompX score**.\n","\n","The `decompx_threshold` parameter:\n","\n","* Controls the **sensitivity** of DecompX to toxicity.\n","* A lower value will tend to assign non-zero importance to more tokens (more sensitive).\n","* A higher value will only highlight more strongly toxic tokens.\n","\n","For each run folder, the notebook writes:\n","\n","* `orig.txt` — original toxic inputs (one per line),\n","* `gen.txt` — chosen detoxified outputs (one per line).\n","\n","Outputs are stored under:\n","\n","```text\n","data/model_outputs/{output_folder}/{data_type}/LLM_Mask_LLM_DecompX/{run_folder}/\n","```\n","\n","where `{run_folder}` encodes LLM infilling hyperparameters and DecompX settings.\n","\n","---\n","\n","## Evaluation\n","\n","If `run_eval=True`, the pipeline calls `evaluation.evaluate_all` to compute:\n","\n","* BERTScore (F1)\n","* MeaningBERT\n","* BLEU-4\n","* Toxicity (orig / gen) using XLM-R\n","* Perplexity (orig / gen) using GPT-2\n","\n","For each run folder, it writes:\n","\n","* `gen_stats.txt` under:\n","\n","  ```text\n","  data/model_outputs/{output_folder}/{data_type}/LLM_Mask_LLM_DecompX/{run_folder}/\n","  ```\n","\n","It also creates a **summary CSV per dataset**:\n","\n","* `data/model_outputs/{output_folder}/{data_type}/{data_type}.csv`\n","\n","The CSV aggregates metrics over all run folders inside `LLM_Mask_LLM_DecompX/`.\n","\n","For compatibility with other XDetox pipelines, the CSV keeps a `threshold` column, which is used here as a **label** (for example `0.20`) tied to the DecompX reranking configuration, not to a masking threshold.\n","\n","---\n","\n","## How to Use `detoxify()`\n","\n","Function signature (conceptual):\n","\n","```python\n","def detoxify(\n","    data_type: str = \"paradetox\",\n","    output_folder: str = \"colab_run_llm_mask_infill_decompx\",\n","    echo: bool = False,\n","    num_examples: int = 100,\n","    overwrite_gen: bool = False,\n","    run_eval: bool = False,\n","    overwrite_eval: bool = False,\n","    skip_ref_eval: bool = False,\n","    # LLM infilling:\n","    num_candidates: int = 3,\n","    llm_temperature: float = 0.7,\n","    llm_top_p: float = 0.95,\n","    llm_max_new_tokens: int = 64,\n","    llm_sample: bool = True,\n","    # DecompX reranking:\n","    decompx_threshold: float = 0.20,\n","    decompx_batch_size: int = 16,\n",")\n","```\n","\n","### Key arguments\n","\n","#### Core I/O\n","\n","* `data_type`: dataset key from `data_configs`, for example:\n","\n","  * `\"paradetox\"`, `\"dynabench_val\"`, `\"dynabench_test\"`,\n","  * `\"jigsaw_toxic\"`, `\"microagressions_val\"`, `\"sbf_val\"`,\n","  * `\"appdia_original\"`, `\"appdia_discourse\"`, etc.\n","\n","* `output_folder`:\n","\n","  * Top-level directory under `data/model_outputs/`:\n","\n","    ```text\n","    data/model_outputs/{output_folder}/{data_type}/...\n","    ```\n","\n","* `num_examples`:\n","\n","  * If set, only the first `num_examples` examples are processed.\n","  * Use `None` to run on the full dataset.\n","\n","* `overwrite_gen`:\n","\n","  * If `False` and a matching `gen.txt` already exists, the notebook reuses previous generations.\n","  * If `True`, it regenerates outputs for that run folder.\n","\n","* `echo`:\n","\n","  * If `True`, the notebook prints:\n","\n","    * Dataset and subset path,\n","    * Output base directory,\n","    * A few example inputs,\n","    * A few LLM-masked outputs,\n","    * A few final detoxified outputs,\n","    * And, if `run_eval=True`, evaluation metrics.\n","\n","#### LLM masking (Mistral)\n","\n","* Masking uses **Mistral-7B-Instruct** with a fixed system prompt and few-shot example.\n","* There is **no masking threshold**; the LLM decides which spans to mask based on the instructions.\n","* Masked sentences are cached in:\n","\n","  ```text\n","  data/model_outputs/{output_folder}/{data_type}/LLM_Mask_LLM_DecompX/masked_inputs.txt\n","  ```\n","\n","and reused if present.\n","\n","#### LLM infilling (Mistral)\n","\n","* `num_candidates`:\n","\n","  * Number of infilling candidates to generate per input.\n","\n","* `llm_temperature`:\n","\n","  * Sampling temperature (only used if `llm_sample=True`).\n","\n","* `llm_top_p`:\n","\n","  * Nucleus sampling parameter during infilling.\n","\n","* `llm_max_new_tokens`:\n","\n","  * Maximum number of new tokens to generate beyond the prompt.\n","\n","* `llm_sample`:\n","\n","  * `True`: stochastic sampling with `temperature` and `top_p`.\n","  * `False`: deterministic decoding (similar to greedy).\n","\n","The infilling step uses **both** the Toxic Sentence and the Masked Sentence, and is strictly instructed to **only change `<mask>` spans** plus minor grammar fixes.\n","\n","#### DecompX reranking\n","\n","* `decompx_threshold`:\n","\n","  * DecompX threshold used when computing token-level toxicity importance.\n","  * Roughly controls how aggressively DecompX marks tokens as toxic.\n","\n","* `decompx_batch_size`:\n","\n","  * Batch size for DecompX processing during reranking.\n","\n","For each input:\n","\n","* The pipeline uses DecompX to score each LLM candidate.\n","* It picks the candidate whose sum of token importance scores is **minimal**.\n","\n","#### Evaluation\n","\n","* `run_eval`:\n","\n","  * If `True`, run `evaluation.evaluate_all` and write `gen_stats.txt`.\n","\n","* `overwrite_eval`:\n","\n","  * If `False` and `gen_stats.txt` exists, keep existing metrics.\n","  * If `True`, recompute metrics.\n","\n","* `skip_ref_eval`:\n","\n","  * If `True`, skip some reference-based parts (for example, reference perplexity).\n","\n","---\n","\n","## Example Calls\n","\n","### Quick sanity check on a small subset\n","\n","```python\n","detoxify(\n","    data_type=\"paradetox\",\n","    output_folder=\"colab_run_llm_mask_infill_decompx_demo_50_examples\",\n","    echo=True,\n","    num_examples=50,               # small subset for testing\n","    run_eval=True,                 # BLEU / BERTScore / MeaningBERT / PPL / Toxicity\n","    overwrite_gen=True,\n","    overwrite_eval=True,\n","    skip_ref_eval=False,\n","    num_candidates=10,             # LLM infilling candidates per input\n","    llm_temperature=0.7,\n","    llm_top_p=0.95,\n","    llm_max_new_tokens=64,\n","    llm_sample=True,\n","    decompx_threshold=0.20,\n","    decompx_batch_size=16,\n",")\n","```\n","\n","### Larger run (more candidates, full dataset)\n","\n","```python\n","detoxify(\n","    data_type=\"paradetox\",\n","    output_folder=\"paradetox_llm_mask_infill_decompx_full\",\n","    echo=True,\n","    num_examples=None,             # full dataset\n","    run_eval=True,\n","    overwrite_gen=False,\n","    overwrite_eval=False,\n","    skip_ref_eval=False,\n","    num_candidates=20,\n","    llm_temperature=0.7,\n","    llm_top_p=0.95,\n","    llm_max_new_tokens=64,\n","    llm_sample=True,\n","    decompx_threshold=0.20,\n","    decompx_batch_size=16,\n",")\n","```\n","\n","After running `detoxify`, you can inspect:\n","\n","* Final inputs and outputs:\n","\n","  ```text\n","  data/model_outputs/{output_folder}/{data_type}/LLM_Mask_LLM_DecompX/{run_folder}/orig.txt\n","  data/model_outputs/{output_folder}/{data_type}/LLM_Mask_LLM_DecompX/{run_folder}/gen.txt\n","  ```\n","\n","* Per-run evaluation metrics:\n","\n","  ```text\n","  data/model_outputs/{output_folder}/{data_type}/LLM_Mask_LLM_DecompX/{run_folder}/gen_stats.txt\n","  ```\n","\n","* Aggregated metrics across runs:\n","\n","  ```text\n","  data/model_outputs/{output_folder}/{data_type}/{data_type}.csv\n","  ```\n","\n","This pipeline lets you compare:\n","\n","* **LLM-masking + LLM-infilling + DecompX reranking**\n","\n","against other XDetox pipelines such as:\n","\n","* **LLM-masking + LLM-infilling + global reranking**,\n","* **DecompX-masking + LLM-infilling + DecompX or global reranking**, and\n","* **LLM-masking or DecompX-masking with MaRCo generation**.\n"]},{"cell_type":"code","source":["#@title Mount Drive, Imports & locate XDetox\n","from google.colab import drive; drive.mount('/content/drive')\n","\n","import os, glob, re, sys, json, shutil, math\n","import numpy as np\n","import pandas as pd\n","from tqdm.auto import tqdm\n","from pathlib import Path\n","from subprocess import run, PIPE\n","import torch\n","import nltk\n","from typing import List\n","\n","# Try My Drive\n","candidate = \"/content/drive/MyDrive/w266 - Project/XDetox\"\n","print(\"Try MyDrive:\", candidate, \"->\", os.path.isdir(candidate))\n","\n","XDETOX_DIR = candidate\n","print(\"Using XDETOX_DIR:\", XDETOX_DIR)\n","assert os.path.isdir(XDETOX_DIR), f\"XDETOX_DIR does not exist: {XDETOX_DIR}\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kfBoQTrjtynY","executionInfo":{"status":"ok","timestamp":1764559915440,"user_tz":480,"elapsed":35813,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"5c994425-f684-48aa-88cd-153182a1fb67"},"id":"kfBoQTrjtynY","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Try MyDrive: /content/drive/MyDrive/w266 - Project/XDetox -> True\n","Using XDETOX_DIR: /content/drive/MyDrive/w266 - Project/XDetox\n"]}]},{"cell_type":"code","source":["#@title Runtime setup (paths, cache, GPU)\n","HF_CACHE = os.path.join(XDETOX_DIR, \"cache\")\n","os.makedirs(HF_CACHE, exist_ok=True)\n","os.environ[\"TRANSFORMERS_CACHE\"] = HF_CACHE\n","\n","if XDETOX_DIR not in sys.path:\n","    sys.path.append(XDETOX_DIR)\n","\n","print(\"XDETOX_DIR:\", XDETOX_DIR)\n","print(\"TRANSFORMERS_CACHE:\", HF_CACHE)\n","print(\"CUDA available:\", torch.cuda.is_available())\n","if torch.cuda.is_available():\n","    print(\"GPU:\", torch.cuda.get_device_name(0))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ITPlTNBtzQx","executionInfo":{"status":"ok","timestamp":1764559915987,"user_tz":480,"elapsed":561,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"6348b089-9d78-4268-a43f-163d1c0a6f91"},"id":"7ITPlTNBtzQx","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["XDETOX_DIR: /content/drive/MyDrive/w266 - Project/XDetox\n","TRANSFORMERS_CACHE: /content/drive/MyDrive/w266 - Project/XDetox/cache\n","CUDA available: True\n","GPU: NVIDIA A100-SXM4-40GB\n"]}]},{"cell_type":"code","source":["#@title Verify XDetox repo layout\n","for d in [\"rewrite\", \"evaluation\", \"datasets\"]:\n","    assert os.path.isdir(os.path.join(XDETOX_DIR, d)), f\"Missing folder: {d}\"\n","print(\"Repo folders OK.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MEy2TGYetzIb","executionInfo":{"status":"ok","timestamp":1764559916005,"user_tz":480,"elapsed":17,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"37ddb218-f598-4c9a-fd2e-32274ce3e849"},"id":"MEy2TGYetzIb","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Repo folders OK.\n"]}]},{"cell_type":"code","source":["#@title Install dependencies (restart runtime if major errors)\n","!pip -q install --upgrade pip setuptools wheel\n","!pip -q install \"transformers==4.41.2\" \"tokenizers==0.19.1\" \\\n","                \"datasets==2.19.0\" \"evaluate==0.4.1\" \\\n","                \"sacrebleu==2.4.1\" sacremoses ftfy nltk matplotlib pandas jedi \\\n","                sentencepiece\n","!pip -q install bert-score\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GeTzwxVDtzNn","executionInfo":{"status":"ok","timestamp":1764559937981,"user_tz":480,"elapsed":21972,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"de0e825b-b60a-4cfa-98a5-ae9ef60a0bdc"},"id":"GeTzwxVDtzNn","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.3.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["#@title Imports from transformers / rewrite\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForCausalLM,\n",")\n","from rewrite.mask_orig import Masker as Masker_single\n","from rewrite import rewrite_example as rx\n","import argparse as _argparse"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tfnuR2YVCmW9","executionInfo":{"status":"ok","timestamp":1764559948925,"user_tz":480,"elapsed":10941,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"e41ca59a-11a6-4d82-ed25-9faac57fcce7"},"id":"tfnuR2YVCmW9","execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["#@title NLTK data\n","nltk.download(\"punkt\", quiet=True)\n","try:\n","    nltk.download(\"punkt_tab\", quiet=True)\n","except Exception:\n","    pass\n","print(\"NLTK ready\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y0Up7SKstzK9","executionInfo":{"status":"ok","timestamp":1764559949369,"user_tz":480,"elapsed":446,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"45b17dba-6b16-48c1-c03d-7ff5e2bde002"},"id":"y0Up7SKstzK9","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["NLTK ready\n"]}]},{"cell_type":"code","source":["#@title Data configs\n","data_configs = {\n","    \"microagressions_val\": {\n","        \"data_path\": \"./datasets/microagressions/val.csv\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.25,\n","        \"temperature\": 2.5,\n","    },\n","    \"microagressions_test\": {\n","        \"data_path\": \"./datasets/microagressions/test.csv\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.25,\n","        \"temperature\": 2.5,\n","    },\n","    \"sbf_val\": {\n","        \"data_path\": \"./datasets/sbf/sbfdev.csv\",\n","        \"rep_penalty\": 1.5,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 5.0,\n","        \"temperature\": 2.9,\n","    },\n","    \"sbf_test\": {\n","        \"data_path\": \"./datasets/sbf/sbftst.csv\",\n","        \"rep_penalty\": 1.5,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 5.0,\n","        \"temperature\": 2.9,\n","    },\n","    \"dynabench_val\": {\n","        \"data_path\": \"./datasets/dynabench/db_dev.csv\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.75,\n","        \"temperature\": 2.5,\n","    },\n","    \"dynabench_test\": {\n","        \"data_path\": \"./datasets/dynabench/db_test.csv\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.75,\n","        \"temperature\": 2.5,\n","    },\n","    \"jigsaw_toxic\": {\n","        \"data_path\": \"./datasets/jigsaw_full_30/test_10k_toxic.txt\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.75,\n","        \"temperature\": 2.5,\n","    },\n","    \"paradetox\": {\n","        \"data_path\": \"./datasets/paradetox/test_toxic_parallel.txt\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.75,\n","        \"temperature\": 2.5,\n","    },\n","    \"appdia_original\": {\n","        \"data_path\": \"./datasets/appdia/original-annotated-data/original-test.tsv\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.75,\n","        \"temperature\": 2.5,\n","    },\n","    \"appdia_discourse\": {\n","        \"data_path\": \"./datasets/appdia/discourse-augmented-data/discourse-test.tsv\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.75,\n","        \"temperature\": 2.5,\n","    }\n","}\n","print(\"Datasets:\", \", \".join(data_configs.keys()))\n","\n","REPO = XDETOX_DIR"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7nBku39IuAgb","executionInfo":{"status":"ok","timestamp":1764559949407,"user_tz":480,"elapsed":35,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"2867d6c9-05da-47b7-c681-6ae9305cb128"},"id":"7nBku39IuAgb","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Datasets: microagressions_val, microagressions_test, sbf_val, sbf_test, dynabench_val, dynabench_test, jigsaw_toxic, paradetox, appdia_original, appdia_discourse\n"]}]},{"cell_type":"code","source":["#@title Helpers: subset data\n","def _abs_repo_path(rel: str) -> str:\n","    return os.path.join(REPO, rel.lstrip(\"./\"))\n","\n","def _ensure_dir(p: str):\n","    Path(p).mkdir(parents=True, exist_ok=True)\n","\n","def _subset_for_data_type(data_type, data_path, n, out_dir):\n","    \"\"\"\n","    Create a small subset file matching rewrite_example.get_data().\n","    Returns the new subset path (or original path if n is None/<=0).\n","    \"\"\"\n","    if n is None or n <= 0:\n","        return data_path\n","\n","    src = _abs_repo_path(data_path)\n","    _ensure_dir(out_dir)\n","\n","    if \"microagressions\" in data_path:\n","        df = pd.read_csv(src)\n","        sub = df.head(n)\n","        out = os.path.join(out_dir, os.path.basename(src))\n","        sub.to_csv(out, index=False)\n","        return out\n","\n","    if \"sbf\" in data_path:\n","        df = pd.read_csv(src)\n","        sub = df.head(n)\n","        out = os.path.join(out_dir, os.path.basename(src))\n","        sub.to_csv(out, index=False)\n","        return out\n","\n","    if \"dynabench\" in data_path:\n","        df = pd.read_csv(src)\n","        sub = df.head(n)\n","        out = os.path.join(out_dir, os.path.basename(src))\n","        sub.to_csv(out, index=False)\n","        return out\n","\n","    if any(k in data_path for k in [\"paradetox\", \"jigsaw\"]):\n","        if data_path.endswith(\".txt\"):\n","            with open(src, \"r\") as f:\n","                lines = [s.rstrip(\"\\n\") for s in f.readlines()]\n","            out = os.path.join(out_dir, os.path.basename(src))\n","            with open(out, \"w\") as g:\n","                for s in lines[:n]:\n","                    g.write(s + \"\\n\")\n","            return out\n","        elif data_path.endswith(\".csv\"):\n","            df = pd.read_csv(src).head(n)\n","            out = os.path.join(out_dir, os.path.basename(src))\n","            df.to_csv(out, index=False)\n","            return out\n","\n","    if \"appdia\" in data_path:\n","        df = pd.read_csv(src, sep=\"\\t\").head(n)\n","        out = os.path.join(out_dir, os.path.basename(src))\n","        df.to_csv(out, sep=\"\\t\", index=False)\n","        return out\n","\n","    out = os.path.join(out_dir, os.path.basename(src))\n","    shutil.copy(src, out)\n","    return out"],"metadata":{"id":"ToytrY0SuAjr","executionInfo":{"status":"ok","timestamp":1764559949412,"user_tz":480,"elapsed":3,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"ToytrY0SuAjr","execution_count":8,"outputs":[]},{"cell_type":"code","source":["#@title DecompX-based reranking helpers\n","\n","def _decompx_mask_texts(texts: List[str],\n","                        threshold: float = 0.20,\n","                        batch_size: int = 16) -> List[str]:\n","    \"\"\"\n","    Run DecompX Masker on a list of texts and return masked versions.\n","    \"\"\"\n","    if not texts:\n","        return []\n","\n","    masker = Masker_single()\n","    masked_all = []\n","    for i in tqdm(range(0, len(texts), batch_size),\n","                  desc=\"DecompX masking for reranking\", leave=False):\n","        batch = texts[i:i + batch_size]\n","        batch_out = masker.process_text(sentence=batch, threshold=threshold)\n","        masked_all.extend(batch_out)\n","    # Clean special tokens\n","    cleaned = [\n","        m.replace(\"<s>\", \"\").replace(\"</s>\", \"\").strip()\n","        for m in masked_all\n","    ]\n","    masker.release_model()\n","    return cleaned\n","\n","def _decompx_toxicity_scores(texts: List[str],\n","                             threshold: float = 0.20,\n","                             batch_size: int = 16) -> np.ndarray:\n","    \"\"\"\n","    Score texts by DecompX toxicity:\n","      score = (# of <mask> tokens DecompX inserts) / (# tokens)\n","\n","    Lower score => less DecompX-toxic.\n","    \"\"\"\n","    if not texts:\n","        return np.zeros((0,), dtype=float)\n","\n","    masked = _decompx_mask_texts(texts, threshold=threshold, batch_size=batch_size)\n","    scores = []\n","    for m in masked:\n","        num_masks = len(re.findall(r\"<mask>\", m))\n","        tokens = m.split()\n","        length = max(len(tokens), 1)\n","        scores.append(num_masks / length)\n","    return np.asarray(scores, dtype=float)\n","\n","def rerank_candidates_decompx(\n","    sources: List[str],\n","    candidates: List[List[str]],\n","    threshold: float = 0.20,\n","    batch_size_mask: int = 16,\n","):\n","    \"\"\"\n","    DecompX-based reranking:\n","      - Flatten candidates\n","      - Score each candidate with DecompX\n","      - For each source, choose candidate with lowest score\n","\n","    Returns:\n","      best_idx: np.ndarray (N,) with chosen candidate index per source\n","      details: dict with 'score' matrix shape [N, C]\n","    \"\"\"\n","    N = len(sources)\n","    assert len(candidates) == N, \"candidates length mismatch\"\n","\n","    if N == 0:\n","        return np.array([], dtype=int), {}\n","\n","    C_list = [len(c) for c in candidates]\n","    assert len(set(C_list)) == 1, \"All inputs must have same num_candidates\"\n","    C = C_list[0]\n","    if C == 0:\n","        raise ValueError(\"num_candidates must be >= 1\")\n","\n","    flat_cands = []\n","    flat_src_idx = []\n","    for i, cand_list in enumerate(candidates):\n","        for cand in cand_list:\n","            flat_cands.append(cand)\n","            flat_src_idx.append(i)\n","    flat_src_idx = np.array(flat_src_idx, dtype=int)\n","\n","    scores = _decompx_toxicity_scores(\n","        flat_cands,\n","        threshold=threshold,\n","        batch_size=batch_size_mask,\n","    )  # [N*C]\n","\n","    scores2 = scores.reshape(N, C)\n","    best_idx = np.argmin(scores2, axis=1)\n","\n","    details = {\n","        \"score\": scores2,\n","    }\n","    return best_idx, details"],"metadata":{"id":"-MxQChqOuAnT","executionInfo":{"status":"ok","timestamp":1764559949427,"user_tz":480,"elapsed":12,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"-MxQChqOuAnT","execution_count":9,"outputs":[]},{"cell_type":"code","source":["#@title Evaluation helpers (evaluate_all.py with MeaningBERT + toxicity)\n","def _eval_with_toxicity(base_path, overwrite_eval=False, skip_ref=False,\n","                        tox_threshold=0.5, tox_batch_size=32):\n","    \"\"\"\n","    Call evaluation.evaluate_all on each gen folder.\n","    \"\"\"\n","    import sys as _sys\n","    for folder in os.listdir(base_path):\n","        gen_dir = os.path.join(base_path, folder)\n","        if not os.path.isdir(gen_dir):\n","            continue\n","        orig_path = os.path.join(gen_dir, \"orig.txt\")\n","        gen_path  = os.path.join(gen_dir, \"gen.txt\")\n","        out_stats = os.path.join(gen_dir, \"gen_stats.txt\")\n","        if not (os.path.exists(orig_path) and os.path.exists(gen_path)):\n","            continue\n","        if os.path.exists(out_stats) and not overwrite_eval:\n","            continue\n","\n","        env = os.environ.copy()\n","        env[\"PYTHONPATH\"] = REPO + (\":\" + env.get(\"PYTHONPATH\",\"\") if env.get(\"PYTHONPATH\") else \"\")\n","        cmd = [\n","            _sys.executable, \"-m\", \"evaluation.evaluate_all\",\n","            \"--orig_path\", orig_path,\n","            \"--gen_path\",  gen_path,\n","            \"--tox_threshold\", str(tox_threshold),\n","            \"--tox_batch_size\", str(tox_batch_size),\n","        ]\n","        if skip_ref:\n","            cmd.append(\"--skip_ref\")\n","        print(\"Eval:\", \" \".join(cmd))\n","        res = run(cmd, cwd=REPO, env=env, stdout=PIPE, stderr=PIPE, text=True)\n","        if res.returncode != 0:\n","            print(res.stdout)\n","            print(res.stderr)\n","            res.check_returncode()\n","\n","def _safe_float(x):\n","    try:\n","        return float(x)\n","    except Exception:\n","        return float('nan')\n","\n","def _read_stats_file(path):\n","    out = {}\n","    with open(path, \"r\") as f:\n","        for line in f:\n","            if \":\" not in line:\n","                continue\n","            k, v = line.strip().split(\": \", 1)\n","            k = k.replace(\"(skipped)\", \"\").strip().lower()\n","            out[k] = _safe_float(v)\n","    return out\n","\n","def _aggregate_eval_csv(output_folder, data_type, base_out_dir):\n","    \"\"\"\n","    Aggregate eval metrics for LLM-masking + LLM-infilling + DecompX reranking.\n","\n","    Layout (absolute base_out_dir):\n","      base_out_dir/\n","        └── {data_type}/\n","            └── LLM_Mask_LLM_DecompX/\n","                └── {run_folder}/\n","                    └── gen_stats.txt\n","\n","    threshold column kept as fixed label (=0.20) for compatibility.\n","    \"\"\"\n","    rows = []\n","\n","    mask_dir = \"LLM_Mask_LLM_DecompX\"\n","    base_path = os.path.join(base_out_dir, data_type, mask_dir)\n","    if not os.path.isdir(base_path):\n","        print(\"No evaluation directory found:\", base_path)\n","        return\n","\n","    for folder in os.listdir(base_path):\n","        gen_dir = os.path.join(base_path, folder)\n","        stats_path = os.path.join(gen_dir, \"gen_stats.txt\")\n","        if not os.path.exists(stats_path):\n","            continue\n","        s = _read_stats_file(stats_path)\n","        rows.append({\n","            \"threshold\":        0.20,  # label only, not used by this pipeline\n","            \"folder\":           folder,\n","            \"bertscore\":        s.get(\"bertscore\", np.nan),\n","            \"meaningbert\":      s.get(\"meaningbert\", np.nan),\n","            \"bleu4\":            s.get(\"bleu4\", np.nan),\n","            \"perplexity_gen\":   s.get(\"perplexity gen\", np.nan),\n","            \"perplexity_orig\":  s.get(\"perplexity orig\", np.nan),\n","            \"toxicity_gen\":     s.get(\"toxicity gen\", np.nan),\n","            \"toxicity_orig\":    s.get(\"toxicity orig\", np.nan),\n","        })\n","\n","    if rows:\n","        cols = [\n","            \"threshold\", \"folder\",\n","            \"bertscore\", \"meaningbert\", \"bleu4\",\n","            \"perplexity_gen\", \"perplexity_orig\",\n","            \"toxicity_gen\", \"toxicity_orig\",\n","        ]\n","        df = pd.DataFrame(rows)\n","        df = df[cols]\n","        out_csv = os.path.join(base_out_dir, data_type, f\"{data_type}.csv\")\n","        df.to_csv(out_csv, index=False)\n","        print(\"Wrote summary CSV:\", out_csv)\n","    else:\n","        print(\"No evaluation files found to summarize.\")"],"metadata":{"id":"u-7I09Uvqb8c","executionInfo":{"status":"ok","timestamp":1764559949433,"user_tz":480,"elapsed":3,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"u-7I09Uvqb8c","execution_count":10,"outputs":[]},{"cell_type":"code","source":["#@title Shared LLM loader (Mistral-7B-Instruct) for masking + infilling\n","USE_LLM_GPU = True\n","DEVICE_LLM = torch.device(\"cuda\" if USE_LLM_GPU and torch.cuda.is_available() else \"cpu\")\n","print(\"LLM device (mask + infill):\", DEVICE_LLM)\n","\n","LLM_MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n","_LLM_TOKENIZER = None\n","_LLM_MODEL = None\n","\n","def _lazy_load_llm():\n","    global _LLM_MODEL, _LLM_TOKENIZER\n","    if _LLM_MODEL is not None and _LLM_TOKENIZER is not None:\n","        return\n","    print(f\"Loading LLM (Mistral-7B-Instruct): {LLM_MODEL_NAME} on {DEVICE_LLM} ...\")\n","    _LLM_TOKENIZER = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n","    _LLM_MODEL = AutoModelForCausalLM.from_pretrained(\n","        LLM_MODEL_NAME,\n","        torch_dtype=torch.float16 if DEVICE_LLM.type == \"cuda\" else torch.float32,\n","        device_map=None,\n","    ).to(DEVICE_LLM)\n","    _LLM_MODEL.eval()\n","    print(\"LLM loaded.\")\n","\n","\n","def _extract_bracket_content(text: str) -> str:\n","    \"\"\"\n","    Extract content inside the first [ ... ] block.\n","    If there is an opening '[' but no closing ']', take everything after '['.\n","    Otherwise, fall back to the whole string.\n","    \"\"\"\n","    text = text.strip()\n","\n","    m = re.search(r\"\\[([^\\]]*)\\]\", text, flags=re.DOTALL)\n","    if m:\n","        return m.group(1).strip()\n","\n","    if \"[\" in text:\n","        return text.split(\"[\", 1)[1].strip()\n","\n","    return text"],"metadata":{"id":"nAVpxvu2b_Me","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764559949452,"user_tz":480,"elapsed":17,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"86afc61d-21c9-42ba-dcf4-1895c1423c8b"},"id":"nAVpxvu2b_Me","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["LLM device (mask + infill): cuda\n"]}]},{"cell_type":"code","source":["#@title LLM Masking (MetaDetox-style) using Mistral-7B-Instruct\n","\n","MASK_SYSTEM_PROMPT = \"\"\"You are a helpful assistant trained to make toxic or offensive sentences safer\n","by MASKING toxic words with the special token <mask>, while keeping the original sentence\n","structure and wording as much as possible.\n","\n","You must behave like a MASKER, not a full rewriter.\n","\n","Your task:\n","1. Identify toxic, offensive, or profane words or short phrases.\n","2. For each toxic span, replace the entire span with a single <mask> token.\n","3. There may be multiple toxic spans in one sentence, so multiple <mask> tokens are allowed.\n","4. If several neighboring words are toxic, you must still use only a single <mask> token\n","   in that place. In other words, if you would place \"<mask> <mask>\" or a longer sequence\n","   of <mask> tokens, collapse them into a single <mask> so that there are never multiple\n","   <mask> tokens in a row.\n","5. Do NOT rewrite, paraphrase, or summarize the sentence.\n","6. Do NOT add, remove, or reorder non-toxic words or punctuation.\n","7. Keep punctuation and spacing as close to the original as possible.\n","8. If there is no toxic content, return the sentence unchanged.\n","\n","Output rules (format is very strict):\n","- ONLY return the final masked sentence inside ONE pair of square brackets, like:\n","  [This is a <mask> example.]\n","- Do NOT print anything before or after the brackets.\n","- Do NOT add explanations, comments, or extra lines.\n","- Do NOT include any language tags or metadata.\n","- Do NOT include additional '[' or ']' characters inside the sentence.\n","\"\"\"\n","\n","MASK_FEW_SHOT = \"\"\"Toxic Sentence: You're such a stupid idiot, nobody wants to hear your crap.\n","Step 1 - Identify toxic words: \"stupid idiot\", \"crap\"\n","Step 2 - Mask toxic words (do NOT rewrite the rest):\n","You're such a <mask>, nobody wants to hear your <mask>.\n","Final Output: [You're such a <mask>, nobody wants to hear your <mask>.]\"\"\"\n","\n","def _postprocess_llm_mask(masked_text: str) -> str:\n","    \"\"\"\n","    Clean up LLM-masked sentences:\n","      - remove stray leading/trailing brackets,\n","      - normalize whitespace,\n","      - normalize <mask> token casing,\n","      - collapse runs of <mask> into a single <mask>.\n","    \"\"\"\n","    s = masked_text.strip()\n","\n","    if s.startswith(\"[\") and s.endswith(\"]\") and len(s) > 2:\n","        s = s[1:-1].strip()\n","    else:\n","        if s.startswith(\"[\"):\n","            s = s[1:].strip()\n","        if s.endswith(\"]\"):\n","            s = s[:-1].strip()\n","\n","    s = re.sub(r\"\\s+\", \" \", s).strip()\n","    s = re.sub(r\"<\\s*mask\\s*>\", \"<mask>\", s, flags=re.IGNORECASE)\n","    s = re.sub(r\"(?:\\s*<mask>\\s*){2,}\", \" <mask> \", s)\n","    s = re.sub(r\"\\s+\", \" \", s).strip()\n","\n","    if not s:\n","        return masked_text.strip()\n","\n","    return s\n","\n","@torch.no_grad()\n","def llm_mask_sentences(sentences: List[str]) -> List[str]:\n","    \"\"\"\n","    Use Mistral-7B-Instruct as a masker:\n","    input: toxic sentence\n","    output: same sentence but toxic words replaced by <mask>.\n","    \"\"\"\n","    _lazy_load_llm()\n","    masked = []\n","    for s in tqdm(sentences, desc=\"LLM masking (Mistral)\", leave=False):\n","        messages = [\n","            {\n","                \"role\": \"system\",\n","                \"content\": MASK_SYSTEM_PROMPT + \"\\n\\nBelow is an example:\\n\" + MASK_FEW_SHOT,\n","            },\n","            {\n","                \"role\": \"user\",\n","                \"content\": f\"Toxic Sentence: {s}\\nFinal Output:\",\n","            },\n","        ]\n","        try:\n","            prompt = _LLM_TOKENIZER.apply_chat_template(\n","                messages,\n","                tokenize=False,\n","                add_generation_prompt=True,\n","            )\n","        except Exception:\n","            prompt = (\n","                MASK_SYSTEM_PROMPT\n","                + \"\\n\\nExample:\\n\"\n","                + MASK_FEW_SHOT\n","                + \"\\n\\nToxic Sentence: \"\n","                + s\n","                + \"\\nFinal Output:\"\n","            )\n","\n","        inputs = _LLM_TOKENIZER(prompt, return_tensors=\"pt\").to(DEVICE_LLM)\n","        gen = _LLM_MODEL.generate(\n","            **inputs,\n","            max_new_tokens=64,\n","            do_sample=False,\n","            temperature=0.0,\n","            pad_token_id=_LLM_TOKENIZER.eos_token_id,\n","        )\n","        gen_text = _LLM_TOKENIZER.decode(\n","            gen[0][inputs[\"input_ids\"].shape[1]:],\n","            skip_special_tokens=True,\n","        )\n","\n","        masked_text = _extract_bracket_content(gen_text)\n","        masked_text = _postprocess_llm_mask(masked_text)\n","\n","        if not masked_text:\n","            masked_text = s\n","        masked.append(masked_text)\n","\n","    return masked"],"metadata":{"id":"nG355GLlqcp-","executionInfo":{"status":"ok","timestamp":1764559949548,"user_tz":480,"elapsed":94,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"nG355GLlqcp-","execution_count":12,"outputs":[]},{"cell_type":"code","source":["#@title LLM infilling (Mistral-7B-Instruct) — fills <mask> tokens\n","\n","INFILL_SYSTEM_PROMPT = \"\"\"You are a helpful assistant trained to make toxic or offensive sentences\n","more polite and respectful by INFILLING the special token <mask>.\n","\n","You are NOT a free rewriter. You must keep all non-masked text as close as possible\n","to the given masked sentence.\n","\n","You are given two inputs:\n","1) Toxic Sentence: the original toxic sentence.\n","2) Masked Sentence: the same sentence, where toxic spans are replaced with <mask>.\n","\n","Your task:\n","1. For each <mask> token in the Masked Sentence, replace it with a short, non-toxic\n","   word or phrase that fits the context and preserves the meaning of the Toxic Sentence.\n","2. Do NOT modify any other words or punctuation outside the <mask> spans, unless a very\n","   small change is needed to fix grammar or agreement.\n","3. Preserve the original meaning and intent as much as possible, but make the sentence\n","   safe and respectful.\n","4. Keep the language the same as the original (do NOT translate).\n","\n","Output rules (VERY STRICT):\n","- ONLY return the final detoxified sentence with all <mask> tokens filled.\n","- Wrap the final sentence in exactly ONE pair of square brackets, e.g.:\n","  [Detoxified sentence here.]\n","- Do NOT include the Toxic Sentence or Masked Sentence in your output.\n","- Do NOT add explanations, comments, or extra lines.\n","- Do NOT include any other '[' or ']' characters.\n","\"\"\"\n","\n","INFILL_FEW_SHOT = \"\"\"Toxic Sentence: You're such a stupid idiot, nobody wants to hear your crap.\n","Masked Sentence: You're such a <mask>, nobody wants to hear your <mask>.\n","Step 1 - Decide safe replacements for each <mask>: \"rude person\", \"opinion\"\n","Step 2 - Infill the masked sentence, keeping all other words the same:\n","You're such a rude person, nobody wants to hear your opinion.\n","Final Output: [You're such a rude person, nobody wants to hear your opinion.]\"\"\"\n","\n","def _postprocess_llm_infill(text: str) -> str:\n","    \"\"\"\n","    Post-process LLM-infilling output:\n","      - remove stray outer brackets if still present,\n","      - normalize whitespace,\n","      - remove any leftover <mask> tokens.\n","    \"\"\"\n","    s = text.strip()\n","\n","    if s.startswith(\"[\") and s.endswith(\"]\") and len(s) > 2:\n","        s = s[1:-1].strip()\n","    else:\n","        if s.startswith(\"[\"):\n","            s = s[1:].strip()\n","        if s.endswith(\"]\"):\n","            s = s[:-1].strip()\n","\n","    s = re.sub(r\"\\s+\", \" \", s).strip()\n","\n","    s = s.replace(\"<mask>\", \" \")\n","    s = re.sub(r\"\\s+\", \" \", s).strip()\n","\n","    if not s:\n","        return text.strip()\n","    return s\n","\n","@torch.no_grad()\n","def llm_infill_candidates(\n","    sources: List[str],\n","    masked: List[str],\n","    num_candidates: int = 3,\n","    temperature: float = 0.7,\n","    top_p: float = 0.95,\n","    max_new_tokens: int = 64,\n","    sample: bool = True,\n",") -> List[List[str]]:\n","    \"\"\"\n","    For each (source, masked) pair, generate `num_candidates` detoxified sentences\n","    by infilling the <mask> tokens using Mistral-7B-Instruct.\n","\n","    Returns:\n","        candidates: list[list[str]] with shape [N][num_candidates]\n","    \"\"\"\n","    assert len(sources) == len(masked), \"sources and masked length mismatch\"\n","    if num_candidates < 1:\n","        raise ValueError(\"num_candidates must be >= 1\")\n","\n","    _lazy_load_llm()\n","    all_cands: List[List[str]] = []\n","\n","    for src, msk in tqdm(\n","        list(zip(sources, masked)),\n","        desc=\"LLM infilling (Mistral)\",\n","        leave=False,\n","    ):\n","        messages = [\n","            {\n","                \"role\": \"system\",\n","                \"content\": INFILL_SYSTEM_PROMPT + \"\\n\\nHere is an example:\\n\" + INFILL_FEW_SHOT,\n","            },\n","            {\n","                \"role\": \"user\",\n","                \"content\": (\n","                    f\"Toxic Sentence: {src}\\n\"\n","                    f\"Masked Sentence: {msk}\\n\"\n","                    f\"Final Output:\"\n","                ),\n","            },\n","        ]\n","        try:\n","            prompt = _LLM_TOKENIZER.apply_chat_template(\n","                messages,\n","                tokenize=False,\n","                add_generation_prompt=True,\n","            )\n","        except Exception:\n","            prompt = (\n","                INFILL_SYSTEM_PROMPT\n","                + \"\\n\\nExample:\\n\"\n","                + INFILL_FEW_SHOT\n","                + \"\\n\\nToxic Sentence: \"\n","                + src\n","                + \"\\nMasked Sentence: \"\n","                + msk\n","                + \"\\nFinal Output:\"\n","            )\n","\n","        inputs = _LLM_TOKENIZER(prompt, return_tensors=\"pt\").to(DEVICE_LLM)\n","        input_len = inputs[\"input_ids\"].shape[1]\n","\n","        gen = _LLM_MODEL.generate(\n","            **inputs,\n","            max_new_tokens=max_new_tokens,\n","            do_sample=sample,\n","            temperature=float(temperature) if sample else 0.0,\n","            top_p=top_p,\n","            num_return_sequences=num_candidates,\n","            pad_token_id=_LLM_TOKENIZER.eos_token_id,\n","        )\n","\n","        cand_list = []\n","        for idx in range(num_candidates):\n","            gen_text = _LLM_TOKENIZER.decode(\n","                gen[idx][input_len:],\n","                skip_special_tokens=True,\n","            )\n","            cleaned = _extract_bracket_content(gen_text)\n","            cleaned = _postprocess_llm_infill(cleaned)\n","            if not cleaned:\n","                cleaned = src\n","            cand_list.append(cleaned)\n","\n","        all_cands.append(cand_list)\n","\n","    return all_cands"],"metadata":{"id":"PDRoBhdM8461","executionInfo":{"status":"ok","timestamp":1764559949584,"user_tz":480,"elapsed":33,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"PDRoBhdM8461","execution_count":13,"outputs":[]},{"cell_type":"code","source":["#@title Helpers for folder naming\n","def _bool2str(x: bool) -> str:\n","    return \"T\" if x else \"F\"\n","\n","def _build_run_folder_name(\n","    llm_temperature: float,\n","    llm_top_p: float,\n","    llm_sample: bool,\n","    num_candidates: int,\n","    max_new_tokens: int,\n","    decompx_threshold: float,\n","):\n","    \"\"\"\n","    Build a folder name encoding LLM + DecompX hyperparameters.\n","    \"\"\"\n","    return (\n","        f\"llmtemp{llm_temperature}_topp{llm_top_p}_\"\n","        f\"sample{_bool2str(llm_sample)}_\"\n","        f\"nc{num_candidates}_\"\n","        f\"maxntok{max_new_tokens}_\"\n","        f\"dxth{decompx_threshold}\"\n","    )"],"metadata":{"id":"SLDKLRVBcKCy","executionInfo":{"status":"ok","timestamp":1764559949601,"user_tz":480,"elapsed":15,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"SLDKLRVBcKCy","execution_count":14,"outputs":[]},{"cell_type":"code","source":["#@title `detoxify()` — LLM masking + LLM infilling + DecompX reranking + optional eval\n","\n","def detoxify(\n","    data_type: str = \"paradetox\",\n","    output_folder: str = \"colab_run_llm_mask_infill_decompx\",\n","    echo: bool = False,\n","    num_examples: int = 100,       # None = full dataset\n","    overwrite_gen: bool = False,\n","    run_eval: bool = False,\n","    overwrite_eval: bool = False,\n","    skip_ref_eval: bool = False,\n","    # LLM infilling hyperparameters\n","    num_candidates: int = 3,\n","    llm_temperature: float = 0.7,\n","    llm_top_p: float = 0.95,\n","    llm_max_new_tokens: int = 64,\n","    llm_sample: bool = True,\n","    # DecompX reranking\n","    decompx_threshold: float = 0.20,\n","    decompx_batch_size_mask: int = 16,\n","):\n","    \"\"\"\n","    Run XDetox with:\n","      - LLM masking (Mistral-7B-Instruct),\n","      - LLM infilling (Mistral-7B-Instruct),\n","      - DecompX-based reranking (Masker_single, threshold=decompx_threshold),\n","      - evaluation via evaluation/evaluate_all.py (BLEU/BERTScore/MeaningBERT/PPL/Toxicity).\n","\n","    DecompX is used only in the reranking stage.\n","    \"\"\"\n","    assert data_type in data_configs, f\"Unknown data_type: {data_type}\"\n","    cfg = data_configs[data_type].copy()\n","\n","    if num_candidates < 1:\n","        raise ValueError(\"num_candidates must be >= 1\")\n","\n","    base_out_rel = os.path.join(\"data\", \"model_outputs\", output_folder)\n","    base_out_abs = os.path.join(REPO, base_out_rel)\n","    _ensure_dir(base_out_abs)\n","\n","    # subset path (file)\n","    original_data_path = cfg[\"data_path\"]\n","    subset_dir = os.path.join(REPO, \"datasets\", \"_subsets\", data_type)\n","    _ensure_dir(subset_dir)\n","    subset_path = _subset_for_data_type(\n","        data_type, original_data_path, num_examples, subset_dir\n","    )\n","\n","    # Load inputs\n","    args_data = _argparse.Namespace(data_type=data_type, data_path=subset_path)\n","    inputs = rx.get_data(args_data)\n","    num_inputs = len(inputs)\n","\n","    if echo:\n","        print(\"=\" * 80)\n","        print(f\"[echo] Dataset: {data_type}\")\n","        print(f\"[echo] Subset path: {subset_path}\")\n","        print(f\"[echo] Output base: {base_out_abs}\")\n","        print(f\"[echo] Number of examples to detoxify: {num_inputs}\")\n","        print(f\"[echo] num_candidates per input: {num_candidates}\")\n","        print(f\"[echo] DecompX threshold (reranking): {decompx_threshold}\")\n","        print(\"\\n[echo] Example inputs (first up to 3):\")\n","        for i, s in enumerate(inputs[:3]):\n","            print(f\"  input[{i}]: {s}\")\n","        print(\"=\" * 80)\n","\n","    # Directory for this pipeline\n","    mask_dir = \"LLM_Mask_LLM_DecompX\"\n","    cur_rel = os.path.join(base_out_rel, data_type, mask_dir)\n","    cur_abs = os.path.join(REPO, cur_rel)\n","    _ensure_dir(cur_abs)\n","\n","    masked_file = os.path.join(cur_abs, \"masked_inputs.txt\")\n","\n","    # Step 1: LLM masking (reuse cached file if available)\n","    if not os.path.exists(masked_file):\n","        print(\"Running LLM masking (Mistral) to create masked_inputs.txt ...\")\n","        masked_inputs = llm_mask_sentences(inputs)\n","        masked_inputs = [re.sub(r\"\\s+\", \" \", d).strip() for d in masked_inputs]\n","        with open(masked_file, \"w\") as f:\n","            for d in masked_inputs:\n","                f.write(d + \"\\n\")\n","    else:\n","        with open(masked_file, \"r\") as f:\n","            masked_inputs = [s.strip() for s in f.readlines()]\n","        print(\"Reusing existing masked_inputs.txt\")\n","\n","    assert len(masked_inputs) == len(inputs), \"Masked vs inputs mismatch\"\n","\n","    if echo:\n","        print(\"\\n[echo] Example LLM-masked inputs (first up to 3):\")\n","        for i, m in enumerate(masked_inputs[:3]):\n","            print(f\"  masked[{i}]: {m}\")\n","\n","    # Build run folder name for this config\n","    run_folder = _build_run_folder_name(\n","        llm_temperature=llm_temperature,\n","        llm_top_p=llm_top_p,\n","        llm_sample=llm_sample,\n","        num_candidates=num_candidates,\n","        max_new_tokens=llm_max_new_tokens,\n","        decompx_threshold=decompx_threshold,\n","    )\n","    final_abs = os.path.join(cur_abs, run_folder)\n","    _ensure_dir(final_abs)\n","    orig_txt = os.path.join(final_abs, \"orig.txt\")\n","    gen_txt = os.path.join(final_abs, \"gen.txt\")\n","\n","    # If outputs already exist and we are not overwriting, just load them\n","    if os.path.exists(gen_txt) and not overwrite_gen:\n","        print(\"Generation already exists at:\", gen_txt, \"— skipping generation.\")\n","        with open(gen_txt, \"r\") as f:\n","            best_generations = [s.strip() for s in f.readlines()]\n","        if echo:\n","            print(\"\\n[echo] Example detoxified outputs (first up to 3):\")\n","            for i in range(min(3, len(best_generations))):\n","                print(f\"  detox[{i}]: {best_generations[i]}\")\n","    else:\n","        # Step 2: LLM infilling\n","        print(f\"LLM infilling: generating {num_candidates} candidates per input (sampling={llm_sample})\")\n","        all_candidates = llm_infill_candidates(\n","            sources=inputs,\n","            masked=masked_inputs,\n","            num_candidates=num_candidates,\n","            temperature=llm_temperature,\n","            top_p=llm_top_p,\n","            max_new_tokens=llm_max_new_tokens,\n","            sample=llm_sample,\n","        )\n","\n","        # Free LLM before DecompX scoring\n","        global _LLM_MODEL, _LLM_TOKENIZER\n","        try:\n","            del _LLM_MODEL\n","            del _LLM_TOKENIZER\n","        except Exception:\n","            pass\n","        _LLM_MODEL = None\n","        _LLM_TOKENIZER = None\n","        if torch.cuda.is_available() and DEVICE_LLM.type == \"cuda\":\n","            torch.cuda.empty_cache()\n","\n","        # Step 3: DecompX-based reranking\n","        print(f\"DecompX reranking (threshold={decompx_threshold:.2f}) ...\")\n","        best_idx, details = rerank_candidates_decompx(\n","            sources=inputs,\n","            candidates=all_candidates,\n","            threshold=decompx_threshold,\n","            batch_size_mask=decompx_batch_size_mask,\n","        )\n","        best_generations = [\n","            all_candidates[i][best_idx[i]] for i in range(len(inputs))\n","        ]\n","\n","        # Save orig + chosen gen\n","        with open(orig_txt, \"w\") as f:\n","            for l in inputs:\n","                f.write(re.sub(r\"\\s+\", \" \", l).strip() + \"\\n\")\n","        with open(gen_txt, \"w\") as f:\n","            for l in best_generations:\n","                f.write(re.sub(r\"\\s+\", \" \", l).strip() + \"\\n\")\n","\n","        print(\"Saved:\", orig_txt)\n","        print(\"Saved:\", gen_txt)\n","\n","        if echo:\n","            print(\"\\n[echo] Example detoxified outputs (first up to 3):\")\n","            for i in range(min(3, len(best_generations))):\n","                print(f\"  detox[{i}]: {best_generations[i]}\")\n","\n","    # Optional evaluation\n","    if run_eval:\n","        base_path = os.path.join(base_out_abs, data_type, mask_dir)\n","        _eval_with_toxicity(\n","            base_path,\n","            overwrite_eval=overwrite_eval,\n","            skip_ref=skip_ref_eval,\n","            tox_threshold=0.5,\n","            tox_batch_size=32,\n","        )\n","        _aggregate_eval_csv(\n","            output_folder,\n","            data_type,\n","            os.path.join(REPO, \"data\", \"model_outputs\", output_folder),\n","        )\n","\n","        if echo:\n","            stats_path = os.path.join(final_abs, \"gen_stats.txt\")\n","            if os.path.exists(stats_path):\n","                stats = _read_stats_file(stats_path)\n","                print(\"\\n[echo] Evaluation metrics for this run:\")\n","                metric_keys = [\n","                    (\"bertscore\", \"BERTScore\"),\n","                    (\"meaningbert\", \"MeaningBERT\"),\n","                    (\"bleu4\", \"BLEU-4\"),\n","                    (\"perplexity gen\", \"Perplexity (gen)\"),\n","                    (\"perplexity orig\", \"Perplexity (orig)\"),\n","                    (\"toxicity gen\", \"Toxicity (gen)\"),\n","                    (\"toxicity orig\", \"Toxicity (orig)\"),\n","                ]\n","                for key, label in metric_keys:\n","                    val = stats.get(key, None)\n","                    if isinstance(val, float) and math.isnan(val):\n","                        continue\n","                    if val is None:\n","                        continue\n","                    print(f\"  {label}: {val:.4f}\")\n","            else:\n","                print(\"\\n[echo] gen_stats.txt not found for this run; no metrics to print.\")"],"metadata":{"id":"U5oOUWRYuA6E","executionInfo":{"status":"ok","timestamp":1764559949606,"user_tz":480,"elapsed":2,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"U5oOUWRYuA6E","execution_count":15,"outputs":[]},{"cell_type":"code","source":["#@title Example run — paradetox, LLM masking + LLM infilling + DecompX reranking\n","\n","# detoxify(\n","#     data_type=\"paradetox\",\n","#     output_folder=\"colab_run_llm_mask_infill_decompx_demo_50_examples\",\n","#     echo=True,\n","#     num_examples=50,           # small subset for testing\n","#     run_eval=True,             # BLEU/BERTScore/MeaningBERT/PPL/Toxicity\n","#     overwrite_gen=True,\n","#     overwrite_eval=True,\n","#     skip_ref_eval=False,\n","#     num_candidates=10,         # LLM candidates per input\n","#     llm_temperature=0.7,\n","#     llm_top_p=0.95,\n","#     llm_max_new_tokens=64,\n","#     llm_sample=True,\n","#     decompx_threshold=0.20,\n","#     decompx_batch_size_mask=16,\n","# )"],"metadata":{"id":"u5LlySYquA9g","collapsed":true,"executionInfo":{"status":"ok","timestamp":1764559949611,"user_tz":480,"elapsed":3,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"u5LlySYquA9g","execution_count":16,"outputs":[]},{"cell_type":"code","source":["detoxify(\n","    data_type=\"paradetox\",\n","    output_folder=\"XDetox_w_LLM-Masking_LLM-Infilling_DecompX-Reranking_Pipeline\",\n","    echo=True,\n","    num_examples=1000,\n","    run_eval=True,             # BLEU/BERTScore/MeaningBERT/PPL/Toxicity\n","    overwrite_gen=True,\n","    overwrite_eval=True,\n","    skip_ref_eval=False,\n","    num_candidates=10,         # LLM candidates per input\n","    llm_temperature=0.7,\n","    llm_top_p=0.95,\n","    llm_max_new_tokens=64,\n","    llm_sample=True,\n","    decompx_threshold=0.20,\n","    decompx_batch_size_mask=16,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":934,"referenced_widgets":["e62b3afb1b0e4c0bbc2e3a8d8738e9e6","6d9a38d293e943259991bd488a1cda2f","ca111dd05a2d4d04953ebc54ccf52969","6b210ff8485149e0a3e3a90f390e8cf4","695084a4635447b8a22a1462ac8c6ac7","1e75d69a538f425b8ddcb0e0a79e0fe6","00ffaf237ac94816a381c5f1ad6c811e","db19e2602e544d0893b1019063f038c6","5a7faeeded5444e1891b25d14d580cbe","608a4af3cf0d43049a1930cff71a32fa","03ce31b14a284e8a8b06c471ff710a57","faef3457741e4f5ea2a17df747bbf85c","f9bf9567d04f43279ead89ad756030d0","1db1ff1a90534808aea9d3592247f1be","09f17fee7c424d6f9e1cc0960346f639","09485d3698f54ecbb98eb2fcacc8fb76","3ebbea1c5a4f471db630f5520ec9e71b","e95863c631dd4f6aa5c87d2ab923596f","4e754d049cf2423fa7860995623cefca","76f12d8d682b4ea98e98fd5c61281297","982534d299bf42509d629f72eed49719","6eb68fbf44814b1abfe43b2fbe8f6e14","034f40fe7a504ab7b4adbc7567e4b8a0","7e04f5cdac6742aa86173a132f478e03","61ce3aa1ba764caa9ee7bae15c6aec2d","41e4bc673b0745bea490beab43e2c06b","1459507584b7484d84cfaca95b9b2e17","25fa85af1379471b857619b43f145d5a","56b8c8e0aeaa45fb9a61d6f8f0dbaf76","76a82a9cda844e11b94442e9b2f7e84f","834e3510609b45b593e2f281c5cdbd4a","f2bf68bf8a79428899afe37ab431202a","7f20fb69f3074dbbae635563df858469","372c39a0a8274706a20d91eb20e8ca19","488d738942224b1c90992536f7783144","ffecc1166f4e4308a242817c3e4e1fef","fa8b7bfd07484ce8bda7bcec816470c7","20eb2903da9e4b0ca08fbe2d47b561fa","d825002a5cc5456f871726a5072604f8","7780d6d63f2b4901b0c471a58f65a62c","fc0e9441394345c38d0e23dd916c1ffc","7295a112f5cb4b75b9fd30d7ecb6e96f","5a3fc1ded11d46739eb7f4a64254062d","c8823295cb444c87a0bb225584c1fbc4","1fb4df54ecb748dca844f683c38d90d1","4871aef7967c42288555d008cad97899","b50af3c2d3fa4a25ac1288f57409b491","e55d45de473b4c41badf8edcac7ee840","d8f5b27dede14ff0bc32847612b8a12d","a29aae4f8d4f478d9f460bb933a1e39b","d94e0ebe956140fbacc6ed30b2685998","ac1102b44fa54fb4a6606478e96af0d7","f1d6d10dd1f6433fa3dc68c4c069a989","34eb0136fb3e487793f1c574e7d835a6","4458e359953e4cdd81d1b072ae16d149"]},"id":"O2S9fX-adatU","executionInfo":{"status":"ok","timestamp":1764563595492,"user_tz":480,"elapsed":3645879,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"9fb2b867-05d7-4aa5-f1a4-4908abbc9a19"},"id":"O2S9fX-adatU","execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","[echo] Dataset: paradetox\n","[echo] Subset path: /content/drive/MyDrive/w266 - Project/XDetox/datasets/_subsets/paradetox/test_toxic_parallel.txt\n","[echo] Output base: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_LLM-Infilling_DecompX-Reranking_Pipeline\n","[echo] Number of examples to detoxify: 671\n","[echo] num_candidates per input: 10\n","[echo] DecompX threshold (reranking): 0.2\n","\n","[echo] Example inputs (first up to 3):\n","  input[0]: . or the loud ass one - thousand ton beast roaring towards you howling its horn .\n","  input[1]: mandated  and \" right fucking now \" would be good .\n","  input[2]: * neither * of my coworkers gave a shit when it came time to ditch mitch . ugh .\n","================================================================================\n","Running LLM masking (Mistral) to create masked_inputs.txt ...\n","Loading LLM (Mistral-7B-Instruct): mistralai/Mistral-7B-Instruct-v0.2 on cuda ...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e62b3afb1b0e4c0bbc2e3a8d8738e9e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"faef3457741e4f5ea2a17df747bbf85c"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["LLM loaded.\n"]},{"output_type":"display_data","data":{"text/plain":["LLM masking (Mistral):   0%|          | 0/671 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"034f40fe7a504ab7b4adbc7567e4b8a0"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","[echo] Example LLM-masked inputs (first up to 3):\n","  masked[0]: . or the loud <mask> one - thousand ton beast roaring towards you howling its horn .\n","  masked[1]: mandated and \"<mask> right <mask> now <mask> \" would be good .\n","  masked[2]: neither of my coworkers gave a <mask> when it came time to ditch mitch . ugh .\n","LLM infilling: generating 10 candidates per input (sampling=True)\n"]},{"output_type":"display_data","data":{"text/plain":["LLM infilling (Mistral):   0%|          | 0/671 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"372c39a0a8274706a20d91eb20e8ca19"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["DecompX reranking (threshold=0.20) ...\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at s-nlp/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"display_data","data":{"text/plain":["DecompX masking for reranking:   0%|          | 0/420 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fb4df54ecb748dca844f683c38d90d1"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:1052: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Saved: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_LLM-Infilling_DecompX-Reranking_Pipeline/paradetox/LLM_Mask_LLM_DecompX/llmtemp0.7_topp0.95_sampleT_nc10_maxntok64_dxth0.2/orig.txt\n","Saved: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_LLM-Infilling_DecompX-Reranking_Pipeline/paradetox/LLM_Mask_LLM_DecompX/llmtemp0.7_topp0.95_sampleT_nc10_maxntok64_dxth0.2/gen.txt\n","\n","[echo] Example detoxified outputs (first up to 3):\n","  detox[0]: or the loud obnoxious one - thousand ton beast roaring towards you howling its horn.\n","  detox[1]: mandated and \"immediate\" \"soon\" \"would be good\".\n","  detox[2]: neither of my coworkers showed concern when it came time to let Mitch go . ugh.\n","Eval: /usr/bin/python3 -m evaluation.evaluate_all --orig_path /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_LLM-Infilling_DecompX-Reranking_Pipeline/paradetox/LLM_Mask_LLM_DecompX/llmtemp0.7_topp0.95_sampleT_nc10_maxntok64_dxth0.2/orig.txt --gen_path /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_LLM-Infilling_DecompX-Reranking_Pipeline/paradetox/LLM_Mask_LLM_DecompX/llmtemp0.7_topp0.95_sampleT_nc10_maxntok64_dxth0.2/gen.txt --tox_threshold 0.5 --tox_batch_size 32\n","Wrote summary CSV: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_LLM-Infilling_DecompX-Reranking_Pipeline/paradetox/paradetox.csv\n","\n","[echo] Evaluation metrics for this run:\n","  BERTScore: 0.9313\n","  MeaningBERT: 62.5550\n","  BLEU-4: 81.5360\n","  Perplexity (gen): 149.2200\n","  Perplexity (orig): 273.7500\n","  Toxicity (gen): 0.1810\n","  Toxicity (orig): 0.9253\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"VgZRbEBED8w4","executionInfo":{"status":"ok","timestamp":1764563595501,"user_tz":480,"elapsed":4,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"VgZRbEBED8w4","execution_count":17,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"A100","machine_shape":"hm"},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"e62b3afb1b0e4c0bbc2e3a8d8738e9e6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6d9a38d293e943259991bd488a1cda2f","IPY_MODEL_ca111dd05a2d4d04953ebc54ccf52969","IPY_MODEL_6b210ff8485149e0a3e3a90f390e8cf4"],"layout":"IPY_MODEL_695084a4635447b8a22a1462ac8c6ac7"}},"6d9a38d293e943259991bd488a1cda2f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e75d69a538f425b8ddcb0e0a79e0fe6","placeholder":"​","style":"IPY_MODEL_00ffaf237ac94816a381c5f1ad6c811e","value":"Downloading shards: 100%"}},"ca111dd05a2d4d04953ebc54ccf52969":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_db19e2602e544d0893b1019063f038c6","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5a7faeeded5444e1891b25d14d580cbe","value":3}},"6b210ff8485149e0a3e3a90f390e8cf4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_608a4af3cf0d43049a1930cff71a32fa","placeholder":"​","style":"IPY_MODEL_03ce31b14a284e8a8b06c471ff710a57","value":" 3/3 [00:02&lt;00:00,  1.15it/s]"}},"695084a4635447b8a22a1462ac8c6ac7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e75d69a538f425b8ddcb0e0a79e0fe6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00ffaf237ac94816a381c5f1ad6c811e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"db19e2602e544d0893b1019063f038c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a7faeeded5444e1891b25d14d580cbe":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"608a4af3cf0d43049a1930cff71a32fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03ce31b14a284e8a8b06c471ff710a57":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"faef3457741e4f5ea2a17df747bbf85c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f9bf9567d04f43279ead89ad756030d0","IPY_MODEL_1db1ff1a90534808aea9d3592247f1be","IPY_MODEL_09f17fee7c424d6f9e1cc0960346f639"],"layout":"IPY_MODEL_09485d3698f54ecbb98eb2fcacc8fb76"}},"f9bf9567d04f43279ead89ad756030d0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ebbea1c5a4f471db630f5520ec9e71b","placeholder":"​","style":"IPY_MODEL_e95863c631dd4f6aa5c87d2ab923596f","value":"Loading checkpoint shards: 100%"}},"1db1ff1a90534808aea9d3592247f1be":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e754d049cf2423fa7860995623cefca","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_76f12d8d682b4ea98e98fd5c61281297","value":3}},"09f17fee7c424d6f9e1cc0960346f639":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_982534d299bf42509d629f72eed49719","placeholder":"​","style":"IPY_MODEL_6eb68fbf44814b1abfe43b2fbe8f6e14","value":" 3/3 [03:59&lt;00:00, 75.57s/it]"}},"09485d3698f54ecbb98eb2fcacc8fb76":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ebbea1c5a4f471db630f5520ec9e71b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e95863c631dd4f6aa5c87d2ab923596f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4e754d049cf2423fa7860995623cefca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76f12d8d682b4ea98e98fd5c61281297":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"982534d299bf42509d629f72eed49719":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6eb68fbf44814b1abfe43b2fbe8f6e14":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"034f40fe7a504ab7b4adbc7567e4b8a0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7e04f5cdac6742aa86173a132f478e03","IPY_MODEL_61ce3aa1ba764caa9ee7bae15c6aec2d","IPY_MODEL_41e4bc673b0745bea490beab43e2c06b"],"layout":"IPY_MODEL_1459507584b7484d84cfaca95b9b2e17"}},"7e04f5cdac6742aa86173a132f478e03":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_25fa85af1379471b857619b43f145d5a","placeholder":"​","style":"IPY_MODEL_56b8c8e0aeaa45fb9a61d6f8f0dbaf76","value":"LLM masking (Mistral): 100%"}},"61ce3aa1ba764caa9ee7bae15c6aec2d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_76a82a9cda844e11b94442e9b2f7e84f","max":671,"min":0,"orientation":"horizontal","style":"IPY_MODEL_834e3510609b45b593e2f281c5cdbd4a","value":671}},"41e4bc673b0745bea490beab43e2c06b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2bf68bf8a79428899afe37ab431202a","placeholder":"​","style":"IPY_MODEL_7f20fb69f3074dbbae635563df858469","value":" 671/671 [19:57&lt;00:00,  1.76s/it]"}},"1459507584b7484d84cfaca95b9b2e17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"25fa85af1379471b857619b43f145d5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56b8c8e0aeaa45fb9a61d6f8f0dbaf76":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"76a82a9cda844e11b94442e9b2f7e84f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"834e3510609b45b593e2f281c5cdbd4a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f2bf68bf8a79428899afe37ab431202a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f20fb69f3074dbbae635563df858469":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"372c39a0a8274706a20d91eb20e8ca19":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_488d738942224b1c90992536f7783144","IPY_MODEL_ffecc1166f4e4308a242817c3e4e1fef","IPY_MODEL_fa8b7bfd07484ce8bda7bcec816470c7"],"layout":"IPY_MODEL_20eb2903da9e4b0ca08fbe2d47b561fa"}},"488d738942224b1c90992536f7783144":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d825002a5cc5456f871726a5072604f8","placeholder":"​","style":"IPY_MODEL_7780d6d63f2b4901b0c471a58f65a62c","value":"LLM infilling (Mistral): 100%"}},"ffecc1166f4e4308a242817c3e4e1fef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc0e9441394345c38d0e23dd916c1ffc","max":671,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7295a112f5cb4b75b9fd30d7ecb6e96f","value":671}},"fa8b7bfd07484ce8bda7bcec816470c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a3fc1ded11d46739eb7f4a64254062d","placeholder":"​","style":"IPY_MODEL_c8823295cb444c87a0bb225584c1fbc4","value":" 671/671 [27:21&lt;00:00,  2.85s/it]"}},"20eb2903da9e4b0ca08fbe2d47b561fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"d825002a5cc5456f871726a5072604f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7780d6d63f2b4901b0c471a58f65a62c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fc0e9441394345c38d0e23dd916c1ffc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7295a112f5cb4b75b9fd30d7ecb6e96f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5a3fc1ded11d46739eb7f4a64254062d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8823295cb444c87a0bb225584c1fbc4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1fb4df54ecb748dca844f683c38d90d1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4871aef7967c42288555d008cad97899","IPY_MODEL_b50af3c2d3fa4a25ac1288f57409b491","IPY_MODEL_e55d45de473b4c41badf8edcac7ee840"],"layout":"IPY_MODEL_d8f5b27dede14ff0bc32847612b8a12d"}},"4871aef7967c42288555d008cad97899":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a29aae4f8d4f478d9f460bb933a1e39b","placeholder":"​","style":"IPY_MODEL_d94e0ebe956140fbacc6ed30b2685998","value":"DecompX masking for reranking: 100%"}},"b50af3c2d3fa4a25ac1288f57409b491":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac1102b44fa54fb4a6606478e96af0d7","max":420,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f1d6d10dd1f6433fa3dc68c4c069a989","value":420}},"e55d45de473b4c41badf8edcac7ee840":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_34eb0136fb3e487793f1c574e7d835a6","placeholder":"​","style":"IPY_MODEL_4458e359953e4cdd81d1b072ae16d149","value":" 420/420 [00:48&lt;00:00, 14.52it/s]"}},"d8f5b27dede14ff0bc32847612b8a12d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"a29aae4f8d4f478d9f460bb933a1e39b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d94e0ebe956140fbacc6ed30b2685998":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac1102b44fa54fb4a6606478e96af0d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1d6d10dd1f6433fa3dc68c4c069a989":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"34eb0136fb3e487793f1c574e7d835a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4458e359953e4cdd81d1b072ae16d149":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}
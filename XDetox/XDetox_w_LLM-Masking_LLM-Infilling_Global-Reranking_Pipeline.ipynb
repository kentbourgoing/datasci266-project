{"cells":[{"cell_type":"markdown","id":"82d4549b","metadata":{"id":"82d4549b"},"source":["# XDetox with LLM Masking, LLM Infilling, and Global Reranking\n","\n","This notebook runs an XDetox pipeline with:\n","\n","1. **LLM masking** using Mistral-7B-Instruct (`mistralai/Mistral-7B-Instruct-v0.2`), which detects toxic spans and replaces them with `<mask>`.\n","2. **LLM infilling** (same Mistral model) that fills the `<mask>` tokens with safe alternatives while keeping the rest of the sentence almost unchanged.\n","3. **Global reranking** of multiple LLM candidates per input using:\n","\n","   * **Toxicity** (XLM-R large classifier).\n","   * **Semantic similarity** (LaBSE).\n","   * **Fluency** (GPT-2 perplexity).\n","\n","The goal is to pick, for each toxic input sentence, **one best detoxified candidate** that is:\n","\n","* As **non-toxic** as possible.\n","* As **semantically close** as possible to the original.\n","* As **fluent and natural** as possible.\n","\n","The main differences from the previous pipelines are:\n","\n","* The **masking** step and the **infilling** step are **both done by an LLM (Mistral-7B-Instruct)**.\n","* There is **no MaRCo / BART generation** in this notebook.\n","* **DecompX is not used at all** (neither for masking nor for reranking).\n","\n","---\n","\n","## Scoring: Global Reranking\n","\n","For each candidate $c$, we compute:\n","\n","* $T(c)$: toxicity in $[0, 1]$ from `textdetox/xlmr-large-toxicity-classifier-v2`\n","  (higher = more toxic).\n","* $S(c)$: semantic similarity in $[0, 1]$ from LaBSE cosine similarity to the **original toxic sentence**.\n","* $F(c)$: fluency in $[0, 1]$ from GPT-2 perplexity mapped to a score\n","  (low perplexity $\\Rightarrow$ high fluency).\n","\n","We convert toxicity into a **safety** score:\n","\n","$$\n","T'(c) = 1 - T(c)\n","$$\n","\n","Then we form a **global score**:\n","\n","$$\n","\\text{Score}(c) = w_T \\cdot T'(c) + w_S \\cdot S(c) + w_F \\cdot F(c)\n","$$\n","\n","You control the weights:\n","\n","* `weights = (w_T, w_S, w_F)`\n","\n","  * `w_T`: importance of **safety** (low toxicity).\n","  * `w_S`: importance of **semantic similarity**.\n","  * `w_F`: importance of **fluency**.\n","\n","For each input sentence, we:\n","\n","1. Generate `num_candidates` LLM infilling candidates.\n","2. Score each candidate using the formula above.\n","3. Select the **highest-scoring** candidate as the final output.\n","\n","---\n","\n","## LLM Masking (Mistral-7B-Instruct)\n","\n","### Prompted masking behavior\n","\n","Masking is done by a chat-style LLM:\n","\n","* Model: `mistralai/Mistral-7B-Instruct-v0.2`.\n","* The LLM is instructed to:\n","\n","  * **Identify toxic, offensive, or profane words or short phrases**.\n","  * Replace **each toxic span** with a **single `<mask>` token**.\n","  * Allow **multiple `<mask>` tokens** in one sentence (one per toxic span).\n","  * If multiple neighboring words are toxic, **collapse them into one `<mask>`**\n","    (never output `<mask> <mask> ...`).\n","  * Keep **all non-toxic words and punctuation unchanged**.\n","  * **Not** paraphrase, summarize, or reorder the sentence.\n","  * Return the masked sentence **inside exactly one pair of brackets**:\n","\n","    ```text\n","    [This is a <mask> example.]\n","    ```\n","\n","This step turns a raw toxic input into a **masked template** that marks exactly where detoxification must happen.\n","\n","### Post-processing of LLM masks\n","\n","Because LLM output can be noisy, the notebook cleans the raw masked output before using it:\n","\n","1. **Extract the bracket content**:\n","\n","   * Try to read the first `[ ... ]` block.\n","   * If there is `[` but no `]`, take everything after `[` as the sentence.\n","   * If there are no brackets, fall back to the full string.\n","\n","2. **Strip stray outer brackets** that may still remain.\n","\n","3. **Normalize whitespace** (collapse multiple spaces into one).\n","\n","4. **Normalize `<mask>` tokens**:\n","\n","   * Any variant like `<Mask>`, `<MASK>`, `< mask >` is normalized to `<mask>`.\n","\n","5. **Collapse runs of `<mask>`**:\n","\n","   * Any sequence like `<mask> <mask> <mask>` becomes a single `<mask>`.\n","\n","6. If, after cleaning, the sentence becomes empty, fall back to the original masked text.\n","\n","All cleaned, LLM-masked sentences are saved to:\n","\n","* `data/model_outputs/{output_folder}/{data_type}/LLM_Mask_LLM_Global/masked_inputs.txt`\n","\n","and reused across later runs with the same `output_folder` and `data_type`.\n","\n","---\n","\n","## LLM Infilling (Mistral-7B-Instruct)\n","\n","After masking, detoxification is also performed by Mistral-7B-Instruct via **infilling**:\n","\n","* For each example we pass two inputs to the LLM:\n","\n","  1. **Toxic Sentence**: the raw toxic input.\n","  2. **Masked Sentence**: the same sentence, but with toxic spans replaced by `<mask>`.\n","\n","The system prompt instructs the LLM to:\n","\n","* Treat the **Masked Sentence** as a template.\n","* For each `<mask>` token, insert a **short, non-toxic phrase** that:\n","\n","  * Fits the local context, and\n","  * Preserves the meaning and intent of the original **Toxic Sentence**.\n","* Keep **all other tokens outside `<mask>` spans unchanged**, except for small grammar fixes.\n","* Keep the **language the same** (no translation).\n","* Output the final filled sentence **inside one pair of brackets**:\n","\n","  ```text\n","  [Detoxified sentence here.]\n","  ```\n","\n","For each `(source, masked)` pair we:\n","\n","1. Build a prompt that includes both **Toxic Sentence** and **Masked Sentence**.\n","2. Call `generate(...)` with:\n","\n","   * `num_return_sequences = num_candidates`,\n","   * `do_sample = llm_sample`,\n","   * `temperature = llm_temperature` (if sampling),\n","   * `top_p = llm_top_p`,\n","   * `max_new_tokens = llm_max_new_tokens`.\n","3. Decode the new tokens, extract bracket content, and clean it.\n","\n","### Post-processing of LLM infilling\n","\n","For each generated candidate we:\n","\n","1. **Extract** the text inside the first `[ ... ]`.\n","2. **Remove stray outer brackets**, if any remain.\n","3. **Normalize whitespace**.\n","4. **Remove leftover `<mask>` tokens**, if the model failed to fill them.\n","\n","If cleaning produces an empty string, we fall back to the original toxic input for safety.\n","\n","The result is a list of candidate sentences:\n","\n","* `candidates[i]` is a list of length `num_candidates` for the $i$-th input.\n","\n","---\n","\n","## Global Reranking (toxicity + similarity + fluency)\n","\n","After we have LLM candidates, we apply **global reranking**:\n","\n","1. **Flatten** all candidates into a single list and keep track of which input each candidate belongs to.\n","\n","2. Compute:\n","\n","   * **Toxicity** with XLM-R large:\n","\n","     * `get_toxicity_scores(...)` returns $T(c) \\in [0, 1]$.\n","     * We define **safety** as $T'(c) = 1 - T(c)$.\n","\n","   * **Semantic similarity** with LaBSE:\n","\n","     * Encode all sources and candidates with LaBSE.\n","     * Compute cosine similarity between each candidate and its source.\n","     * Map cosine similarity from $[-1, 1]$ to $[0, 1]$.\n","\n","   * **Fluency** with GPT-2:\n","\n","     * Compute sentence-level perplexity for each candidate.\n","     * Map perplexity to a **fluency score** $F(c) \\in [0, 1]$ using a log-scale mapping\n","       with clipping between `ppl_min` and `ppl_max`.\n","\n","3. Combine these into a **global score**:\n","\n","   $$\n","   \\text{Score}(c) = w_T \\cdot T'(c) + w_S \\cdot S(c) + w_F \\cdot F(c)\n","   $$\n","\n","4. For each input we:\n","\n","   * Reshape the scores into an $N \\times C$ matrix (inputs $\\times$ candidates).\n","   * Select the candidate with the **highest score**.\n","\n","For each run folder, we write:\n","\n","* `orig.txt` — original toxic inputs (one per line).\n","* `gen.txt` — chosen detoxified outputs (one per line).\n","\n","Outputs are stored under:\n","\n","```text\n","data/model_outputs/{output_folder}/{data_type}/LLM_Mask_LLM_Global/{run_folder}/\n","```\n","\n","where `{run_folder}` encodes LLM hyperparameters (temperature, top-p, sampling flag, number of candidates, max new tokens, etc.).\n","\n","---\n","\n","## Evaluation\n","\n","If `run_eval=True`, the pipeline calls `evaluation.evaluate_all` to compute:\n","\n","* BERTScore (F1)\n","* MeaningBERT\n","* BLEU-4\n","* Toxicity (orig / gen) using XLM-R\n","* Perplexity (orig / gen) using GPT-2\n","\n","For each run folder, it writes:\n","\n","* `gen_stats.txt` under:\n","\n","  ```text\n","  data/model_outputs/{output_folder}/{data_type}/LLM_Mask_LLM_Global/{run_folder}/\n","  ```\n","\n","It also creates a **summary CSV per dataset**:\n","\n","* `data/model_outputs/{output_folder}/{data_type}/{data_type}.csv`\n","\n","The CSV aggregates metrics over all run folders in `LLM_Mask_LLM_Global/`.\n","The `threshold` column is kept as a **fixed label** (for example `0.20`) for compatibility with other XDetox pipelines. It does **not** control masking or reranking here.\n","\n","---\n","\n","## How to Use `detoxify()`\n","\n","Function signature (conceptual):\n","\n","```python\n","def detoxify(\n","    data_type: str = \"paradetox\",\n","    output_folder: str = \"colab_run_llm_mask_infill_global\",\n","    echo: bool = False,\n","    num_examples: int = 100,\n","    overwrite_gen: bool = False,\n","    run_eval: bool = False,\n","    overwrite_eval: bool = False,\n","    skip_ref_eval: bool = False,\n","    # LLM infilling:\n","    num_candidates: int = 3,\n","    llm_temperature: float = 0.7,\n","    llm_top_p: float = 0.95,\n","    llm_max_new_tokens: int = 64,\n","    llm_sample: bool = True,\n","    # Global reranking:\n","    weights = (0.5, 0.3, 0.2),  # (w_T, w_S, w_F)\n","    ppl_min: float = 5.0,\n","    ppl_max: float = 300.0,\n",")\n","```\n","\n","### Key arguments\n","\n","#### Core I/O\n","\n","* `data_type`: dataset key from `data_configs`, such as:\n","\n","  * `\"paradetox\"`, `\"dynabench_val\"`, `\"dynabench_test\"`,\n","  * `\"jigsaw_toxic\"`, `\"microagressions_val\"`, `\"sbf_val\"`,\n","  * `\"appdia_original\"`, `\"appdia_discourse\"`, etc.\n","\n","* `output_folder`: top-level directory under `data/model_outputs/`:\n","\n","  ```text\n","  data/model_outputs/{output_folder}/{data_type}/...\n","  ```\n","\n","* `num_examples`: if set, only the first `num_examples` examples are used (useful for quick tests).\n","  Use `None` to run on the full dataset.\n","\n","* `overwrite_gen`:\n","\n","  * If `False` and `gen.txt` already exists for a given run folder, skip generation and reuse previous outputs.\n","  * If `True`, regenerate outputs even if `gen.txt` exists.\n","\n","* `echo`:\n","\n","  * If `True`, print:\n","\n","    * Dataset and output paths,\n","    * A few example inputs,\n","    * A few masked sentences,\n","    * A few final detoxified outputs,\n","    * And, if `run_eval=True`, evaluation metrics.\n","\n","#### LLM masking (Mistral)\n","\n","* Masking uses **Mistral-7B-Instruct** with a fixed **masking prompt** and a single **few-shot example**.\n","* The behavior does **not** depend on numeric hyperparameters.\n","* Masked sentences are cached in:\n","\n","  ```text\n","  data/model_outputs/{output_folder}/{data_type}/LLM_Mask_LLM_Global/masked_inputs.txt\n","  ```\n","\n","and reused if the file already exists.\n","\n","#### LLM infilling (Mistral)\n","\n","* `num_candidates`: how many detoxified candidates to generate per input.\n","* `llm_temperature`:\n","\n","  * Controls randomness when `llm_sample=True`.\n","* `llm_top_p`:\n","\n","  * Nucleus sampling parameter.\n","* `llm_max_new_tokens`:\n","\n","  * Maximum number of new tokens generated beyond the prompt.\n","* `llm_sample`:\n","\n","  * `True`: sampling with `temperature` and `top_p`.\n","  * `False`: deterministic decoding (greedy-like).\n","\n","The infilling step uses both **Toxic Sentence** and **Masked Sentence** in the prompt, and is instructed to **only modify `<mask>` tokens** plus small grammar fixes.\n","\n","#### Global reranking\n","\n","* `weights = (w_T, w_S, w_F)`:\n","\n","  * `w_T`: weight on **safety** (1 − toxicity).\n","  * `w_S`: weight on **semantic similarity**.\n","  * `w_F`: weight on **fluency**.\n","\n","* `ppl_min`, `ppl_max`:\n","\n","  * Bounds for mapping GPT-2 perplexity into a $[0, 1]$ fluency score.\n","  * Lower perplexity (between `ppl_min` and `ppl_max`) gives higher fluency.\n","\n","Larger `num_candidates` usually improve reranking quality but increase computation.\n","\n","#### Evaluation\n","\n","* `run_eval`: if `True`, run `evaluation.evaluate_all` after generation and write `gen_stats.txt`.\n","* `overwrite_eval`:\n","\n","  * If `False`, keep existing `gen_stats.txt` when present.\n","  * If `True`, recompute evaluation.\n","* `skip_ref_eval`: if `True`, skip some reference-based metrics (for example, reference perplexity).\n","\n","---\n","\n","## Example Calls\n","\n","### Quick sanity check on a small subset\n","\n","```python\n","detoxify(\n","    data_type=\"paradetox\",\n","    output_folder=\"colab_run_llm_mask_infill_global_demo_50_examples\",\n","    echo=True,\n","    num_examples=50,           # small subset for testing\n","    run_eval=True,             # BLEU / BERTScore / MeaningBERT / PPL / Toxicity\n","    overwrite_gen=True,\n","    overwrite_eval=True,\n","    skip_ref_eval=False,\n","    num_candidates=10,         # LLM candidates per input\n","    llm_temperature=0.7,\n","    llm_top_p=0.95,\n","    llm_max_new_tokens=64,\n","    llm_sample=True,\n","    weights=(0.5, 0.3, 0.2),   # safety, similarity, fluency\n",")\n","```\n","\n","### Larger run (more candidates, full dataset)\n","\n","```python\n","detoxify(\n","    data_type=\"paradetox\",\n","    output_folder=\"paradetox_llm_mask_infill_global_full\",\n","    echo=True,\n","    num_examples=None,         # full dataset\n","    run_eval=True,\n","    overwrite_gen=False,\n","    overwrite_eval=False,\n","    skip_ref_eval=False,\n","    num_candidates=20,\n","    llm_temperature=0.7,\n","    llm_top_p=0.95,\n","    llm_max_new_tokens=64,\n","    llm_sample=True,\n","    weights=(0.5, 0.3, 0.2),\n",")\n","```\n","\n","After running `detoxify`, you can inspect:\n","\n","* `orig.txt` and `gen.txt` under:\n","\n","  ```text\n","  data/model_outputs/{output_folder}/{data_type}/LLM_Mask_LLM_Global/{run_folder}/\n","  ```\n","\n","* Per-run metrics:\n","\n","  ```text\n","  data/model_outputs/{output_folder}/{data_type}/LLM_Mask_LLM_Global/{run_folder}/gen_stats.txt\n","  ```\n","\n","* Aggregated metrics:\n","\n","  ```text\n","  data/model_outputs/{output_folder}/{data_type}/{data_type}.csv\n","  ```\n","\n","This setup lets you directly compare:\n","\n","* **LLM-masking + LLM-infilling + global reranking**\n","  against other XDetox pipelines such as:\n","\n","* **DecompX-masking + global reranking**,\n","\n","* **LLM-masking + MaRCo + global reranking**, and\n","\n","* **LLM-masking + LLM-infilling + DecompX reranking**.\n"]},{"cell_type":"code","source":["#@title Mount Drive, Imports & locate XDetox\n","from google.colab import drive; drive.mount('/content/drive')\n","\n","import os, glob, re, sys, json, shutil, math\n","import numpy as np\n","import pandas as pd\n","from tqdm.auto import tqdm\n","from pathlib import Path\n","from subprocess import run, PIPE\n","import torch\n","import nltk\n","from typing import List\n","\n","# Try My Drive\n","candidate = \"/content/drive/MyDrive/w266 - Project/XDetox\"\n","print(\"Try MyDrive:\", candidate, \"->\", os.path.isdir(candidate))\n","\n","XDETOX_DIR = candidate\n","print(\"Using XDETOX_DIR:\", XDETOX_DIR)\n","assert os.path.isdir(XDETOX_DIR), f\"XDETOX_DIR does not exist: {XDETOX_DIR}\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kfBoQTrjtynY","executionInfo":{"status":"ok","timestamp":1764552010989,"user_tz":480,"elapsed":38720,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"c4613dc3-7604-4f22-ce89-73cc985262c2"},"id":"kfBoQTrjtynY","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Try MyDrive: /content/drive/MyDrive/w266 - Project/XDetox -> True\n","Using XDETOX_DIR: /content/drive/MyDrive/w266 - Project/XDetox\n"]}]},{"cell_type":"code","source":["#@title Runtime setup (paths, cache, GPU)\n","# HuggingFace cache inside the repo (persists on Drive)\n","HF_CACHE = os.path.join(XDETOX_DIR, \"cache\")\n","os.makedirs(HF_CACHE, exist_ok=True)\n","os.environ[\"TRANSFORMERS_CACHE\"] = HF_CACHE\n","\n","# Add repo to PYTHONPATH\n","if XDETOX_DIR not in sys.path:\n","    sys.path.append(XDETOX_DIR)\n","\n","print(\"XDETOX_DIR:\", XDETOX_DIR)\n","print(\"TRANSFORMERS_CACHE:\", HF_CACHE)\n","print(\"CUDA available:\", torch.cuda.is_available())\n","if torch.cuda.is_available():\n","    print(\"GPU:\", torch.cuda.get_device_name(0))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ITPlTNBtzQx","executionInfo":{"status":"ok","timestamp":1764552011406,"user_tz":480,"elapsed":414,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"fe7e3776-e567-4178-e6f2-b6f423365e07"},"id":"7ITPlTNBtzQx","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["XDETOX_DIR: /content/drive/MyDrive/w266 - Project/XDetox\n","TRANSFORMERS_CACHE: /content/drive/MyDrive/w266 - Project/XDetox/cache\n","CUDA available: True\n","GPU: NVIDIA A100-SXM4-80GB\n"]}]},{"cell_type":"code","source":["#@title Verify XDetox repo layout\n","for d in [\"rewrite\", \"evaluation\", \"datasets\"]:\n","    assert os.path.isdir(os.path.join(XDETOX_DIR, d)), f\"Missing folder: {d}\"\n","print(\"Repo folders OK.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MEy2TGYetzIb","executionInfo":{"status":"ok","timestamp":1764552011428,"user_tz":480,"elapsed":18,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"453d0e27-d9da-4d2d-e9c6-6f12a7944214"},"id":"MEy2TGYetzIb","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Repo folders OK.\n"]}]},{"cell_type":"code","source":["#@title Install dependencies (restart runtime if major errors)\n","!pip -q install --upgrade pip setuptools wheel\n","!pip -q install \"transformers==4.41.2\" \"tokenizers==0.19.1\" \\\n","                \"datasets==2.19.0\" \"evaluate==0.4.1\" \\\n","                \"sacrebleu==2.4.1\" sacremoses ftfy nltk matplotlib pandas jedi \\\n","                sentencepiece\n","!pip -q install bert-score\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GeTzwxVDtzNn","executionInfo":{"status":"ok","timestamp":1764552033990,"user_tz":480,"elapsed":22559,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"8b8be032-63ac-41df-cf72-78256afc18b8"},"id":"GeTzwxVDtzNn","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.3.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["#@title Imports from transformers / rewrite\n","from transformers import (\n","    AutoTokenizer, AutoModel, AutoModelForSequenceClassification,\n","    AutoModelForCausalLM,\n","    GPT2LMHeadModel, GPT2TokenizerFast,\n",")\n","from rewrite import rewrite_example as rx\n","import argparse as _argparse"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tfnuR2YVCmW9","executionInfo":{"status":"ok","timestamp":1764552041665,"user_tz":480,"elapsed":7663,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"bba1681c-79b6-4f22-d431-7572d0b41c89"},"id":"tfnuR2YVCmW9","execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["#@title NLTK data\n","nltk.download(\"punkt\", quiet=True)\n","try:\n","    nltk.download(\"punkt_tab\", quiet=True)\n","except Exception:\n","    pass\n","print(\"NLTK ready\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y0Up7SKstzK9","executionInfo":{"status":"ok","timestamp":1764552042186,"user_tz":480,"elapsed":518,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"1403f2aa-da51-4cff-8d7a-88cec28e91e4"},"id":"y0Up7SKstzK9","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["NLTK ready\n"]}]},{"cell_type":"code","source":["#@title Data configs\n","data_configs = {\n","    \"microagressions_val\": {\n","        \"data_path\": \"./datasets/microagressions/val.csv\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.25,\n","        \"temperature\": 2.5,\n","    },\n","    \"microagressions_test\": {\n","        \"data_path\": \"./datasets/microagressions/test.csv\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.25,\n","        \"temperature\": 2.5,\n","    },\n","    \"sbf_val\": {\n","        \"data_path\": \"./datasets/sbf/sbfdev.csv\",\n","        \"rep_penalty\": 1.5,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 5.0,\n","        \"temperature\": 2.9,\n","    },\n","    \"sbf_test\": {\n","        \"data_path\": \"./datasets/sbf/sbftst.csv\",\n","        \"rep_penalty\": 1.5,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 5.0,\n","        \"temperature\": 2.9,\n","    },\n","    \"dynabench_val\": {\n","        \"data_path\": \"./datasets/dynabench/db_dev.csv\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.75,\n","        \"temperature\": 2.5,\n","    },\n","    \"dynabench_test\": {\n","        \"data_path\": \"./datasets/dynabench/db_test.csv\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.75,\n","        \"temperature\": 2.5,\n","    },\n","    \"jigsaw_toxic\": {\n","        \"data_path\": \"./datasets/jigsaw_full_30/test_10k_toxic.txt\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.75,\n","        \"temperature\": 2.5,\n","    },\n","    \"paradetox\": {\n","        \"data_path\": \"./datasets/paradetox/test_toxic_parallel.txt\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.75,\n","        \"temperature\": 2.5,\n","    },\n","    \"appdia_original\": {\n","        \"data_path\": \"./datasets/appdia/original-annotated-data/original-test.tsv\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.75,\n","        \"temperature\": 2.5,\n","    },\n","    \"appdia_discourse\": {\n","        \"data_path\": \"./datasets/appdia/discourse-augmented-data/discourse-test.tsv\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.75,\n","        \"temperature\": 2.5,\n","    }\n","}\n","print(\"Datasets:\", \", \".join(data_configs.keys()))\n","\n","REPO = XDETOX_DIR"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7nBku39IuAgb","executionInfo":{"status":"ok","timestamp":1764552042199,"user_tz":480,"elapsed":12,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"548ec004-858c-49ab-c12f-1be347d342fd"},"id":"7nBku39IuAgb","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Datasets: microagressions_val, microagressions_test, sbf_val, sbf_test, dynabench_val, dynabench_test, jigsaw_toxic, paradetox, appdia_original, appdia_discourse\n"]}]},{"cell_type":"code","source":["#@title Helpers: subset data\n","def _abs_repo_path(rel: str) -> str:\n","    return os.path.join(REPO, rel.lstrip(\"./\"))\n","\n","def _ensure_dir(p: str):\n","    Path(p).mkdir(parents=True, exist_ok=True)\n","\n","def _subset_for_data_type(data_type, data_path, n, out_dir):\n","    \"\"\"\n","    Create a small subset file matching rewrite_example.get_data().\n","    Returns the new subset path (or original path if n is None/<=0).\n","    \"\"\"\n","    if n is None or n <= 0:\n","        return data_path\n","\n","    src = _abs_repo_path(data_path)\n","    _ensure_dir(out_dir)\n","\n","    if \"microagressions\" in data_path:\n","        df = pd.read_csv(src)\n","        sub = df.head(n)\n","        out = os.path.join(out_dir, os.path.basename(src))\n","        sub.to_csv(out, index=False)\n","        return out\n","\n","    if \"sbf\" in data_path:\n","        df = pd.read_csv(src)\n","        sub = df.head(n)\n","        out = os.path.join(out_dir, os.path.basename(src))\n","        sub.to_csv(out, index=False)\n","        return out\n","\n","    if \"dynabench\" in data_path:\n","        df = pd.read_csv(src)\n","        sub = df.head(n)\n","        out = os.path.join(out_dir, os.path.basename(src))\n","        sub.to_csv(out, index=False)\n","        return out\n","\n","    if any(k in data_path for k in [\"paradetox\", \"jigsaw\"]):\n","        if data_path.endswith(\".txt\"):\n","            with open(src, \"r\") as f:\n","                lines = [s.rstrip(\"\\n\") for s in f.readlines()]\n","            out = os.path.join(out_dir, os.path.basename(src))\n","            with open(out, \"w\") as g:\n","                for s in lines[:n]:\n","                    g.write(s + \"\\n\")\n","            return out\n","        elif data_path.endswith(\".csv\"):\n","            df = pd.read_csv(src).head(n)\n","            out = os.path.join(out_dir, os.path.basename(src))\n","            df.to_csv(out, index=False)\n","            return out\n","\n","    if \"appdia\" in data_path:\n","        df = pd.read_csv(src, sep=\"\\t\").head(n)\n","        out = os.path.join(out_dir, os.path.basename(src))\n","        df.to_csv(out, sep=\"\\t\", index=False)\n","        return out\n","\n","    out = os.path.join(out_dir, os.path.basename(src))\n","    shutil.copy(src, out)\n","    return out"],"metadata":{"id":"ToytrY0SuAjr","executionInfo":{"status":"ok","timestamp":1764552042203,"user_tz":480,"elapsed":2,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"ToytrY0SuAjr","execution_count":8,"outputs":[]},{"cell_type":"code","source":["#@title Global scoring helpers: toxicity, similarity, fluency\n","\n","DEVICE_SCORE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Scoring models will use: {DEVICE_SCORE}\")\n","\n","# ---------- Toxicity model (textdetox/xlmr-large-toxicity-classifier-v2) ----------\n","_TOX_MODEL_NAME = \"textdetox/xlmr-large-toxicity-classifier-v2\"\n","_TOX_TOKENIZER = None\n","_TOX_MODEL = None\n","\n","def _lazy_load_tox():\n","    global _TOX_TOKENIZER, _TOX_MODEL\n","    if _TOX_TOKENIZER is None or _TOX_MODEL is None:\n","        _TOX_TOKENIZER = AutoTokenizer.from_pretrained(_TOX_MODEL_NAME)\n","        _TOX_MODEL = AutoModelForSequenceClassification.from_pretrained(\n","            _TOX_MODEL_NAME\n","        ).to(DEVICE_SCORE)\n","        _TOX_MODEL.eval()\n","\n","@torch.no_grad()\n","def get_toxicity_scores(texts, batch_size=32):\n","    \"\"\"\n","    Returns toxicity probabilities in [0,1] for each input text.\n","    (0 = non-toxic, 1 = very toxic)\n","    \"\"\"\n","    _lazy_load_tox()\n","    scores = []\n","    for i in tqdm(range(0, len(texts), batch_size), desc=\"Toxicity\", leave=False):\n","        batch = texts[i:i+batch_size]\n","        enc = _TOX_TOKENIZER(\n","            batch, return_tensors=\"pt\",\n","            truncation=True, max_length=512, padding=True\n","        ).to(DEVICE_SCORE)\n","        logits = _TOX_MODEL(**enc).logits\n","        probs = torch.softmax(logits, dim=-1)  # [..., 2]\n","        scores.extend(probs[:, 1].detach().cpu().tolist())  # toxic prob\n","    return scores\n","\n","# ---------- Semantic similarity (LaBSE) ----------\n","_LABSE_NAME = \"sentence-transformers/LaBSE\"\n","_LABSE_TOKENIZER = None\n","_LABSE_MODEL = None\n","\n","def _lazy_load_labse():\n","    global _LABSE_TOKENIZER, _LABSE_MODEL\n","    if _LABSE_TOKENIZER is None or _LABSE_MODEL is None:\n","        _LABSE_TOKENIZER = AutoTokenizer.from_pretrained(_LABSE_NAME)\n","        _LABSE_MODEL = AutoModel.from_pretrained(_LABSE_NAME).to(DEVICE_SCORE)\n","        _LABSE_MODEL.eval()\n","\n","@torch.no_grad()\n","def get_labse_embeddings(texts, batch_size=32):\n","    \"\"\"\n","    Returns a numpy array of shape (len(texts), hidden_dim).\n","    Mean-pooled LaBSE sentence embeddings.\n","    \"\"\"\n","    _lazy_load_labse()\n","    embs = []\n","    for i in tqdm(range(0, len(texts), batch_size), desc=\"LaBSE embeddings\", leave=False):\n","        batch = texts[i:i+batch_size]\n","        enc = _LABSE_TOKENIZER(\n","            batch, return_tensors=\"pt\",\n","            truncation=True, max_length=256, padding=True\n","        ).to(DEVICE_SCORE)\n","        outputs = _LABSE_MODEL(**enc)\n","        hidden = outputs.last_hidden_state  # [B, L, H]\n","        mask = enc[\"attention_mask\"].unsqueeze(-1)  # [B, L, 1]\n","        masked = hidden * mask\n","        summed = masked.sum(dim=1)  # [B, H]\n","        counts = mask.sum(dim=1).clamp(min=1e-6)  # [B,1]\n","        sent_emb = (summed / counts).cpu().numpy()\n","        embs.append(sent_emb)\n","    if not embs:\n","        return np.zeros((0, 768), dtype=np.float32)\n","    return np.vstack(embs)\n","\n","# ---------- Fluency via GPT-2 perplexity ----------\n","_GPT2_NAME = \"gpt2\"\n","_GPT2_TOKENIZER = None\n","_GPT2_MODEL = None\n","\n","def _lazy_load_gpt2():\n","    global _GPT2_TOKENIZER, _GPT2_MODEL\n","    if _GPT2_TOKENIZER is None or _GPT2_MODEL is None:\n","        _GPT2_TOKENIZER = GPT2TokenizerFast.from_pretrained(_GPT2_NAME)\n","        _GPT2_MODEL = GPT2LMHeadModel.from_pretrained(_GPT2_NAME).to(DEVICE_SCORE)\n","        _GPT2_MODEL.eval()\n","\n","@torch.no_grad()\n","def get_gpt2_perplexities(texts):\n","    \"\"\"\n","    Simple sentence-level perplexity using GPT-2.\n","    Returns a list of floats (one per text).\n","    \"\"\"\n","    import math as _math\n","    _lazy_load_gpt2()\n","    ppls = []\n","    for s in tqdm(texts, desc=\"GPT-2 PPL\", leave=False):\n","        enc = _GPT2_TOKENIZER(s, return_tensors=\"pt\").to(DEVICE_SCORE)\n","        out = _GPT2_MODEL(enc[\"input_ids\"], labels=enc[\"input_ids\"])\n","        ppl = _math.exp(out.loss.item())\n","        if ppl > 1e4:\n","            ppl = 1e4  # clip extreme\n","        ppls.append(float(ppl))\n","    return ppls\n","\n","def perplexity_to_fluency(ppls, p_min=5.0, p_max=300.0):\n","    \"\"\"\n","    Map perplexities to [0,1] fluency scores.\n","    Low perplexity -> high fluency.\n","    \"\"\"\n","    import math as _math\n","    ppls = np.asarray(ppls, dtype=float)\n","    p = np.clip(ppls, p_min, p_max)\n","    log_p = np.log(p)\n","    log_min = _math.log(p_min)\n","    log_max = _math.log(p_max)\n","    F = (log_max - log_p) / (log_max - log_min + 1e-8)\n","    F = np.clip(F, 0.0, 1.0)\n","    return F\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-MxQChqOuAnT","executionInfo":{"status":"ok","timestamp":1764552042221,"user_tz":480,"elapsed":16,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"886239f5-d09d-4cf2-c257-943e64fd5edb"},"id":"-MxQChqOuAnT","execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Scoring models will use: cuda\n"]}]},{"cell_type":"code","source":["#@title Evaluation helpers (evaluate_all.py with MeaningBERT + toxicity)\n","def _eval_with_toxicity(base_path, overwrite_eval=False, skip_ref=False,\n","                        tox_threshold=0.5, tox_batch_size=32):\n","    \"\"\"\n","    Call evaluation.evaluate_all on each gen folder.\n","    \"\"\"\n","    import sys as _sys\n","    for folder in os.listdir(base_path):\n","        gen_dir = os.path.join(base_path, folder)\n","        if not os.path.isdir(gen_dir):\n","            continue\n","        orig_path = os.path.join(gen_dir, \"orig.txt\")\n","        gen_path  = os.path.join(gen_dir, \"gen.txt\")\n","        out_stats = os.path.join(gen_dir, \"gen_stats.txt\")\n","        if not (os.path.exists(orig_path) and os.path.exists(gen_path)):\n","            continue\n","        if os.path.exists(out_stats) and not overwrite_eval:\n","            continue\n","\n","        env = os.environ.copy()\n","        env[\"PYTHONPATH\"] = REPO + (\":\" + env.get(\"PYTHONPATH\",\"\") if env.get(\"PYTHONPATH\") else \"\")\n","        cmd = [\n","            _sys.executable, \"-m\", \"evaluation.evaluate_all\",\n","            \"--orig_path\", orig_path,\n","            \"--gen_path\",  gen_path,\n","            \"--tox_threshold\", str(tox_threshold),\n","            \"--tox_batch_size\", str(tox_batch_size),\n","        ]\n","        if skip_ref:\n","            cmd.append(\"--skip_ref\")\n","        print(\"Eval:\", \" \".join(cmd))\n","        res = run(cmd, cwd=REPO, env=env, stdout=PIPE, stderr=PIPE, text=True)\n","        if res.returncode != 0:\n","            print(res.stdout)\n","            print(res.stderr)\n","            res.check_returncode()\n","\n","def _safe_float(x):\n","    try:\n","        return float(x)\n","    except Exception:\n","        return float('nan')\n","\n","def _read_stats_file(path):\n","    out = {}\n","    with open(path, \"r\") as f:\n","        for line in f:\n","            if \":\" not in line:\n","                continue\n","            k, v = line.strip().split(\": \", 1)\n","            k = k.replace(\"(skipped)\", \"\").strip().lower()\n","            out[k] = _safe_float(v)\n","    return out\n","\n","def _aggregate_eval_csv(output_folder, data_type, base_out_dir):\n","    \"\"\"\n","    Aggregate eval metrics for LLM-masking + LLM-infilling + global reranking.\n","\n","    Layout (absolute base_out_dir):\n","      base_out_dir/\n","        └── {data_type}/\n","            └── LLM_Mask_LLM_Global/\n","                └── {run_folder}/\n","                    └── gen_stats.txt\n","\n","    threshold column kept as fixed label (=0.20) for compatibility.\n","    \"\"\"\n","    rows = []\n","\n","    mask_dir = \"LLM_Mask_LLM_Global\"\n","    base_path = os.path.join(base_out_dir, data_type, mask_dir)\n","    if not os.path.isdir(base_path):\n","        print(\"No evaluation directory found:\", base_path)\n","        return\n","\n","    for folder in os.listdir(base_path):\n","        gen_dir = os.path.join(base_path, folder)\n","        stats_path = os.path.join(gen_dir, \"gen_stats.txt\")\n","        if not os.path.exists(stats_path):\n","            continue\n","        s = _read_stats_file(stats_path)\n","        rows.append({\n","            \"threshold\":        0.20,  # label only, not used by this pipeline\n","            \"folder\":           folder,\n","            \"bertscore\":        s.get(\"bertscore\", np.nan),\n","            \"meaningbert\":      s.get(\"meaningbert\", np.nan),\n","            \"bleu4\":            s.get(\"bleu4\", np.nan),\n","            \"perplexity_gen\":   s.get(\"perplexity gen\", np.nan),\n","            \"perplexity_orig\":  s.get(\"perplexity orig\", np.nan),\n","            \"toxicity_gen\":     s.get(\"toxicity gen\", np.nan),\n","            \"toxicity_orig\":    s.get(\"toxicity orig\", np.nan),\n","        })\n","\n","    if rows:\n","        cols = [\n","            \"threshold\", \"folder\",\n","            \"bertscore\", \"meaningbert\", \"bleu4\",\n","            \"perplexity_gen\", \"perplexity_orig\",\n","            \"toxicity_gen\", \"toxicity_orig\",\n","        ]\n","        df = pd.DataFrame(rows)\n","        df = df[cols]\n","        out_csv = os.path.join(base_out_dir, data_type, f\"{data_type}.csv\")\n","        df.to_csv(out_csv, index=False)\n","        print(\"Wrote summary CSV:\", out_csv)\n","    else:\n","        print(\"No evaluation files found to summarize.\")"],"metadata":{"id":"u-7I09Uvqb8c","executionInfo":{"status":"ok","timestamp":1764552042229,"user_tz":480,"elapsed":6,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"u-7I09Uvqb8c","execution_count":10,"outputs":[]},{"cell_type":"code","source":["#@title Shared LLM loader (Mistral-7B-Instruct) for masking + infilling\n","USE_LLM_GPU = True\n","DEVICE_LLM = torch.device(\"cuda\" if USE_LLM_GPU and torch.cuda.is_available() else \"cpu\")\n","print(\"LLM device (mask + infill):\", DEVICE_LLM)\n","\n","LLM_MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n","_LLM_TOKENIZER = None\n","_LLM_MODEL = None\n","\n","def _lazy_load_llm():\n","    global _LLM_MODEL, _LLM_TOKENIZER\n","    if _LLM_MODEL is not None and _LLM_TOKENIZER is not None:\n","        return\n","    print(f\"Loading LLM (Mistral-7B-Instruct): {LLM_MODEL_NAME} on {DEVICE_LLM} ...\")\n","    _LLM_TOKENIZER = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n","    _LLM_MODEL = AutoModelForCausalLM.from_pretrained(\n","        LLM_MODEL_NAME,\n","        torch_dtype=torch.float16 if DEVICE_LLM.type == \"cuda\" else torch.float32,\n","        device_map=None,\n","    ).to(DEVICE_LLM)\n","    _LLM_MODEL.eval()\n","    print(\"LLM loaded.\")\n","\n","\n","def _extract_bracket_content(text: str) -> str:\n","    \"\"\"\n","    Extract content inside the first [ ... ] block.\n","    If there is an opening '[' but no closing ']', take everything after '['.\n","    Otherwise, fall back to the whole string.\n","    \"\"\"\n","    text = text.strip()\n","\n","    m = re.search(r\"\\[([^\\]]*)\\]\", text, flags=re.DOTALL)\n","    if m:\n","        return m.group(1).strip()\n","\n","    if \"[\" in text:\n","        return text.split(\"[\", 1)[1].strip()\n","\n","    return text"],"metadata":{"id":"nAVpxvu2b_Me","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764552042285,"user_tz":480,"elapsed":54,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"3e792510-17e9-4593-df77-2f7eb1cd58a5"},"id":"nAVpxvu2b_Me","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["LLM device (mask + infill): cuda\n"]}]},{"cell_type":"code","source":["#@title LLM Masking (MetaDetox-style) using Mistral-7B-Instruct\n","\n","MASK_SYSTEM_PROMPT = \"\"\"You are a helpful assistant trained to make toxic or offensive sentences safer\n","by MASKING toxic words with the special token <mask>, while keeping the original sentence\n","structure and wording as much as possible.\n","\n","You must behave like a MASKER, not a full rewriter.\n","\n","Your task:\n","1. Identify toxic, offensive, or profane words or short phrases.\n","2. For each toxic span, replace the entire span with a single <mask> token.\n","3. There may be multiple toxic spans in one sentence, so multiple <mask> tokens are allowed.\n","4. If several neighboring words are toxic, you must still use only a single <mask> token\n","   in that place. In other words, if you would place \"<mask> <mask>\" or a longer sequence\n","   of <mask> tokens, collapse them into a single <mask> so that there are never multiple\n","   <mask> tokens in a row.\n","5. Do NOT rewrite, paraphrase, or summarize the sentence.\n","6. Do NOT add, remove, or reorder non-toxic words or punctuation.\n","7. Keep punctuation and spacing as close to the original as possible.\n","8. If there is no toxic content, return the sentence unchanged.\n","\n","Output rules (format is very strict):\n","- ONLY return the final masked sentence inside ONE pair of square brackets, like:\n","  [This is a <mask> example.]\n","- Do NOT print anything before or after the brackets.\n","- Do NOT add explanations, comments, or extra lines.\n","- Do NOT include any language tags or metadata.\n","- Do NOT include additional '[' or ']' characters inside the sentence.\n","\"\"\"\n","\n","MASK_FEW_SHOT = \"\"\"Toxic Sentence: You're such a stupid idiot, nobody wants to hear your crap.\n","Step 1 - Identify toxic words: \"stupid idiot\", \"crap\"\n","Step 2 - Mask toxic words (do NOT rewrite the rest):\n","You're such a <mask>, nobody wants to hear your <mask>.\n","Final Output: [You're such a <mask>, nobody wants to hear your <mask>.]\"\"\"\n","\n","def _postprocess_llm_mask(masked_text: str) -> str:\n","    \"\"\"\n","    Clean up LLM-masked sentences:\n","      - remove stray leading/trailing brackets,\n","      - normalize whitespace,\n","      - normalize <mask> token casing,\n","      - collapse runs of <mask> into a single <mask>.\n","    \"\"\"\n","    s = masked_text.strip()\n","\n","    # Remove any leftover outer brackets if still present\n","    if s.startswith(\"[\") and s.endswith(\"]\") and len(s) > 2:\n","        s = s[1:-1].strip()\n","    else:\n","        if s.startswith(\"[\"):\n","            s = s[1:].strip()\n","        if s.endswith(\"]\"):\n","            s = s[:-1].strip()\n","\n","    # Normalize whitespace\n","    s = re.sub(r\"\\s+\", \" \", s).strip()\n","\n","    # Normalize mask token casing and spacing: <Mask>, <MASK>, < mask > -> <mask>\n","    s = re.sub(r\"<\\s*mask\\s*>\", \"<mask>\", s, flags=re.IGNORECASE)\n","\n","    # Collapse runs of <mask> (e.g., \"<mask> <mask> <mask>\" -> \"<mask>\")\n","    s = re.sub(r\"(?:\\s*<mask>\\s*){2,}\", \" <mask> \", s)\n","    s = re.sub(r\"\\s+\", \" \", s).strip()\n","\n","    # Simple safety: if we somehow deleted everything, fall back to the original masked_text\n","    if not s:\n","        return masked_text.strip()\n","\n","    return s\n","\n","@torch.no_grad()\n","def llm_mask_sentences(sentences: List[str]) -> List[str]:\n","    \"\"\"\n","    Use Mistral-7B-Instruct as a masker:\n","    input: toxic sentence\n","    output: same sentence but toxic words replaced by <mask>.\n","    \"\"\"\n","    _lazy_load_llm()\n","    masked = []\n","    for s in tqdm(sentences, desc=\"LLM masking (Mistral)\", leave=False):\n","        messages = [\n","            {\n","                \"role\": \"system\",\n","                \"content\": MASK_SYSTEM_PROMPT + \"\\n\\nBelow is an example:\\n\" + MASK_FEW_SHOT,\n","            },\n","            {\n","                \"role\": \"user\",\n","                \"content\": f\"Toxic Sentence: {s}\\nFinal Output:\",\n","            },\n","        ]\n","        try:\n","            prompt = _LLM_TOKENIZER.apply_chat_template(\n","                messages,\n","                tokenize=False,\n","                add_generation_prompt=True,\n","            )\n","        except Exception:\n","            # Fallback: plain prompt if chat template not available\n","            prompt = (\n","                MASK_SYSTEM_PROMPT\n","                + \"\\n\\nExample:\\n\"\n","                + MASK_FEW_SHOT\n","                + \"\\n\\nToxic Sentence: \"\n","                + s\n","                + \"\\nFinal Output:\"\n","            )\n","\n","        inputs = _LLM_TOKENIZER(prompt, return_tensors=\"pt\").to(DEVICE_LLM)\n","        gen = _LLM_MODEL.generate(\n","            **inputs,\n","            max_new_tokens=64,\n","            do_sample=False,\n","            temperature=0.0,\n","            pad_token_id=_LLM_TOKENIZER.eos_token_id,\n","        )\n","        # Only decode newly generated tokens\n","        gen_text = _LLM_TOKENIZER.decode(\n","            gen[0][inputs[\"input_ids\"].shape[1]:],\n","            skip_special_tokens=True,\n","        )\n","\n","        # 1) extract bracket content (robust to missing ']')\n","        masked_text = _extract_bracket_content(gen_text)\n","        # 2) normalize for downstream processing\n","        masked_text = _postprocess_llm_mask(masked_text)\n","\n","        # Ensure we at least return something\n","        if not masked_text:\n","            masked_text = s\n","        masked.append(masked_text)\n","\n","    return masked\n"],"metadata":{"id":"nG355GLlqcp-","executionInfo":{"status":"ok","timestamp":1764552042333,"user_tz":480,"elapsed":46,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"nG355GLlqcp-","execution_count":12,"outputs":[]},{"cell_type":"code","source":["#@title LLM infilling (Mistral-7B-Instruct) — fills <mask> tokens\n","\n","INFILL_SYSTEM_PROMPT = \"\"\"You are a helpful assistant trained to make toxic or offensive sentences\n","more polite and respectful by INFILLING the special token <mask>.\n","\n","You are NOT a free rewriter. You must keep all non-masked text as close as possible\n","to the given masked sentence.\n","\n","You are given two inputs:\n","1) Toxic Sentence: the original toxic sentence.\n","2) Masked Sentence: the same sentence, where toxic spans are replaced with <mask>.\n","\n","Your task:\n","1. For each <mask> token in the Masked Sentence, replace it with a short, non-toxic\n","   word or phrase that fits the context and preserves the meaning of the Toxic Sentence.\n","2. Do NOT modify any other words or punctuation outside the <mask> spans, unless a very\n","   small change is needed to fix grammar or agreement.\n","3. Preserve the original meaning and intent as much as possible, but make the sentence\n","   safe and respectful.\n","4. Keep the language the same as the original (do NOT translate).\n","\n","Output rules (VERY STRICT):\n","- ONLY return the final detoxified sentence with all <mask> tokens filled.\n","- Wrap the final sentence in exactly ONE pair of square brackets, e.g.:\n","  [Detoxified sentence here.]\n","- Do NOT include the Toxic Sentence or Masked Sentence in your output.\n","- Do NOT add explanations, comments, or extra lines.\n","- Do NOT include any other '[' or ']' characters.\n","\"\"\"\n","\n","INFILL_FEW_SHOT = \"\"\"Toxic Sentence: You're such a stupid idiot, nobody wants to hear your crap.\n","Masked Sentence: You're such a <mask>, nobody wants to hear your <mask>.\n","Step 1 - Decide safe replacements for each <mask>: \"rude person\", \"opinion\"\n","Step 2 - Infill the masked sentence, keeping all other words the same:\n","You're such a rude person, nobody wants to hear your opinion.\n","Final Output: [You're such a rude person, nobody wants to hear your opinion.]\"\"\"\n","\n","def _postprocess_llm_infill(text: str) -> str:\n","    \"\"\"\n","    Post-process LLM-infilling output:\n","      - remove stray outer brackets if still present,\n","      - normalize whitespace,\n","      - remove any leftover <mask> tokens.\n","    \"\"\"\n","    s = text.strip()\n","\n","    if s.startswith(\"[\") and s.endswith(\"]\") and len(s) > 2:\n","        s = s[1:-1].strip()\n","    else:\n","        if s.startswith(\"[\"):\n","            s = s[1:].strip()\n","        if s.endswith(\"]\"):\n","            s = s[:-1].strip()\n","\n","    s = re.sub(r\"\\s+\", \" \", s).strip()\n","\n","    # Remove any leftover <mask> tokens if the model did not fill them\n","    s = s.replace(\"<mask>\", \" \")\n","    s = re.sub(r\"\\s+\", \" \", s).strip()\n","\n","    if not s:\n","        return text.strip()\n","    return s\n","\n","@torch.no_grad()\n","def llm_infill_candidates(\n","    sources: List[str],\n","    masked: List[str],\n","    num_candidates: int = 3,\n","    temperature: float = 0.7,\n","    top_p: float = 0.95,\n","    max_new_tokens: int = 64,\n","    sample: bool = True,\n",") -> List[List[str]]:\n","    \"\"\"\n","    For each (source, masked) pair, generate `num_candidates` detoxified sentences\n","    by infilling the <mask> tokens using Mistral-7B-Instruct.\n","\n","    Returns:\n","        candidates: list[list[str]] with shape [N][num_candidates]\n","    \"\"\"\n","    assert len(sources) == len(masked), \"sources and masked length mismatch\"\n","    if num_candidates < 1:\n","        raise ValueError(\"num_candidates must be >= 1\")\n","\n","    _lazy_load_llm()\n","    all_cands: List[List[str]] = []\n","\n","    for src, msk in tqdm(\n","        list(zip(sources, masked)),\n","        desc=\"LLM infilling (Mistral)\",\n","        leave=False,\n","    ):\n","        messages = [\n","            {\n","                \"role\": \"system\",\n","                \"content\": INFILL_SYSTEM_PROMPT + \"\\n\\nHere is an example:\\n\" + INFILL_FEW_SHOT,\n","            },\n","            {\n","                \"role\": \"user\",\n","                \"content\": (\n","                    f\"Toxic Sentence: {src}\\n\"\n","                    f\"Masked Sentence: {msk}\\n\"\n","                    f\"Final Output:\"\n","                ),\n","            },\n","        ]\n","        try:\n","            prompt = _LLM_TOKENIZER.apply_chat_template(\n","                messages,\n","                tokenize=False,\n","                add_generation_prompt=True,\n","            )\n","        except Exception:\n","            prompt = (\n","                INFILL_SYSTEM_PROMPT\n","                + \"\\n\\nExample:\\n\"\n","                + INFILL_FEW_SHOT\n","                + \"\\n\\nToxic Sentence: \"\n","                + src\n","                + \"\\nMasked Sentence: \"\n","                + msk\n","                + \"\\nFinal Output:\"\n","            )\n","\n","        inputs = _LLM_TOKENIZER(prompt, return_tensors=\"pt\").to(DEVICE_LLM)\n","        input_len = inputs[\"input_ids\"].shape[1]\n","\n","        gen = _LLM_MODEL.generate(\n","            **inputs,\n","            max_new_tokens=max_new_tokens,\n","            do_sample=sample,\n","            temperature=float(temperature) if sample else 0.0,\n","            top_p=top_p,\n","            num_return_sequences=num_candidates,\n","            pad_token_id=_LLM_TOKENIZER.eos_token_id,\n","        )\n","\n","        cand_list = []\n","        for idx in range(num_candidates):\n","            gen_text = _LLM_TOKENIZER.decode(\n","                gen[idx][input_len:],\n","                skip_special_tokens=True,\n","            )\n","            cleaned = _extract_bracket_content(gen_text)\n","            cleaned = _postprocess_llm_infill(cleaned)\n","            if not cleaned:\n","                cleaned = src  # fall back to original if something goes wrong\n","            cand_list.append(cleaned)\n","\n","        all_cands.append(cand_list)\n","\n","    return all_cands"],"metadata":{"id":"6FFAXYgJpgSs","executionInfo":{"status":"ok","timestamp":1764552042339,"user_tz":480,"elapsed":3,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"6FFAXYgJpgSs","execution_count":13,"outputs":[]},{"cell_type":"code","source":["#@title Global reranking: combine toxicity, similarity, fluency\n","def rerank_candidates_global(\n","    sources,\n","    candidates,\n","    weights=(0.5, 0.3, 0.2),\n","    ppl_min=5.0,\n","    ppl_max=300.0,\n","):\n","    \"\"\"\n","    sources: list[str], length N\n","    candidates: list[list[str]], shape N x C\n","    weights: (w_T, w_S, w_F)\n","\n","    Returns:\n","        best_idx: np.ndarray (N,), index of chosen candidate per source\n","        details: dict with matrices [N x C] for tox, safety, sim, flu, score\n","    \"\"\"\n","    w_T, w_S, w_F = weights\n","    N = len(sources)\n","    assert len(candidates) == N, \"candidates length mismatch\"\n","\n","    if N == 0:\n","        return np.array([], dtype=int), {}\n","\n","    C_list = [len(c) for c in candidates]\n","    assert len(set(C_list)) == 1, \"All inputs must have same num_candidates\"\n","    C = C_list[0]\n","    if C == 0:\n","        raise ValueError(\"num_candidates must be >= 1\")\n","\n","    # Flatten candidates and map to source indices\n","    flat_cands = []\n","    flat_src_idx = []\n","    for i, cand_list in enumerate(candidates):\n","        for cand in cand_list:\n","            flat_cands.append(cand)\n","            flat_src_idx.append(i)\n","    flat_src_idx = np.array(flat_src_idx, dtype=int)\n","\n","    # Toxicity\n","    tox = np.array(get_toxicity_scores(flat_cands), dtype=float)  # [N*C]\n","\n","    # Semantic similarity (LaBSE)\n","    src_embs = get_labse_embeddings(sources)      # [N, D]\n","    cand_embs = get_labse_embeddings(flat_cands)  # [N*C, D]\n","    # Normalize\n","    src_embs = src_embs / np.clip(np.linalg.norm(src_embs, axis=1, keepdims=True), 1e-8, None)\n","    cand_embs = cand_embs / np.clip(np.linalg.norm(cand_embs, axis=1, keepdims=True), 1e-8, None)\n","    # Cosine between each candidate and its source\n","    sims = np.sum(cand_embs * src_embs[flat_src_idx], axis=1)  # [-1,1]\n","    sims = (sims + 1.0) / 2.0  # -> [0,1]\n","\n","    # Fluency: GPT-2 PPL -> F in [0,1]\n","    ppls = np.array(get_gpt2_perplexities(flat_cands), dtype=float)\n","    flus = perplexity_to_fluency(ppls, p_min=ppl_min, p_max=ppl_max)\n","\n","    # Safety\n","    safety = 1.0 - tox\n","\n","    # Global score\n","    scores = w_T * safety + w_S * sims + w_F * flus\n","\n","    # Reshape to [N, C]\n","    tox2     = tox.reshape(N, C)\n","    safety2  = safety.reshape(N, C)\n","    sims2    = sims.reshape(N, C)\n","    flus2    = flus.reshape(N, C)\n","    scores2  = scores.reshape(N, C)\n","\n","    best_idx = scores2.argmax(axis=1)\n","    details = {\n","        \"tox\": tox2,\n","        \"safety\": safety2,\n","        \"sim\": sims2,\n","        \"flu\": flus2,\n","        \"score\": scores2,\n","    }\n","    return best_idx, details"],"metadata":{"id":"PDRoBhdM8461","executionInfo":{"status":"ok","timestamp":1764552042348,"user_tz":480,"elapsed":7,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"PDRoBhdM8461","execution_count":14,"outputs":[]},{"cell_type":"code","source":["#@title Helpers for folder naming\n","def _bool2str(x: bool) -> str:\n","    return \"T\" if x else \"F\"\n","\n","def _build_run_folder_name(\n","    llm_temperature: float,\n","    llm_top_p: float,\n","    llm_sample: bool,\n","    num_candidates: int,\n","    max_new_tokens: int,\n","):\n","    \"\"\"\n","    Build a folder name encoding LLM hyperparameters.\n","    \"\"\"\n","    return (\n","        f\"llmtemp{llm_temperature}_topp{llm_top_p}_\"\n","        f\"sample{_bool2str(llm_sample)}_\"\n","        f\"nc{num_candidates}_\"\n","        f\"maxntok{max_new_tokens}\"\n","    )\n","\n"],"metadata":{"id":"SLDKLRVBcKCy","executionInfo":{"status":"ok","timestamp":1764552042352,"user_tz":480,"elapsed":2,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"SLDKLRVBcKCy","execution_count":15,"outputs":[]},{"cell_type":"code","source":["#@title Masking + LLM infilling + global reranking (per threshold)\n","\n","def _bool2str(x: bool) -> str:\n","    return \"T\" if x else \"F\"\n","\n","def _build_llm_gen_folder_name(\n","    temperature, sample, top_p, max_new_tokens, num_candidates\n","):\n","    return (\n","        \"llm\"\n","        \"_temp\" + str(temperature) +\n","        \"_sample\" + _bool2str(sample) +\n","        \"_topp\" + str(top_p) +\n","        \"_maxnew\" + str(max_new_tokens) +\n","        \"_ncand\" + str(num_candidates)\n","    )\n","\n","def _run_decompx_masking_and_llm_global_reranking_for_threshold(\n","    data_type,\n","    subset_path,\n","    thresh,\n","    base_out_rel,\n","    batch_size_mask,\n","    num_candidates,\n","    weights,\n","    llm_temperature,\n","    llm_top_p,\n","    llm_max_new_tokens,\n","    llm_sample,\n","    overwrite_gen=False,\n","    echo: bool = False,\n","    inputs=None,\n","):\n","    \"\"\"\n","    For one DecompX threshold:\n","      - load inputs using rewrite_example.get_data (or reuse pre-loaded `inputs`)\n","      - mask with DecompX (Masker_single) -> masked_inputs.txt\n","      - LLM infilling (Mistral) -> num_candidates candidates per input\n","      - global reranking (toxicity + similarity + fluency)\n","      - save orig.txt / gen.txt under DecompX{thr}/{run_folder}\n","    \"\"\"\n","    # Load inputs if not provided\n","    if inputs is None:\n","        args_data = _argparse.Namespace(data_type=data_type, data_path=subset_path)\n","        inputs = rx.get_data(args_data)\n","    print(f\"#inputs at thresh={thresh}: {len(inputs)}\")\n","\n","    # Paths\n","    mask_dir = f\"DecompX{abs(thresh):g}\" if thresh != 0 else \"DecompX0.0\"\n","    cur_rel = os.path.join(base_out_rel, data_type, mask_dir)\n","    cur_abs = os.path.join(REPO, cur_rel)\n","    _ensure_dir(cur_abs)\n","\n","    masked_file = os.path.join(cur_abs, \"masked_inputs.txt\")\n","\n","    # DecompX masking (reuse if exists)\n","    if not os.path.exists(masked_file):\n","        masker = Masker_single()\n","        decoded_masked_inputs_batches = _process_in_batches(\n","            masker, inputs, batch_size=batch_size_mask, thresh=thresh\n","        )\n","        decoded_masked_inputs = [\n","            item for sublist in decoded_masked_inputs_batches for item in sublist\n","        ]\n","        decoded_mask_inputs = [\n","            d.replace(\"<s>\", \"\").replace(\"</s>\", \"\") for d in decoded_masked_inputs\n","        ]\n","        with open(masked_file, \"w\") as f:\n","            for d in decoded_mask_inputs:\n","                f.write(re.sub(r\"\\s+\", \" \", d).strip() + \"\\n\")\n","        masker.release_model()\n","    else:\n","        with open(masked_file, \"r\") as f:\n","            decoded_mask_inputs = [s.strip() for s in f.readlines()]\n","        print(\"Reusing existing masked_inputs.txt\")\n","\n","    assert len(decoded_mask_inputs) == len(inputs), \"Masked vs inputs mismatch\"\n","\n","    if echo:\n","        print(\"\\n[echo] Example masked inputs at threshold \"\n","              f\"{thresh:.2f} (first up to 3):\")\n","        for i, m in enumerate(decoded_mask_inputs[:3]):\n","            print(f\"  masked[{i}]: {m}\")\n","\n","    # Build generation folder name for LLM\n","    gen_folder = _build_llm_gen_folder_name(\n","        temperature=llm_temperature,\n","        sample=llm_sample,\n","        top_p=llm_top_p,\n","        max_new_tokens=llm_max_new_tokens,\n","        num_candidates=num_candidates,\n","    )\n","    final_abs = os.path.join(cur_abs, gen_folder)\n","    gen_txt = os.path.join(final_abs, \"gen.txt\")\n","    orig_txt = os.path.join(final_abs, \"orig.txt\")\n","\n","    if os.path.exists(gen_txt) and not overwrite_gen:\n","        print(\"Generation already exists at:\", gen_txt, \"— skipping generation.\")\n","        return\n","\n","    _ensure_dir(final_abs)\n","\n","    # LLM infilling: generate num_candidates per input\n","    print(f\"LLM infilling: {num_candidates} candidates per input (sampling={llm_sample})\")\n","    all_candidates = llm_infill_candidates(\n","        sources=inputs,\n","        masked=decoded_mask_inputs,\n","        num_candidates=num_candidates,\n","        temperature=llm_temperature,\n","        top_p=llm_top_p,\n","        max_new_tokens=llm_max_new_tokens,\n","        sample=llm_sample,\n","    )\n","\n","    # Free LLM model before heavy scoring\n","    global _LLM_MODEL, _LLM_TOKENIZER\n","    try:\n","        del _LLM_MODEL\n","        del _LLM_TOKENIZER\n","    except Exception:\n","        pass\n","    _LLM_MODEL = None\n","    _LLM_TOKENIZER = None\n","    if torch.cuda.is_available() and DEVICE_LLM.type == \"cuda\":\n","        torch.cuda.empty_cache()\n","\n","    # Global reranking\n","    print(\"Global reranking (toxicity + similarity + fluency)...\")\n","    best_idx, details = rerank_candidates_global(\n","        sources=inputs,\n","        candidates=all_candidates,\n","        weights=weights,\n","    )\n","    best_generations = [\n","        all_candidates[i][best_idx[i]] for i in range(len(inputs))\n","    ]\n","\n","    if echo:\n","        print(\"\\n[echo] Example detoxified outputs at threshold \"\n","              f\"{thresh:.2f} (first up to 3):\")\n","        for i, g in enumerate(best_generations[:3]):\n","            print(f\"  detox[{i}]: {g}\")\n","\n","    # Save orig + chosen gen\n","    with open(orig_txt, \"w\") as f:\n","        for l in inputs:\n","            f.write(re.sub(r\"\\s+\", \" \", l).strip() + \"\\n\")\n","    with open(gen_txt, \"w\") as f:\n","        for l in best_generations:\n","            f.write(re.sub(r\"\\s+\", \" \", l).strip() + \"\\n\")\n","\n","    print(\"Saved:\", orig_txt)\n","    print(\"Saved:\", gen_txt)\n"],"metadata":{"id":"U5oOUWRYuA6E","executionInfo":{"status":"ok","timestamp":1764552042364,"user_tz":480,"elapsed":10,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"U5oOUWRYuA6E","execution_count":16,"outputs":[]},{"cell_type":"code","source":["#@title `detoxify()` — LLM masking + LLM infilling + global reranking + optional eval\n","\n","def detoxify(\n","    data_type: str = \"paradetox\",\n","    output_folder: str = \"colab_run_llm_mask_infill_global\",\n","    echo: bool = False,\n","    num_examples: int = 100,       # None = full dataset\n","    overwrite_gen: bool = False,\n","    run_eval: bool = False,\n","    overwrite_eval: bool = False,\n","    skip_ref_eval: bool = False,\n","    # LLM infilling hyperparameters\n","    num_candidates: int = 3,\n","    llm_temperature: float = 0.7,\n","    llm_top_p: float = 0.95,\n","    llm_max_new_tokens: int = 64,\n","    llm_sample: bool = True,\n","    # global reranking\n","    weights = (0.5, 0.3, 0.2),     # (w_T, w_S, w_F)\n","):\n","    \"\"\"\n","    Run XDetox with:\n","      - LLM masking (Mistral-7B-Instruct),\n","      - LLM infilling (Mistral-7B-Instruct),\n","      - global reranking based on:\n","          - toxicity (XLM-R),\n","          - semantic similarity (LaBSE),\n","          - fluency (GPT-2 perplexity -> [0,1]),\n","      - evaluation via evaluation/evaluate_all.py.\n","\n","    No DecompX is used in this notebook.\n","    \"\"\"\n","    assert data_type in data_configs, f\"Unknown data_type: {data_type}\"\n","    cfg = data_configs[data_type].copy()\n","\n","    if num_candidates < 1:\n","        raise ValueError(\"num_candidates must be >= 1\")\n","\n","    base_out_rel = os.path.join(\"data\", \"model_outputs\", output_folder)\n","    base_out_abs = os.path.join(REPO, base_out_rel)\n","    _ensure_dir(base_out_abs)\n","\n","    # subset path (file)\n","    original_data_path = cfg[\"data_path\"]\n","    subset_dir = os.path.join(REPO, \"datasets\", \"_subsets\", data_type)\n","    _ensure_dir(subset_dir)\n","    subset_path = _subset_for_data_type(\n","        data_type, original_data_path, num_examples, subset_dir\n","    )\n","\n","    # Load inputs\n","    args_data = _argparse.Namespace(data_type=data_type, data_path=subset_path)\n","    inputs = rx.get_data(args_data)\n","    num_inputs = len(inputs)\n","\n","    if echo:\n","        print(\"=\" * 80)\n","        print(f\"[echo] Dataset: {data_type}\")\n","        print(f\"[echo] Subset path: {subset_path}\")\n","        print(f\"[echo] Output base: {base_out_abs}\")\n","        print(f\"[echo] Number of examples to detoxify: {num_inputs}\")\n","        print(f\"[echo] Weights (w_T, w_S, w_F): {weights}\")\n","        print(f\"[echo] num_candidates per input: {num_candidates}\")\n","        print(\"\\n[echo] Example inputs (first up to 3):\")\n","        for i, s in enumerate(inputs[:3]):\n","            print(f\"  input[{i}]: {s}\")\n","        print(\"=\" * 80)\n","\n","    # Directory for this pipeline\n","    mask_dir = \"LLM_Mask_LLM_Global\"\n","    cur_rel = os.path.join(base_out_rel, data_type, mask_dir)\n","    cur_abs = os.path.join(REPO, cur_rel)\n","    _ensure_dir(cur_abs)\n","\n","    masked_file = os.path.join(cur_abs, \"masked_inputs.txt\")\n","\n","    # Step 1: LLM masking (reuse cached file if available)\n","    if not os.path.exists(masked_file):\n","        print(\"Running LLM masking (Mistral) to create masked_inputs.txt ...\")\n","        masked_inputs = llm_mask_sentences(inputs)\n","        masked_inputs = [re.sub(r\"\\s+\", \" \", d).strip() for d in masked_inputs]\n","        with open(masked_file, \"w\") as f:\n","            for d in masked_inputs:\n","                f.write(d + \"\\n\")\n","    else:\n","        with open(masked_file, \"r\") as f:\n","            masked_inputs = [s.strip() for s in f.readlines()]\n","        print(\"Reusing existing masked_inputs.txt\")\n","\n","    assert len(masked_inputs) == len(inputs), \"Masked vs inputs mismatch\"\n","\n","    if echo:\n","        print(\"\\n[echo] Example LLM-masked inputs (first up to 3):\")\n","        for i, m in enumerate(masked_inputs[:3]):\n","            print(f\"  masked[{i}]: {m}\")\n","\n","    # Build run folder name for this LLM config\n","    run_folder = _build_run_folder_name(\n","        llm_temperature=llm_temperature,\n","        llm_top_p=llm_top_p,\n","        llm_sample=llm_sample,\n","        num_candidates=num_candidates,\n","        max_new_tokens=llm_max_new_tokens,\n","    )\n","    final_abs = os.path.join(cur_abs, run_folder)\n","    _ensure_dir(final_abs)\n","    orig_txt = os.path.join(final_abs, \"orig.txt\")\n","    gen_txt = os.path.join(final_abs, \"gen.txt\")\n","\n","    # If outputs already exist and we are not overwriting, just load them\n","    if os.path.exists(gen_txt) and not overwrite_gen:\n","        print(\"Generation already exists at:\", gen_txt, \"— skipping generation.\")\n","        with open(gen_txt, \"r\") as f:\n","            best_generations = [s.strip() for s in f.readlines()]\n","        if echo:\n","            print(\"\\n[echo] Example detoxified outputs (first up to 3):\")\n","            for i in range(min(3, len(best_generations))):\n","                print(f\"  detox[{i}]: {best_generations[i]}\")\n","    else:\n","        # Step 2: LLM infilling\n","        print(f\"LLM infilling: generating {num_candidates} candidates per input (sampling={llm_sample})\")\n","        all_candidates = llm_infill_candidates(\n","            sources=inputs,\n","            masked=masked_inputs,\n","            num_candidates=num_candidates,\n","            temperature=llm_temperature,\n","            top_p=llm_top_p,\n","            max_new_tokens=llm_max_new_tokens,\n","            sample=llm_sample,\n","        )\n","\n","        # Free LLM before loading heavy scoring models\n","        global _LLM_MODEL, _LLM_TOKENIZER\n","        try:\n","            del _LLM_MODEL\n","            del _LLM_TOKENIZER\n","        except Exception:\n","            pass\n","        _LLM_MODEL = None\n","        _LLM_TOKENIZER = None\n","        if torch.cuda.is_available() and DEVICE_LLM.type == \"cuda\":\n","            torch.cuda.empty_cache()\n","\n","        # Step 3: global reranking\n","        print(\"Global reranking (toxicity + similarity + fluency)...\")\n","        best_idx, details = rerank_candidates_global(\n","            sources=inputs,\n","            candidates=all_candidates,\n","            weights=weights,\n","        )\n","        best_generations = [\n","            all_candidates[i][best_idx[i]] for i in range(len(inputs))\n","        ]\n","\n","        # Save orig + chosen gen\n","        with open(orig_txt, \"w\") as f:\n","            for l in inputs:\n","                f.write(re.sub(r\"\\s+\", \" \", l).strip() + \"\\n\")\n","        with open(gen_txt, \"w\") as f:\n","            for l in best_generations:\n","                f.write(re.sub(r\"\\s+\", \" \", l).strip() + \"\\n\")\n","\n","        print(\"Saved:\", orig_txt)\n","        print(\"Saved:\", gen_txt)\n","\n","        if echo:\n","            print(\"\\n[echo] Example detoxified outputs (first up to 3):\")\n","            for i in range(min(3, len(best_generations))):\n","                print(f\"  detox[{i}]: {best_generations[i]}\")\n","\n","    # Optional evaluation\n","    if run_eval:\n","        base_path = os.path.join(base_out_abs, data_type, mask_dir)\n","        _eval_with_toxicity(\n","            base_path,\n","            overwrite_eval=overwrite_eval,\n","            skip_ref=skip_ref_eval,\n","            tox_threshold=0.5,\n","            tox_batch_size=32,\n","        )\n","        _aggregate_eval_csv(\n","            output_folder,\n","            data_type,\n","            os.path.join(REPO, \"data\", \"model_outputs\", output_folder),\n","        )\n","\n","        # If echo, print metrics for THIS run from gen_stats.txt\n","        if echo:\n","            stats_path = os.path.join(final_abs, \"gen_stats.txt\")\n","            if os.path.exists(stats_path):\n","                stats = _read_stats_file(stats_path)\n","                print(\"\\n[echo] Evaluation metrics for this run:\")\n","                metric_keys = [\n","                    (\"bertscore\", \"BERTScore\"),\n","                    (\"meaningbert\", \"MeaningBERT\"),\n","                    (\"bleu4\", \"BLEU-4\"),\n","                    (\"perplexity gen\", \"Perplexity (gen)\"),\n","                    (\"perplexity orig\", \"Perplexity (orig)\"),\n","                    (\"toxicity gen\", \"Toxicity (gen)\"),\n","                    (\"toxicity orig\", \"Toxicity (orig)\"),\n","                ]\n","                for key, label in metric_keys:\n","                    val = stats.get(key, None)\n","                    if isinstance(val, float) and math.isnan(val):\n","                        continue\n","                    if val is None:\n","                        continue\n","                    print(f\"  {label}: {val:.4f}\")\n","            else:\n","                print(\"\\n[echo] gen_stats.txt not found for this run; no metrics to print.\")\n"],"metadata":{"id":"oqBLx5OSuA72","executionInfo":{"status":"ok","timestamp":1764552042371,"user_tz":480,"elapsed":4,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"oqBLx5OSuA72","execution_count":17,"outputs":[]},{"cell_type":"code","source":["#@title Example run — paradetox, LLM masking + LLM infilling + global reranking\n","\n","# Example: small demo (adjust num_examples as you like)\n","# detoxify(\n","#     data_type=\"paradetox\",\n","#     output_folder=\"colab_run_llm_mask_infill_global_demo_50_examples\",\n","#     echo=True,\n","#     num_examples=50,           # small subset for testing\n","#     run_eval=True,             # BLEU/BERTScore/MeaningBERT/PPL/Toxicity\n","#     overwrite_gen=True,\n","#     overwrite_eval=True,\n","#     skip_ref_eval=False,\n","#     num_candidates=10,         # LLM candidates per input\n","#     llm_temperature=0.7,\n","#     llm_top_p=0.95,\n","#     llm_max_new_tokens=64,\n","#     llm_sample=True,\n","#     weights=(0.5, 0.3, 0.2),   # safety, similarity, fluency\n","# )"],"metadata":{"id":"u5LlySYquA9g","collapsed":true,"executionInfo":{"status":"ok","timestamp":1764552042394,"user_tz":480,"elapsed":20,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"u5LlySYquA9g","execution_count":18,"outputs":[]},{"cell_type":"code","source":["detoxify(\n","    data_type=\"paradetox\",\n","    output_folder=\"XDetox_w_LLM-Masking_LLM-Infilling_Global-Reranking_Pipeline\",\n","    echo=True,\n","    num_examples=1000,\n","    run_eval=True,             # BLEU/BERTScore/MeaningBERT/PPL/Toxicity\n","    overwrite_gen=True,\n","    overwrite_eval=True,\n","    skip_ref_eval=False,\n","    num_candidates=10,         # LLM candidates per input\n","    llm_temperature=0.7,\n","    llm_top_p=0.95,\n","    llm_max_new_tokens=64,\n","    llm_sample=True,\n","    weights=(0.5, 0.3, 0.2),   # safety, similarity, fluency\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":813,"referenced_widgets":["bbd942a3a88e48a9a11b37ff4a5f5bd0","bb94d14ff6934d4fa2a48894cee2d575","fbf003e5018d4b888a80fde014ccfb33","1535ba05dd384032a864fdb897d7e1c2","c74262d3fe534f58b05a53eb1a36b638","73e79c3f174a4b979520d304d8136bb9","af03d7d9fe9d4a5491258d22fd1c1a61","5120f4eee056441a8dc8b3ae0c8b4ad4","b88010384b5d497b8383cb35cfc42396","bfef3c8bdd1a46629f29292dff771c85","63d6bbcc101948b5b0215a3619b25325","84962f8f05884e99a3ef8e44c733fb42","2f9ae3e979494afcbd66eb4e5f8db9e0","27d2f02dcb9a4a0e922f624a3bb70a04","743294610c51466ab160e6251957c37e","45f2a340c0684eb0a1ee0123bf2801b4","6b09f02bdeec4085a8c8e5018c9f5535","4e29b796e1cd4c07ac3e4523a9fe655f","24f560fbcdf945e58aa199d8509d22e8","73f629c1befb4d2bbf5f22f99bc24ff3","d33fcc3d086c4e65ac4d8831ba54c633","4dffbda24d3c40b9ac65b7e5afa8c84d","d0b888f88e31424b902666b4a90292c6","2883f5ab17444714bb8eb57a5fec9e73","1effe7857bb045d3a37990573abd56df","cb45a3ea1d8f40d98a2566b6570c7a68","37bed018dcd64a369a8b5952c92bf44d","9e42f8e77cc64635bcc92570b6303c1d","585b7f4959984fe2b17d098007ef07c3","c7831547f3e04bb294138cfe11c39dc4","62f4770af1414f89a78d4a87c91be6f2","7a0a6701ee9b450ea02fdabb9fe14202","0f4417e567e34254958bc5c9ea7a7037","bfdee28990b947ae88d8868d595650f3","331afce0a54a4601ac6443d12193644d","6b70bbfbc5df44939cac122bf416551e","3f8f581f04fa4b0e8604d1da7a5e810f","ac6fea544d7d492f982a232e30746399","c25604f59f38468480876f84d86310c1","6a1acd7608714cbcaf9e146580ee4f29","06fca227abf548a78e3b0d0f7b6975ba","e8a8b792ea284826b76a296488559c4f","ca6d66e89fcb4655a040199424c14ccc","002c9c6ffe144697866f5ca9819688ef","d9318a3714264889b8c013ca0ba18866","4df00a5530204418a6f56c04123b339e","838d4f9c15ff490996ccbbe41a66b6aa","40348d03ffa74e28a05dc4b23d372ddf","7975d5791bea468896ce8d9bf9cc82c7","7727b10bb3c94b01bf95907f55605a16","b6dd27dee86e484188518a4dadf69184","2e6c55cdaca74777abe99aac1e4103a3","9892a501d56646889c3ae57b86f085a8","9597fcd63d7843cd86ac45d9cda1f0ea","f083712efcdc43caa8bfa1d1e309a603","fbac33301bab4c15a33114fa3a54f397","c0e4ea3f069549e9b94513a8ed84de8e","1c86e7b091cb4126820382f78699fa6d","277d8a28e8dc4383bac0414961942c3f","a581d89d27f94b61957705a458427d90","e4306487a590499d89aa90898dab578a","bf3c25a9b9774a068676ead26f2c8bf2","4a3bde441dc340f5b3b08672f074e6c3","19817602f4bb40bba0a918e85f49fe3c","63574a7723d1476e92d9b18dd9c1d8f9","e69535049c2b4404a30419f9c25584a3","821b5990e0904fc39c681d6de17586a2","22c3c841fc1e4c14bee092b0739426b0","5dfa704f46ad4ca7a8bc99602b2965a9","15671f9c74fc4ef09b9340d736bae8a0","d60800f8b47b4e88b2d9374c514656e8","757274e2997b4ecebc58188cba715fe5","9a9bab9e08c64b7a8f84c74e32a48f9a","2fb588e5f14b4c98ae6e907a8eef209d","f4acb70379854f06a2a29ded8b122490","a17529a935694e17b7f1385610aa87f3","65add36f06dd498d94772d5e9826c6b8"]},"id":"O2S9fX-adatU","executionInfo":{"status":"ok","timestamp":1764554458922,"user_tz":480,"elapsed":2416525,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"6e8ca10f-1d7a-4c0f-c769-d0c6c15e7892"},"id":"O2S9fX-adatU","execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","[echo] Dataset: paradetox\n","[echo] Subset path: /content/drive/MyDrive/w266 - Project/XDetox/datasets/_subsets/paradetox/test_toxic_parallel.txt\n","[echo] Output base: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_LLM-Infilling_Global-Reranking_Pipeline\n","[echo] Number of examples to detoxify: 671\n","[echo] Weights (w_T, w_S, w_F): (0.5, 0.3, 0.2)\n","[echo] num_candidates per input: 10\n","\n","[echo] Example inputs (first up to 3):\n","  input[0]: . or the loud ass one - thousand ton beast roaring towards you howling its horn .\n","  input[1]: mandated  and \" right fucking now \" would be good .\n","  input[2]: * neither * of my coworkers gave a shit when it came time to ditch mitch . ugh .\n","================================================================================\n","Reusing existing masked_inputs.txt\n","\n","[echo] Example LLM-masked inputs (first up to 3):\n","  masked[0]: . or the loud <mask> one - thousand ton beast roaring towards you howling its horn .\n","  masked[1]: mandated and \"<mask> right <mask> now <mask> \" would be good .\n","  masked[2]: neither of my coworkers gave a <mask> when it came time to ditch mitch . ugh .\n","LLM infilling: generating 10 candidates per input (sampling=True)\n","Loading LLM (Mistral-7B-Instruct): mistralai/Mistral-7B-Instruct-v0.2 on cuda ...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbd942a3a88e48a9a11b37ff4a5f5bd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84962f8f05884e99a3ef8e44c733fb42"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["LLM loaded.\n"]},{"output_type":"display_data","data":{"text/plain":["LLM infilling (Mistral):   0%|          | 0/671 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0b888f88e31424b902666b4a90292c6"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Global reranking (toxicity + similarity + fluency)...\n"]},{"output_type":"display_data","data":{"text/plain":["Toxicity:   0%|          | 0/210 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfdee28990b947ae88d8868d595650f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["LaBSE embeddings:   0%|          | 0/21 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9318a3714264889b8c013ca0ba18866"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["LaBSE embeddings:   0%|          | 0/210 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbac33301bab4c15a33114fa3a54f397"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["GPT-2 PPL:   0%|          | 0/6710 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"821b5990e0904fc39c681d6de17586a2"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saved: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_LLM-Infilling_Global-Reranking_Pipeline/paradetox/LLM_Mask_LLM_Global/llmtemp0.7_topp0.95_sampleT_nc10_maxntok64/orig.txt\n","Saved: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_LLM-Infilling_Global-Reranking_Pipeline/paradetox/LLM_Mask_LLM_Global/llmtemp0.7_topp0.95_sampleT_nc10_maxntok64/gen.txt\n","\n","[echo] Example detoxified outputs (first up to 3):\n","  detox[0]: or the loud obnoxious one - thousand ton beast roaring towards you howling its horn.\n","  detox[1]: mandated and \"immediately\" \"right now\" would be good.\n","  detox[2]: neither of my coworkers gave a lackadaisical response when it came time to let Mitch go. ugh.\n","Eval: /usr/bin/python3 -m evaluation.evaluate_all --orig_path /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_LLM-Infilling_Global-Reranking_Pipeline/paradetox/LLM_Mask_LLM_Global/llmtemp0.7_topp0.95_sampleT_nc10_maxntok64/orig.txt --gen_path /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_LLM-Infilling_Global-Reranking_Pipeline/paradetox/LLM_Mask_LLM_Global/llmtemp0.7_topp0.95_sampleT_nc10_maxntok64/gen.txt --tox_threshold 0.5 --tox_batch_size 32\n","Wrote summary CSV: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_LLM-Infilling_Global-Reranking_Pipeline/paradetox/paradetox.csv\n","\n","[echo] Evaluation metrics for this run:\n","  BERTScore: 0.9312\n","  MeaningBERT: 62.4520\n","  BLEU-4: 81.5360\n","  Perplexity (gen): 141.8900\n","  Perplexity (orig): 273.7500\n","  Toxicity (gen): 0.1177\n","  Toxicity (orig): 0.9253\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"VgZRbEBED8w4","executionInfo":{"status":"ok","timestamp":1764554458927,"user_tz":480,"elapsed":8,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"VgZRbEBED8w4","execution_count":19,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"A100","machine_shape":"hm"},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"bbd942a3a88e48a9a11b37ff4a5f5bd0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bb94d14ff6934d4fa2a48894cee2d575","IPY_MODEL_fbf003e5018d4b888a80fde014ccfb33","IPY_MODEL_1535ba05dd384032a864fdb897d7e1c2"],"layout":"IPY_MODEL_c74262d3fe534f58b05a53eb1a36b638"}},"bb94d14ff6934d4fa2a48894cee2d575":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73e79c3f174a4b979520d304d8136bb9","placeholder":"​","style":"IPY_MODEL_af03d7d9fe9d4a5491258d22fd1c1a61","value":"Downloading shards: 100%"}},"fbf003e5018d4b888a80fde014ccfb33":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5120f4eee056441a8dc8b3ae0c8b4ad4","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b88010384b5d497b8383cb35cfc42396","value":3}},"1535ba05dd384032a864fdb897d7e1c2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bfef3c8bdd1a46629f29292dff771c85","placeholder":"​","style":"IPY_MODEL_63d6bbcc101948b5b0215a3619b25325","value":" 3/3 [00:01&lt;00:00,  1.58it/s]"}},"c74262d3fe534f58b05a53eb1a36b638":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73e79c3f174a4b979520d304d8136bb9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af03d7d9fe9d4a5491258d22fd1c1a61":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5120f4eee056441a8dc8b3ae0c8b4ad4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b88010384b5d497b8383cb35cfc42396":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bfef3c8bdd1a46629f29292dff771c85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63d6bbcc101948b5b0215a3619b25325":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"84962f8f05884e99a3ef8e44c733fb42":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2f9ae3e979494afcbd66eb4e5f8db9e0","IPY_MODEL_27d2f02dcb9a4a0e922f624a3bb70a04","IPY_MODEL_743294610c51466ab160e6251957c37e"],"layout":"IPY_MODEL_45f2a340c0684eb0a1ee0123bf2801b4"}},"2f9ae3e979494afcbd66eb4e5f8db9e0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b09f02bdeec4085a8c8e5018c9f5535","placeholder":"​","style":"IPY_MODEL_4e29b796e1cd4c07ac3e4523a9fe655f","value":"Loading checkpoint shards: 100%"}},"27d2f02dcb9a4a0e922f624a3bb70a04":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_24f560fbcdf945e58aa199d8509d22e8","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_73f629c1befb4d2bbf5f22f99bc24ff3","value":3}},"743294610c51466ab160e6251957c37e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d33fcc3d086c4e65ac4d8831ba54c633","placeholder":"​","style":"IPY_MODEL_4dffbda24d3c40b9ac65b7e5afa8c84d","value":" 3/3 [03:14&lt;00:00, 61.06s/it]"}},"45f2a340c0684eb0a1ee0123bf2801b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b09f02bdeec4085a8c8e5018c9f5535":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e29b796e1cd4c07ac3e4523a9fe655f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"24f560fbcdf945e58aa199d8509d22e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73f629c1befb4d2bbf5f22f99bc24ff3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d33fcc3d086c4e65ac4d8831ba54c633":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4dffbda24d3c40b9ac65b7e5afa8c84d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d0b888f88e31424b902666b4a90292c6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2883f5ab17444714bb8eb57a5fec9e73","IPY_MODEL_1effe7857bb045d3a37990573abd56df","IPY_MODEL_cb45a3ea1d8f40d98a2566b6570c7a68"],"layout":"IPY_MODEL_37bed018dcd64a369a8b5952c92bf44d"}},"2883f5ab17444714bb8eb57a5fec9e73":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e42f8e77cc64635bcc92570b6303c1d","placeholder":"​","style":"IPY_MODEL_585b7f4959984fe2b17d098007ef07c3","value":"LLM infilling (Mistral): 100%"}},"1effe7857bb045d3a37990573abd56df":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7831547f3e04bb294138cfe11c39dc4","max":671,"min":0,"orientation":"horizontal","style":"IPY_MODEL_62f4770af1414f89a78d4a87c91be6f2","value":671}},"cb45a3ea1d8f40d98a2566b6570c7a68":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a0a6701ee9b450ea02fdabb9fe14202","placeholder":"​","style":"IPY_MODEL_0f4417e567e34254958bc5c9ea7a7037","value":" 671/671 [27:41&lt;00:00,  2.88s/it]"}},"37bed018dcd64a369a8b5952c92bf44d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"9e42f8e77cc64635bcc92570b6303c1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"585b7f4959984fe2b17d098007ef07c3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c7831547f3e04bb294138cfe11c39dc4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62f4770af1414f89a78d4a87c91be6f2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7a0a6701ee9b450ea02fdabb9fe14202":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f4417e567e34254958bc5c9ea7a7037":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bfdee28990b947ae88d8868d595650f3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_331afce0a54a4601ac6443d12193644d","IPY_MODEL_6b70bbfbc5df44939cac122bf416551e","IPY_MODEL_3f8f581f04fa4b0e8604d1da7a5e810f"],"layout":"IPY_MODEL_ac6fea544d7d492f982a232e30746399"}},"331afce0a54a4601ac6443d12193644d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c25604f59f38468480876f84d86310c1","placeholder":"​","style":"IPY_MODEL_6a1acd7608714cbcaf9e146580ee4f29","value":"Toxicity:  99%"}},"6b70bbfbc5df44939cac122bf416551e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_06fca227abf548a78e3b0d0f7b6975ba","max":210,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e8a8b792ea284826b76a296488559c4f","value":210}},"3f8f581f04fa4b0e8604d1da7a5e810f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca6d66e89fcb4655a040199424c14ccc","placeholder":"​","style":"IPY_MODEL_002c9c6ffe144697866f5ca9819688ef","value":" 208/210 [00:08&lt;00:00, 24.22it/s]"}},"ac6fea544d7d492f982a232e30746399":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"c25604f59f38468480876f84d86310c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a1acd7608714cbcaf9e146580ee4f29":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"06fca227abf548a78e3b0d0f7b6975ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8a8b792ea284826b76a296488559c4f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ca6d66e89fcb4655a040199424c14ccc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"002c9c6ffe144697866f5ca9819688ef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d9318a3714264889b8c013ca0ba18866":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4df00a5530204418a6f56c04123b339e","IPY_MODEL_838d4f9c15ff490996ccbbe41a66b6aa","IPY_MODEL_40348d03ffa74e28a05dc4b23d372ddf"],"layout":"IPY_MODEL_7975d5791bea468896ce8d9bf9cc82c7"}},"4df00a5530204418a6f56c04123b339e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7727b10bb3c94b01bf95907f55605a16","placeholder":"​","style":"IPY_MODEL_b6dd27dee86e484188518a4dadf69184","value":"LaBSE embeddings:  90%"}},"838d4f9c15ff490996ccbbe41a66b6aa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e6c55cdaca74777abe99aac1e4103a3","max":21,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9892a501d56646889c3ae57b86f085a8","value":21}},"40348d03ffa74e28a05dc4b23d372ddf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9597fcd63d7843cd86ac45d9cda1f0ea","placeholder":"​","style":"IPY_MODEL_f083712efcdc43caa8bfa1d1e309a603","value":" 19/21 [00:00&lt;00:00, 59.91it/s]"}},"7975d5791bea468896ce8d9bf9cc82c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"7727b10bb3c94b01bf95907f55605a16":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6dd27dee86e484188518a4dadf69184":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2e6c55cdaca74777abe99aac1e4103a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9892a501d56646889c3ae57b86f085a8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9597fcd63d7843cd86ac45d9cda1f0ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f083712efcdc43caa8bfa1d1e309a603":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fbac33301bab4c15a33114fa3a54f397":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c0e4ea3f069549e9b94513a8ed84de8e","IPY_MODEL_1c86e7b091cb4126820382f78699fa6d","IPY_MODEL_277d8a28e8dc4383bac0414961942c3f"],"layout":"IPY_MODEL_a581d89d27f94b61957705a458427d90"}},"c0e4ea3f069549e9b94513a8ed84de8e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4306487a590499d89aa90898dab578a","placeholder":"​","style":"IPY_MODEL_bf3c25a9b9774a068676ead26f2c8bf2","value":"LaBSE embeddings:  97%"}},"1c86e7b091cb4126820382f78699fa6d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a3bde441dc340f5b3b08672f074e6c3","max":210,"min":0,"orientation":"horizontal","style":"IPY_MODEL_19817602f4bb40bba0a918e85f49fe3c","value":210}},"277d8a28e8dc4383bac0414961942c3f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_63574a7723d1476e92d9b18dd9c1d8f9","placeholder":"​","style":"IPY_MODEL_e69535049c2b4404a30419f9c25584a3","value":" 204/210 [00:02&lt;00:00, 72.47it/s]"}},"a581d89d27f94b61957705a458427d90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"e4306487a590499d89aa90898dab578a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf3c25a9b9774a068676ead26f2c8bf2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4a3bde441dc340f5b3b08672f074e6c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19817602f4bb40bba0a918e85f49fe3c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"63574a7723d1476e92d9b18dd9c1d8f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e69535049c2b4404a30419f9c25584a3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"821b5990e0904fc39c681d6de17586a2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_22c3c841fc1e4c14bee092b0739426b0","IPY_MODEL_5dfa704f46ad4ca7a8bc99602b2965a9","IPY_MODEL_15671f9c74fc4ef09b9340d736bae8a0"],"layout":"IPY_MODEL_d60800f8b47b4e88b2d9374c514656e8"}},"22c3c841fc1e4c14bee092b0739426b0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_757274e2997b4ecebc58188cba715fe5","placeholder":"​","style":"IPY_MODEL_9a9bab9e08c64b7a8f84c74e32a48f9a","value":"GPT-2 PPL: 100%"}},"5dfa704f46ad4ca7a8bc99602b2965a9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_2fb588e5f14b4c98ae6e907a8eef209d","max":6710,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f4acb70379854f06a2a29ded8b122490","value":6710}},"15671f9c74fc4ef09b9340d736bae8a0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a17529a935694e17b7f1385610aa87f3","placeholder":"​","style":"IPY_MODEL_65add36f06dd498d94772d5e9826c6b8","value":" 6704/6710 [01:15&lt;00:00, 88.31it/s]"}},"d60800f8b47b4e88b2d9374c514656e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"757274e2997b4ecebc58188cba715fe5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a9bab9e08c64b7a8f84c74e32a48f9a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2fb588e5f14b4c98ae6e907a8eef209d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4acb70379854f06a2a29ded8b122490":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a17529a935694e17b7f1385610aa87f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65add36f06dd498d94772d5e9826c6b8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}
{"cells":[{"cell_type":"markdown","id":"82d4549b","metadata":{"id":"82d4549b"},"source":["# XDetox with LLM Masking and Global Reranking\n","\n","This notebook runs the full XDetox pipeline with:\n","\n","1. **LLM masking** using Mistral-7B-Instruct (`mistralai/Mistral-7B-Instruct-v0.2`), which detects toxic spans and replaces them with `<mask>`.\n","2. **MaRCo-style generation** (base / expert / anti-expert BART mixture).\n","3. **Global reranking** of multiple candidates per input using:\n","   - **Toxicity** (XLM-R large classifier).\n","   - **Semantic similarity** (LaBSE).\n","   - **Fluency** (GPT-2 perplexity).\n","\n","The goal is to pick, for each toxic input sentence, **one best detoxified candidate** that is:\n","\n","- As **non-toxic** as possible.\n","- As **semantically close** as possible to the original.\n","- As **fluent** as possible.\n","\n","The main difference from the original DecompX-based pipeline is that **masking is done by an LLM instead of DecompX**, and there is **no DecompX threshold hyperparameter**. The DecompX model is not used in this notebook.\n","\n","---\n","\n","## Scoring: Global Reranking\n","\n","For each candidate $c$, we compute:\n","\n","- $T(c)$: toxicity in $[0, 1]$ from `textdetox/xlmr-large-toxicity-classifier-v2`.\n","- $S(c)$: semantic similarity in $[0, 1]$, from LaBSE cosine similarity.\n","- $F(c)$: fluency in $[0, 1]$, from GPT-2 perplexity mapped into a score  \n","  (low perplexity → high fluency).\n","\n","We convert toxicity into a **safety** score:\n","\n","$$\n","T'(c) = 1 - T(c)\n","$$\n","\n","Then we form a **global score**:\n","\n","$$\n","\\text{Score}(c) = w_T \\cdot T'(c) + w_S \\cdot S(c) + w_F \\cdot F(c)\n","$$\n","\n","You control the weights:\n","\n","- `weights = (w_T, w_S, w_F)`  \n","  - `w_T`: importance of **safety** (low toxicity).  \n","  - `w_S`: importance of **semantic similarity**.  \n","  - `w_F`: importance of **fluency**.\n","\n","For each input sentence, we:\n","\n","1. Generate `num_candidates` candidates.\n","2. Score each candidate with the formula above.\n","3. Select the **highest-scoring** candidate as the final output.\n","\n","---\n","\n","## LLM Masking (Mistral-7B-Instruct)\n","\n","### Prompted masking behavior\n","\n","Masking is done by a chat-style LLM:\n","\n","- Model: `mistralai/Mistral-7B-Instruct-v0.2`.\n","- The LLM is instructed to:\n","  - **Identify toxic, offensive, or profane words or short phrases**.\n","  - Replace **each toxic span** with a **single `<mask>` token**.\n","  - Allow **multiple `<mask>` tokens** in one sentence (one per toxic span).\n","  - If multiple neighboring words are toxic, **collapse them into one `<mask>`**  \n","    (so it does not produce `<mask> <mask> ...` sequences).\n","  - Keep **all non-toxic words and punctuation in place**.\n","  - **Not** paraphrase, summarize, or otherwise rewrite the sentence.\n","  - Return the masked sentence **inside exactly one pair of brackets**:\n","\n","    ```text\n","    [This is a <mask> example.]\n","    ```\n","\n","### Post-processing of LLM masks\n","\n","Because LLM output is noisy, the notebook cleans the raw output before feeding it into MaRCo. For each LLM output, we:\n","\n","1. **Extract the bracket content**:\n","\n","   - Try to read the first `[ ... ]` block.\n","   - If there is `[` but no `]`, take everything after the first `[` as the sentence.\n","   - If no brackets exist, fall back to the full string.\n","\n","2. **Strip stray outer brackets** that might still remain.\n","\n","3. **Normalize whitespace** (collapse multiple spaces).\n","\n","4. **Normalize `<mask>` tokens**:\n","\n","   - Any variant like `<Mask>`, `<MASK>`, `< mask >` is normalized to `<mask>`.\n","\n","5. **Collapse runs of `<mask>`**:\n","\n","   - Any sequence like `<mask> <mask> <mask>` becomes a single `<mask>`.\n","   - This avoids the earlier failure mode where the LLM produced many `<mask>` tokens in a row, which confused the generator.\n","\n","6. If, after cleaning, the sentence becomes empty, fall back to the original masked text.\n","\n","All cleaned, LLM-masked sentences are saved to:\n","\n","- `data/model_outputs/{output_folder}/{data_type}/LLM_Masking/masked_inputs.txt`\n","\n","and reused across later runs with the same `output_folder` and `data_type`.\n","\n","---\n","\n","## Generation (MaRCo: BART base / expert / anti-expert)\n","\n","For each dataset:\n","\n","1. **Subset selection**\n","\n","   - The script can run on the full dataset or only on the first `num_examples` rows.\n","   - A subset file is written under:\n","\n","     ```text\n","     datasets/_subsets/{data_type}/\n","     ```\n","\n","2. **LLM masking**\n","\n","   - Input toxic sentences are masked by Mistral-7B-Instruct as described above.\n","   - Cleaned masked sentences are written to:\n","\n","     ```text\n","     data/model_outputs/{output_folder}/{data_type}/LLM_Masking/masked_inputs.txt\n","     ```\n","\n","3. **MaRCo generation**\n","\n","   - We use `rewrite.generation.Infiller` with:\n","     - Base model: `facebook/bart-base`,\n","     - Anti-expert (toxic) model: `hallisky/bart-base-toxic-antiexpert`,\n","     - Expert (non-toxic) model: `hallisky/bart-base-nontoxic-expert`.\n","   - Generation is controlled by:\n","     - `alpha_a`, `alpha_e`, `alpha_b`: anti-expert, expert, base weights,\n","     - `temperature`,\n","     - `top_k_gen`, `top_p`, `filter_p`,\n","     - `rep_penalty`,\n","     - `max_length`,\n","     - `sample` (sampling vs greedy decoding).\n","   - For each input sentence, the notebook **samples `num_candidates` candidates**.\n","\n","4. **Global reranking**\n","\n","   - For each input, all candidates are scored with the **global score** using:\n","     - XLM-R toxicity classifier,\n","     - LaBSE similarity,\n","     - GPT-2 perplexity.\n","   - The candidate with the **highest global score** is chosen.\n","   - For each run folder, we write:\n","     - `orig.txt` — original toxic inputs (one per line),\n","     - `gen.txt` — chosen detoxified outputs (one per line).\n","\n","Outputs are stored under:\n","\n","```text\n","data/model_outputs/{output_folder}/{data_type}/LLM_Masking/{run_folder}/\n","````\n","\n","where `{run_folder}` encodes model and decoding hyperparameters (alphas, temperature, top-k, top-p, etc.).\n","\n","---\n","\n","## Evaluation\n","\n","If `run_eval=True`, the pipeline calls `evaluation.evaluate_all` to compute:\n","\n","* BERTScore (F1)\n","* MeaningBERT\n","* BLEU-4\n","* Toxicity (orig / gen) using XLM-R\n","* Perplexity (orig / gen) using GPT-2\n","\n","For each run folder, it writes:\n","\n","* `gen_stats.txt` under:\n","\n","  ```text\n","  data/model_outputs/{output_folder}/{data_type}/LLM_Masking/{run_folder}/\n","  ```\n","\n","It also creates a **summary CSV per dataset**:\n","\n","* `data/model_outputs/{output_folder}/{data_type}/{data_type}.csv`\n","\n","The CSV aggregates metrics over all run folders in `LLM_Masking/`.\n","The `threshold` column is kept as a **fixed label (`0.20`)** for compatibility, but it does **not** control masking here (there is no DecompX threshold).\n","\n","---\n","\n","## How to Use `detoxify()`\n","\n","Function signature:\n","\n","```python\n","def detoxify(\n","    data_type: str = \"paradetox\",\n","    output_folder: str = \"colab_run_llm_mask\",\n","    echo: bool = False,\n","    batch_size: int = 10,\n","    sample: bool = True,\n","    top_k_gen: int = 50,\n","    top_p: float = 0.95,\n","    filter_p: float = 1.0,\n","    max_length: int = 128,\n","    alpha_a: float = None,\n","    alpha_e: float = None,\n","    alpha_b: float = 1.0,\n","    temperature: float = None,\n","    rep_penalty: float = None,\n","    num_examples: int = 100,\n","    overwrite_gen: bool = False,\n","    run_eval: bool = False,\n","    overwrite_eval: bool = False,\n","    skip_ref_eval: bool = False,\n","    # global reranking:\n","    weights = (0.5, 0.3, 0.2),\n","    num_candidates: int = 3,\n",")\n","```\n","\n","### Key arguments\n","\n","#### Core I/O\n","\n","* `data_type`: dataset key from `data_configs`, for example:\n","\n","  * `\"paradetox\"`, `\"dynabench_val\"`, `\"jigsaw_toxic\"`,\n","  * `\"microagressions_val\"`, `\"sbf_val\"`, `\"appdia_original\"`, etc.\n","\n","* `output_folder`: top-level directory under `data/model_outputs/` where results are stored:\n","\n","  ```text\n","  data/model_outputs/{output_folder}/{data_type}/...\n","  ```\n","\n","* `num_examples`: if set, only the first `num_examples` examples are used (for quick tests).\n","  Use `None` to run on the full dataset.\n","\n","#### LLM masking (Mistral)\n","\n","* Masking uses **Mistral-7B-Instruct** with a fixed system prompt and a few-shot example.\n","* The LLM decides which spans to mask; there is **no numeric threshold**.\n","* The notebook **caches** LLM-masked sentences to `masked_inputs.txt` and reuses them if they exist.\n","\n","You do not pass masking options directly into `detoxify()`.\n","The masking behavior is controlled by the fixed prompt and post-processing code.\n","\n","#### Generation (MaRCo / BART)\n","\n","* `sample`:\n","\n","  * `True`: stochastic sampling.\n","  * `False`: greedy decoding.\n","* `top_k_gen`: top-k on ensembled logits for sampling.\n","* `top_p`: nucleus sampling on ensembled logits.\n","* `filter_p`: nucleus filter on **base** logits (advanced; often `1.0`).\n","* `max_length`: maximum generation length (in tokens).\n","* `alpha_a`, `alpha_e`, `alpha_b`:\n","\n","  * Anti-expert, expert, and base weights for MaRCo.\n","  * If `None`, defaults come from `data_configs[data_type]`.\n","* `temperature`: sampling temperature; if `None`, uses dataset default.\n","* `rep_penalty`: repetition penalty; if `None`, uses dataset default.\n","* `batch_size`: generation batch size.\n","\n","#### Global reranking\n","\n","* `weights = (w_T, w_S, w_F)`:\n","\n","  * `w_T`: safety (1 − toxicity).\n","  * `w_S`: semantic similarity.\n","  * `w_F`: fluency.\n","* `num_candidates`: how many candidates to generate per input.\n","\n","  * Larger values give better reranking at higher computational cost.\n","\n","#### Evaluation\n","\n","* `run_eval`: if `True`, run evaluation and write `gen_stats.txt`.\n","* `overwrite_gen`:\n","\n","  * If `False` and `gen.txt` exists, reuse existing generations.\n","  * If `True`, regenerate even if `gen.txt` exists.\n","* `overwrite_eval`:\n","\n","  * If `False` and `gen_stats.txt` exists, keep existing evaluation.\n","  * If `True`, recompute evaluation.\n","* `skip_ref_eval`: if `True`, skip some reference-based evaluation (for example, perplexity on gold references).\n","\n","#### Echo / debugging\n","\n","* `echo`:\n","\n","  * If `True`, print:\n","\n","    * Basic dataset and output information,\n","    * A few example inputs,\n","    * A few LLM-masked sentences,\n","    * A few final detoxified outputs,\n","    * And (if `run_eval=True`) evaluation metrics for the specific run.\n","\n","---\n","\n","## Example Calls\n","\n","### Quick sanity check on a small subset\n","\n","```python\n","detoxify(\n","    data_type=\"paradetox\",\n","    output_folder=\"colab_run_llm_mask_V2_demo_50_examples\",\n","    echo=True,\n","    batch_size=8,\n","    sample=True,\n","    top_k_gen=50,\n","    top_p=0.95,\n","    max_length=96,\n","    num_examples=50,           # small subset for testing\n","    run_eval=True,             # BLEU / BERTScore / MeaningBERT / PPL / Toxicity\n","    overwrite_gen=True,\n","    overwrite_eval=True,\n","    skip_ref_eval=False,\n","    weights=(0.5, 0.3, 0.2),   # safety, similarity, fluency\n","    num_candidates=10,         # candidates per input\n",")\n","```\n","\n","### Larger run (more candidates, same dataset)\n","\n","```python\n","detoxify(\n","    data_type=\"paradetox\",\n","    output_folder=\"paradetox_llm_mask_global\",\n","    echo=True,\n","    batch_size=8,\n","    sample=True,\n","    top_k_gen=50,\n","    top_p=0.95,\n","    max_length=96,\n","    num_examples=None,         # full dataset\n","    run_eval=True,\n","    overwrite_gen=False,\n","    overwrite_eval=False,\n","    skip_ref_eval=False,\n","    weights=(0.5, 0.3, 0.2),\n","    num_candidates=20,\n",")\n","```\n","\n","After running `detoxify`, you can inspect:\n","\n","* `orig.txt` and `gen.txt` under:\n","\n","  ```text\n","  data/model_outputs/{output_folder}/{data_type}/LLM_Masking/{run_folder}/\n","  ```\n","\n","* Per-run metrics:\n","\n","  ```text\n","  data/model_outputs/{output_folder}/{data_type}/LLM_Masking/{run_folder}/gen_stats.txt\n","  ```\n","\n","* Aggregated metrics:\n","\n","  ```text\n","  data/model_outputs/{output_folder}/{data_type}/{data_type}.csv\n","  ```\n","\n","This setup lets you directly compare:\n","\n","* The original **DecompX-masking + global reranking pipeline**, and\n","* The new **LLM-masking + global reranking pipeline**\n","\n","on the same datasets and with the same global scoring scheme.\n"]},{"cell_type":"code","source":["#@title Mount Drive, Imports & locate XDetox\n","from google.colab import drive; drive.mount('/content/drive')\n","\n","import os, glob, re, sys, json, shutil, math\n","import numpy as np\n","import pandas as pd\n","from tqdm.auto import tqdm\n","from pathlib import Path\n","from subprocess import run, PIPE\n","import torch\n","import nltk\n","from typing import List\n","\n","# Try My Drive\n","candidate = \"/content/drive/MyDrive/w266 - Project/XDetox\"\n","print(\"Try MyDrive:\", candidate, \"->\", os.path.isdir(candidate))\n","\n","XDETOX_DIR = candidate\n","print(\"Using XDETOX_DIR:\", XDETOX_DIR)\n","assert os.path.isdir(XDETOX_DIR), f\"XDETOX_DIR does not exist: {XDETOX_DIR}\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kfBoQTrjtynY","executionInfo":{"status":"ok","timestamp":1764492935124,"user_tz":480,"elapsed":36316,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"64c0e7be-5f6f-4296-e752-4c3b5ea1eff6"},"id":"kfBoQTrjtynY","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Try MyDrive: /content/drive/MyDrive/w266 - Project/XDetox -> True\n","Using XDETOX_DIR: /content/drive/MyDrive/w266 - Project/XDetox\n"]}]},{"cell_type":"code","source":["#@title Runtime setup (paths, cache, GPU)\n","# HuggingFace cache inside the repo (persists on Drive)\n","HF_CACHE = os.path.join(XDETOX_DIR, \"cache\")\n","os.makedirs(HF_CACHE, exist_ok=True)\n","os.environ[\"TRANSFORMERS_CACHE\"] = HF_CACHE\n","\n","# Add repo to PYTHONPATH\n","if XDETOX_DIR not in sys.path:\n","    sys.path.append(XDETOX_DIR)\n","\n","print(\"XDETOX_DIR:\", XDETOX_DIR)\n","print(\"TRANSFORMERS_CACHE:\", HF_CACHE)\n","print(\"CUDA available:\", torch.cuda.is_available())\n","if torch.cuda.is_available():\n","    print(\"GPU:\", torch.cuda.get_device_name(0))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ITPlTNBtzQx","executionInfo":{"status":"ok","timestamp":1764492935968,"user_tz":480,"elapsed":847,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"05953584-44c0-4700-cf16-5f9f46ae5489"},"id":"7ITPlTNBtzQx","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["XDETOX_DIR: /content/drive/MyDrive/w266 - Project/XDetox\n","TRANSFORMERS_CACHE: /content/drive/MyDrive/w266 - Project/XDetox/cache\n","CUDA available: True\n","GPU: NVIDIA A100-SXM4-80GB\n"]}]},{"cell_type":"code","source":["#@title Verify XDetox repo layout\n","for d in [\"rewrite\", \"evaluation\", \"datasets\"]:\n","    assert os.path.isdir(os.path.join(XDETOX_DIR, d)), f\"Missing folder: {d}\"\n","print(\"Repo folders OK.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MEy2TGYetzIb","executionInfo":{"status":"ok","timestamp":1764492936000,"user_tz":480,"elapsed":28,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"0e22d114-0fc8-4fd4-b76f-f107f932ea02"},"id":"MEy2TGYetzIb","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Repo folders OK.\n"]}]},{"cell_type":"code","source":["#@title Install dependencies (restart runtime if major errors)\n","!pip -q install --upgrade pip setuptools wheel\n","!pip -q install \"transformers==4.41.2\" \"tokenizers==0.19.1\" \\\n","                \"datasets==2.19.0\" \"evaluate==0.4.1\" \\\n","                \"sacrebleu==2.4.1\" sacremoses ftfy nltk matplotlib pandas jedi \\\n","                sentencepiece\n","# BERTScore dependency required by evaluation/bertscore.py\n","!pip -q install bert-score\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GeTzwxVDtzNn","executionInfo":{"status":"ok","timestamp":1764492957973,"user_tz":480,"elapsed":21968,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"a5ce0358-058b-4aa7-c5ac-5d5258a5641e"},"id":"GeTzwxVDtzNn","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.3.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["#@title Import from 'transformers'\n","from transformers import (\n","    AutoTokenizer, AutoModel, AutoModelForSequenceClassification,\n","    AutoModelForCausalLM,\n","    GPT2LMHeadModel, GPT2TokenizerFast,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tfnuR2YVCmW9","executionInfo":{"status":"ok","timestamp":1764492959875,"user_tz":480,"elapsed":1861,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"9e4ef029-6034-49ee-fa50-a51f16a57749"},"id":"tfnuR2YVCmW9","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["#@title Import from 'rewrite'\n","from rewrite.generation import Infiller\n","from rewrite import rewrite_example as rx\n","import argparse as _argparse"],"metadata":{"id":"ccJxAWrjA8Qc"},"id":"ccJxAWrjA8Qc","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title NLTK data\n","nltk.download(\"punkt\", quiet=True)\n","try:\n","    nltk.download(\"punkt_tab\", quiet=True)\n","except Exception:\n","    pass\n","print(\"NLTK ready\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y0Up7SKstzK9","executionInfo":{"status":"ok","timestamp":1764492970072,"user_tz":480,"elapsed":496,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"53449cc1-22e1-410e-824b-b6e9140437ac"},"id":"y0Up7SKstzK9","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["NLTK ready\n"]}]},{"cell_type":"code","source":["#@title Data configs\n","data_configs = {\n","    \"microagressions_val\": {\n","        \"data_path\": \"./datasets/microagressions/val.csv\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.25,\n","        \"temperature\": 2.5,\n","    },\n","    \"microagressions_test\": {\n","        \"data_path\": \"./datasets/microagressions/test.csv\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.25,\n","        \"temperature\": 2.5,\n","    },\n","    \"sbf_val\": {\n","        \"data_path\": \"./datasets/sbf/sbfdev.csv\",\n","        \"rep_penalty\": 1.5,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 5.0,\n","        \"temperature\": 2.9,\n","    },\n","    \"sbf_test\": {\n","        \"data_path\": \"./datasets/sbf/sbftst.csv\",\n","        \"rep_penalty\": 1.5,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 5.0,\n","        \"temperature\": 2.9,\n","    },\n","    \"dynabench_val\": {\n","        \"data_path\": \"./datasets/dynabench/db_dev.csv\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.75,\n","        \"temperature\": 2.5,\n","    },\n","    \"dynabench_test\": {\n","        \"data_path\": \"./datasets/dynabench/db_test.csv\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.75,\n","        \"temperature\": 2.5,\n","    },\n","    \"jigsaw_toxic\": {\n","        \"data_path\": \"./datasets/jigsaw_full_30/test_10k_toxic.txt\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.75,\n","        \"temperature\": 2.5,\n","    },\n","    \"paradetox\": {\n","        \"data_path\": \"./datasets/paradetox/test_toxic_parallel.txt\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.75,\n","        \"temperature\": 2.5,\n","    },\n","    \"appdia_original\": {\n","        \"data_path\": \"./datasets/appdia/original-annotated-data/original-test.tsv\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.75,\n","        \"temperature\": 2.5,\n","    },\n","    \"appdia_discourse\": {\n","        \"data_path\": \"./datasets/appdia/discourse-augmented-data/discourse-test.tsv\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.75,\n","        \"temperature\": 2.5,\n","    }\n","}\n","print(\"Datasets:\", \", \".join(data_configs.keys()))\n","\n","REPO = XDETOX_DIR\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7nBku39IuAgb","executionInfo":{"status":"ok","timestamp":1764492970107,"user_tz":480,"elapsed":23,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"2b6514f1-b085-4b20-c05d-e177aa93da9c"},"id":"7nBku39IuAgb","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Datasets: microagressions_val, microagressions_test, sbf_val, sbf_test, dynabench_val, dynabench_test, jigsaw_toxic, paradetox, appdia_original, appdia_discourse\n"]}]},{"cell_type":"code","source":["#@title Helpers: subset data\n","def _abs_repo_path(rel: str) -> str:\n","    return os.path.join(REPO, rel.lstrip(\"./\"))\n","\n","def _ensure_dir(p: str):\n","    Path(p).mkdir(parents=True, exist_ok=True)\n","\n","def _subset_for_data_type(data_type, data_path, n, out_dir):\n","    \"\"\"\n","    Create a small subset file matching the expected format used by rewrite_example.get_data().\n","    Returns the path to the *new* subset file (or original path if n is None).\n","    \"\"\"\n","    if n is None or n <= 0:\n","        return data_path\n","\n","    src = _abs_repo_path(data_path)\n","    _ensure_dir(out_dir)\n","\n","    if \"microagressions\" in data_path:\n","        df = pd.read_csv(src)\n","        sub = df.head(n)\n","        out = os.path.join(out_dir, os.path.basename(src))\n","        sub.to_csv(out, index=False)\n","        return out\n","\n","    if \"sbf\" in data_path:\n","        df = pd.read_csv(src)\n","        sub = df.head(n)\n","        out = os.path.join(out_dir, os.path.basename(src))\n","        sub.to_csv(out, index=False)\n","        return out\n","\n","    if \"dynabench\" in data_path:\n","        df = pd.read_csv(src)\n","        sub = df.head(n)\n","        out = os.path.join(out_dir, os.path.basename(src))\n","        sub.to_csv(out, index=False)\n","        return out\n","\n","    if any(k in data_path for k in [\"paradetox\", \"jigsaw\"]):\n","        if data_path.endswith(\".txt\"):\n","            with open(src, \"r\") as f:\n","                lines = [s.rstrip(\"\\n\") for s in f.readlines()]\n","            out = os.path.join(out_dir, os.path.basename(src))\n","            with open(out, \"w\") as g:\n","                for s in lines[:n]:\n","                    g.write(s + \"\\n\")\n","            return out\n","        elif data_path.endswith(\".csv\"):\n","            df = pd.read_csv(src).head(n)\n","            out = os.path.join(out_dir, os.path.basename(src))\n","            df.to_csv(out, index=False)\n","            return out\n","\n","    if \"appdia\" in data_path:\n","        df = pd.read_csv(src, sep=\"\\t\").head(n)\n","        out = os.path.join(out_dir, os.path.basename(src))\n","        df.to_csv(out, sep=\"\\t\", index=False)\n","        return out\n","\n","    out = os.path.join(out_dir, os.path.basename(src))\n","    shutil.copy(src, out)\n","    return out"],"metadata":{"id":"ToytrY0SuAjr"},"id":"ToytrY0SuAjr","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Global scoring helpers: toxicity, similarity, fluency\n","\n","DEVICE_SCORE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Scoring models (tox/sim/flu) will use: {DEVICE_SCORE}\")\n","\n","# ---------- Toxicity model (textdetox/xlmr-large-toxicity-classifier-v2) ----------\n","_TOX_MODEL_NAME = \"textdetox/xlmr-large-toxicity-classifier-v2\"\n","_TOX_TOKENIZER = None\n","_TOX_MODEL = None\n","\n","def _lazy_load_tox():\n","    global _TOX_TOKENIZER, _TOX_MODEL\n","    if _TOX_TOKENIZER is None or _TOX_MODEL is None:\n","        _TOX_TOKENIZER = AutoTokenizer.from_pretrained(_TOX_MODEL_NAME)\n","        _TOX_MODEL = AutoModelForSequenceClassification.from_pretrained(\n","            _TOX_MODEL_NAME\n","        ).to(DEVICE_SCORE)\n","        _TOX_MODEL.eval()\n","\n","@torch.no_grad()\n","def get_toxicity_scores(texts, batch_size=32):\n","    _lazy_load_tox()\n","    scores = []\n","    for i in tqdm(range(0, len(texts), batch_size), desc=\"Toxicity\", leave=False):\n","        batch = texts[i:i+batch_size]\n","        enc = _TOX_TOKENIZER(\n","            batch, return_tensors=\"pt\",\n","            truncation=True, max_length=512, padding=True\n","        ).to(DEVICE_SCORE)\n","        logits = _TOX_MODEL(**enc).logits\n","        probs = torch.softmax(logits, dim=-1)\n","        scores.extend(probs[:, 1].detach().cpu().tolist())\n","    return scores\n","\n","# ---------- Semantic similarity (LaBSE) ----------\n","_LABSE_NAME = \"sentence-transformers/LaBSE\"\n","_LABSE_TOKENIZER = None\n","_LABSE_MODEL = None\n","\n","def _lazy_load_labse():\n","    global _LABSE_TOKENIZER, _LABSE_MODEL\n","    if _LABSE_TOKENIZER is None or _LABSE_MODEL is None:\n","        _LABSE_TOKENIZER = AutoTokenizer.from_pretrained(_LABSE_NAME)\n","        _LABSE_MODEL = AutoModel.from_pretrained(_LABSE_NAME).to(DEVICE_SCORE)\n","        _LABSE_MODEL.eval()\n","\n","@torch.no_grad()\n","def get_labse_embeddings(texts, batch_size=32):\n","    _lazy_load_labse()\n","    embs = []\n","    for i in tqdm(range(0, len(texts), batch_size), desc=\"LaBSE embeddings\", leave=False):\n","        batch = texts[i:i+batch_size]\n","        enc = _LABSE_TOKENIZER(\n","            batch, return_tensors=\"pt\",\n","            truncation=True, max_length=256, padding=True\n","        ).to(DEVICE_SCORE)\n","        outputs = _LABSE_MODEL(**enc)\n","        hidden = outputs.last_hidden_state\n","        mask = enc[\"attention_mask\"].unsqueeze(-1)\n","        masked = hidden * mask\n","        summed = masked.sum(dim=1)\n","        counts = mask.sum(dim=1).clamp(min=1e-6)\n","        sent_emb = (summed / counts).cpu().numpy()\n","        embs.append(sent_emb)\n","    if not embs:\n","        return np.zeros((0, 768), dtype=np.float32)\n","    return np.vstack(embs)\n","\n","# ---------- Fluency via GPT-2 perplexity ----------\n","_GPT2_NAME = \"gpt2\"\n","_GPT2_TOKENIZER = None\n","_GPT2_MODEL = None\n","\n","def _lazy_load_gpt2():\n","    global _GPT2_TOKENIZER, _GPT2_MODEL\n","    if _GPT2_TOKENIZER is None or _GPT2_MODEL is None:\n","        _GPT2_TOKENIZER = GPT2TokenizerFast.from_pretrained(_GPT2_NAME)\n","        _GPT2_MODEL = GPT2LMHeadModel.from_pretrained(_GPT2_NAME).to(DEVICE_SCORE)\n","        _GPT2_MODEL.eval()\n","\n","@torch.no_grad()\n","def get_gpt2_perplexities(texts):\n","    import math as _math\n","    _lazy_load_gpt2()\n","    ppls = []\n","    for s in tqdm(texts, desc=\"GPT-2 PPL\", leave=False):\n","        enc = _GPT2_TOKENIZER(s, return_tensors=\"pt\").to(DEVICE_SCORE)\n","        out = _GPT2_MODEL(enc[\"input_ids\"], labels=enc[\"input_ids\"])\n","        ppl = _math.exp(out.loss.item())\n","        if ppl > 1e4:\n","            ppl = 1e4\n","        ppls.append(float(ppl))\n","    return ppls\n","\n","def perplexity_to_fluency(ppls, p_min=5.0, p_max=300.0):\n","    import math as _math\n","    ppls = np.asarray(ppls, dtype=float)\n","    p = np.clip(ppls, p_min, p_max)\n","    log_p = np.log(p)\n","    log_min = _math.log(p_min)\n","    log_max = _math.log(p_max)\n","    F = (log_max - log_p) / (log_max - log_min + 1e-8)\n","    F = np.clip(F, 0.0, 1.0)\n","    return F"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-MxQChqOuAnT","executionInfo":{"status":"ok","timestamp":1764492970141,"user_tz":480,"elapsed":12,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"0013c80f-7383-47de-b334-0bb7995c28f1"},"id":"-MxQChqOuAnT","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Scoring models (tox/sim/flu) will use: cuda\n"]}]},{"cell_type":"code","source":["#@title Evaluation helpers (evaluate_all.py with MeaningBERT + toxicity)\n","def _parse_run_folder_name(folder_name):\n","    pattern = r\"aa(\\d+\\.\\d+)_ae(\\d+\\.\\d+)_ab(\\d+\\.\\d+)_base(.*?)_anti(.*?)_expert(.*?)_temp(\\d+\\.\\d+)_sample(.*?)_topk(\\d+)_reppenalty(\\d+\\.\\d+)_filterp(\\d+\\.\\d+)_maxlength(\\d+)_topp(\\d+\\.\\d+)\"\n","    m = re.match(pattern, folder_name)\n","    return bool(m)\n","\n","def _eval_with_toxicity(base_path, overwrite_eval=False, skip_ref=False, tox_threshold=0.5, tox_batch_size=32):\n","    import sys as _sys, os as _os\n","    for folder in os.listdir(base_path):\n","        gen_dir = os.path.join(base_path, folder)\n","        if not os.path.isdir(gen_dir) or not _parse_run_folder_name(folder):\n","            continue\n","        orig_path = os.path.join(gen_dir, \"orig.txt\")\n","        gen_path  = os.path.join(gen_dir, \"gen.txt\")\n","        out_stats = os.path.join(gen_dir, \"gen_stats.txt\")\n","        if not (os.path.exists(orig_path) and os.path.exists(gen_path)):\n","            continue\n","        if os.path.exists(out_stats) and not overwrite_eval:\n","            continue\n","\n","        env = os.environ.copy()\n","        env[\"PYTHONPATH\"] = REPO + (\":\" + env.get(\"PYTHONPATH\",\"\") if env.get(\"PYTHONPATH\") else \"\")\n","        cmd = [\n","            _sys.executable, \"-m\", \"evaluation.evaluate_all\",\n","            \"--orig_path\", orig_path,\n","            \"--gen_path\",  gen_path,\n","            \"--tox_threshold\", str(tox_threshold),\n","            \"--tox_batch_size\", str(tox_batch_size),\n","        ]\n","        if skip_ref:\n","            cmd.append(\"--skip_ref\")\n","        print(\"Eval:\", \" \".join(cmd))\n","        res = run(cmd, cwd=REPO, env=env, stdout=PIPE, stderr=PIPE, text=True)\n","        if res.returncode != 0:\n","            print(res.stdout)\n","            print(res.stderr)\n","            res.check_returncode()\n","\n","def _safe_float(x):\n","    try:\n","        return float(x)\n","    except Exception:\n","        return float('nan')\n","\n","def _read_stats_file(path):\n","    out = {}\n","    with open(path, \"r\") as f:\n","        for line in f:\n","            if \":\" not in line:\n","                continue\n","            k, v = line.strip().split(\": \", 1)\n","            k = k.replace(\"(skipped)\", \"\").strip().lower()\n","            out[k] = _safe_float(v)\n","    return out\n","\n","def _aggregate_eval_csv(output_folder, data_type, base_out_dir):\n","    \"\"\"\n","    Aggregate evaluation metrics for the LLM-masking pipeline.\n","\n","    Directory layout (absolute base_out_dir):\n","      base_out_dir/\n","        └── {data_type}/\n","            └── LLM_Masking/\n","                └── {run_folder}/\n","                    └── gen_stats.txt\n","\n","    We keep a `threshold` column for compatibility, but it is fixed to 0.20.\n","    \"\"\"\n","    rows = []\n","\n","    mask_dir = \"LLM_Masking\"\n","    base_path = os.path.join(base_out_dir, data_type, mask_dir)\n","    if not os.path.isdir(base_path):\n","        print(\"No evaluation directory found:\", base_path)\n","        return\n","\n","    for folder in os.listdir(base_path):\n","        gen_dir = os.path.join(base_path, folder)\n","        stats_path = os.path.join(gen_dir, \"gen_stats.txt\")\n","        if not os.path.exists(stats_path):\n","            continue\n","        s = _read_stats_file(stats_path)\n","        rows.append({\n","            \"threshold\":        0.20,  # fixed label for this LLM pipeline\n","            \"folder\":           folder,\n","            \"bertscore\":        s.get(\"bertscore\", np.nan),\n","            \"meaningbert\":      s.get(\"meaningbert\", np.nan),\n","            \"bleu4\":            s.get(\"bleu4\", np.nan),\n","            \"perplexity_gen\":   s.get(\"perplexity gen\", np.nan),\n","            \"perplexity_orig\":  s.get(\"perplexity orig\", np.nan),\n","            \"toxicity_gen\":     s.get(\"toxicity gen\", np.nan),\n","            \"toxicity_orig\":    s.get(\"toxicity orig\", np.nan),\n","        })\n","\n","    if rows:\n","        cols = [\n","            \"threshold\", \"folder\",\n","            \"bertscore\", \"meaningbert\", \"bleu4\",\n","            \"perplexity_gen\", \"perplexity_orig\",\n","            \"toxicity_gen\", \"toxicity_orig\",\n","        ]\n","        df = pd.DataFrame(rows)\n","        df = df[cols]\n","        out_csv = os.path.join(base_out_dir, data_type, f\"{data_type}.csv\")\n","        df.to_csv(out_csv, index=False)\n","        print(\"Wrote summary CSV:\", out_csv)\n","    else:\n","        print(\"No evaluation files found to summarize.\")\n"],"metadata":{"id":"u-7I09Uvqb8c"},"id":"u-7I09Uvqb8c","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title  LLM Masking (MetaDetox-style) using Mistral-7B-Instruct\n","# By default, run Mistral on CPU to avoid GPU OOM when combined with BART/LaBSE/XLM-R/GPT-2.\n","# If you have a large GPU (e.g., A100 40GB) and want speed, set USE_LLM_GPU = True.\n","USE_LLM_GPU = True\n","\n","DEVICE_LLM = torch.device(\"cuda\" if USE_LLM_GPU and torch.cuda.is_available() else \"cpu\")\n","print(\"LLM masker device:\", DEVICE_LLM)\n","\n","LLM_MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n","_LLM_TOKENIZER = None\n","_LLM_MODEL = None\n","\n","MASK_SYSTEM_PROMPT = \"\"\"You are a helpful assistant trained to make toxic or offensive sentences safer\n","by MASKING toxic words with the special token <mask>, while keeping the original sentence\n","structure and wording as much as possible.\n","\n","You must behave like a MASKER, not a full rewriter.\n","\n","Your task:\n","1. Identify toxic, offensive, or profane words or short phrases.\n","2. For each toxic span, replace the entire span with a single <mask> token.\n","3. There may be multiple toxic spans in one sentence, so multiple <mask> tokens are allowed.\n","4. If several neighboring words are toxic, you must still use only a single <mask> token\n","   in that place. In other words, if you would place \"<mask> <mask>\" or a longer sequence\n","   of <mask> tokens, collapse them into a single <mask> so that there are never multiple\n","   <mask> tokens in a row.\n","5. Do NOT rewrite, paraphrase, or summarize the sentence.\n","6. Do NOT add, remove, or reorder non-toxic words or punctuation.\n","7. Keep punctuation and spacing as close to the original as possible.\n","8. If there is no toxic content, return the sentence unchanged.\n","\n","Output rules (format is very strict):\n","- ONLY return the final masked sentence inside ONE pair of square brackets, like:\n","  [This is a <mask> example.]\n","- Do NOT print anything before or after the brackets.\n","- Do NOT add explanations, comments, or extra lines.\n","- Do NOT include any language tags or metadata.\n","- Do NOT include additional '[' or ']' characters inside the sentence.\n","\"\"\"\n","\n","MASK_FEW_SHOT = \"\"\"Toxic Sentence: You're such a stupid idiot, nobody wants to hear your crap.\n","Step 1 - Identify toxic words: \"stupid idiot\", \"crap\"\n","Step 2 - Mask toxic words (do NOT rewrite the rest):\n","You're such a <mask>, nobody wants to hear your <mask>.\n","Final Output: [You're such a <mask>, nobody wants to hear your <mask>.]\"\"\"\n","\n","\n","def _lazy_load_llm_masker():\n","    global _LLM_MODEL, _LLM_TOKENIZER\n","    if _LLM_MODEL is not None and _LLM_TOKENIZER is not None:\n","        return\n","    print(f\"Loading LLM masker: {LLM_MODEL_NAME} on {DEVICE_LLM} ...\")\n","    _LLM_TOKENIZER = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n","    _LLM_MODEL = AutoModelForCausalLM.from_pretrained(\n","        LLM_MODEL_NAME,\n","        torch_dtype=torch.float16 if DEVICE_LLM.type == \"cuda\" else torch.float32,\n","        device_map=None,\n","    ).to(DEVICE_LLM)\n","    _LLM_MODEL.eval()\n","    print(\"LLM masker loaded.\")\n","\n","def _extract_bracket_content(text: str) -> str:\n","    \"\"\"\n","    Extract content inside the first [ ... ] block.\n","    If there is an opening '[' but no closing ']', take everything after '['.\n","    Otherwise, fall back to the whole string.\n","    \"\"\"\n","    text = text.strip()\n","\n","    # Case 1: well-formed [ ... ]\n","    m = re.search(r\"\\[([^\\]]*)\\]\", text, flags=re.DOTALL)\n","    if m:\n","        return m.group(1).strip()\n","\n","    # Case 2: has '[' but no ']' (truncate everything before '[')\n","    if \"[\" in text:\n","        return text.split(\"[\", 1)[1].strip()\n","\n","    # Fallback: no brackets at all\n","    return text\n","\n","def _postprocess_llm_mask(masked_text: str) -> str:\n","    \"\"\"\n","    Clean up LLM-masked sentences to be MaRCo-friendly:\n","      - remove stray leading/trailing brackets,\n","      - normalize whitespace,\n","      - normalize <mask> token casing,\n","      - collapse runs of <mask> into a single <mask>.\n","    \"\"\"\n","    s = masked_text.strip()\n","\n","    # Remove any leftover outer brackets if still present\n","    if s.startswith(\"[\") and s.endswith(\"]\") and len(s) > 2:\n","        s = s[1:-1].strip()\n","    else:\n","        if s.startswith(\"[\"):\n","            s = s[1:].strip()\n","        if s.endswith(\"]\"):\n","            s = s[:-1].strip()\n","\n","    # Normalize whitespace\n","    s = re.sub(r\"\\s+\", \" \", s).strip()\n","\n","    # Normalize mask token casing and spacing: <Mask>, <MASK>, < mask > -> <mask>\n","    s = re.sub(r\"<\\s*mask\\s*>\", \"<mask>\", s, flags=re.IGNORECASE)\n","\n","    # Collapse runs of <mask> (e.g., \"<mask> <mask> <mask>\" -> \"<mask>\")\n","    s = re.sub(r\"(?:\\s*<mask>\\s*){2,}\", \" <mask> \", s)\n","    s = re.sub(r\"\\s+\", \" \", s).strip()\n","\n","    # Simple safety: if we somehow deleted everything, fall back to the original masked_text\n","    if not s:\n","        return masked_text.strip()\n","\n","    return s\n","\n","@torch.no_grad()\n","def llm_mask_sentences(sentences: List[str]) -> List[str]:\n","    \"\"\"\n","    Use Mistral-7B-Instruct as a masker:\n","    input: toxic sentence\n","    output: same sentence but toxic words replaced by <mask>.\n","    \"\"\"\n","    _lazy_load_llm_masker()\n","    masked = []\n","    for s in tqdm(sentences, desc=\"LLM masking (Mistral)\", leave=False):\n","        messages = [\n","            {\n","                \"role\": \"system\",\n","                \"content\": MASK_SYSTEM_PROMPT + \"\\n\\nBelow is an example:\\n\" + MASK_FEW_SHOT,\n","            },\n","            {\n","                \"role\": \"user\",\n","                \"content\": f\"Toxic Sentence: {s}\\nFinal Output:\",\n","            },\n","        ]\n","        try:\n","            prompt = _LLM_TOKENIZER.apply_chat_template(\n","                messages,\n","                tokenize=False,\n","                add_generation_prompt=True,\n","            )\n","        except Exception:\n","            # Fallback: plain prompt if chat template not available\n","            prompt = (\n","                MASK_SYSTEM_PROMPT\n","                + \"\\n\\nExample:\\n\"\n","                + MASK_FEW_SHOT\n","                + \"\\n\\nToxic Sentence: \"\n","                + s\n","                + \"\\nFinal Output:\"\n","            )\n","\n","        inputs = _LLM_TOKENIZER(prompt, return_tensors=\"pt\").to(DEVICE_LLM)\n","        gen = _LLM_MODEL.generate(\n","            **inputs,\n","            max_new_tokens=64,\n","            do_sample=False,\n","            temperature=0.0,\n","            pad_token_id=_LLM_TOKENIZER.eos_token_id,\n","        )\n","        # Only decode newly generated tokens\n","        gen_text = _LLM_TOKENIZER.decode(\n","            gen[0][inputs[\"input_ids\"].shape[1]:],\n","            skip_special_tokens=True,\n","        )\n","\n","        # 1) extract bracket content (robust to missing ']')\n","        masked_text = _extract_bracket_content(gen_text)\n","        # 2) normalize for MaRCo (collapse mask runs, strip stray brackets, etc.)\n","        masked_text = _postprocess_llm_mask(masked_text)\n","\n","        # Ensure we at least return something\n","        if not masked_text:\n","            masked_text = s\n","        masked.append(masked_text)\n","\n","    return masked"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nG355GLlqcp-","executionInfo":{"status":"ok","timestamp":1764492970252,"user_tz":480,"elapsed":82,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"bea3ccde-3e57-4eb6-8085-308f48e5a7ea"},"id":"nG355GLlqcp-","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["LLM masker device: cuda\n"]}]},{"cell_type":"code","source":["#@title Global reranking: combine toxicity, similarity, fluency\n","def rerank_candidates_global(\n","    sources,\n","    candidates,\n","    weights=(0.5, 0.3, 0.2),\n","    ppl_min=5.0,\n","    ppl_max=300.0,\n","):\n","    \"\"\"\n","    sources: list[str], length N\n","    candidates: list[list[str]], shape N x C\n","    weights: (w_T, w_S, w_F)\n","    Returns:\n","        best_idx: np.ndarray of shape (N,), index of chosen candidate per source\n","        details: dict with matrices [N x C] for tox, safety, sim, flu, score\n","    \"\"\"\n","    w_T, w_S, w_F = weights\n","    N = len(sources)\n","    assert len(candidates) == N, \"candidates length mismatch\"\n","\n","    if N == 0:\n","        return np.array([], dtype=int), {}\n","\n","    C_list = [len(c) for c in candidates]\n","    assert len(set(C_list)) == 1, \"All inputs must have same num_candidates\"\n","    C = C_list[0]\n","    if C == 0:\n","        raise ValueError(\"num_candidates must be >= 1\")\n","\n","    # Flatten candidates and map to source indices\n","    flat_cands = []\n","    flat_src_idx = []\n","    for i, cand_list in enumerate(candidates):\n","        for cand in cand_list:\n","            flat_cands.append(cand)\n","            flat_src_idx.append(i)\n","    flat_src_idx = np.array(flat_src_idx, dtype=int)\n","\n","    # Toxicity\n","    tox = np.array(get_toxicity_scores(flat_cands), dtype=float)  # [N*C]\n","\n","    # Semantic similarity (LaBSE)\n","    src_embs = get_labse_embeddings(sources)  # [N, D]\n","    cand_embs = get_labse_embeddings(flat_cands)  # [N*C, D]\n","    # Normalize\n","    src_embs = src_embs / np.clip(np.linalg.norm(src_embs, axis=1, keepdims=True), 1e-8, None)\n","    cand_embs = cand_embs / np.clip(np.linalg.norm(cand_embs, axis=1, keepdims=True), 1e-8, None)\n","    # Cosine between each candidate and its source\n","    sims = np.sum(cand_embs * src_embs[flat_src_idx], axis=1)  # [-1,1]\n","    sims = (sims + 1.0) / 2.0  # -> [0,1]\n","\n","    # Fluency: GPT-2 PPL -> F in [0,1]\n","    ppls = np.array(get_gpt2_perplexities(flat_cands), dtype=float)\n","    flus = perplexity_to_fluency(ppls, p_min=ppl_min, p_max=ppl_max)\n","\n","    # Safety\n","    safety = 1.0 - tox\n","\n","    # Global score\n","    scores = w_T * safety + w_S * sims + w_F * flus\n","\n","    # Reshape to [N, C]\n","    tox2     = tox.reshape(N, C)\n","    safety2  = safety.reshape(N, C)\n","    sims2    = sims.reshape(N, C)\n","    flus2    = flus.reshape(N, C)\n","    scores2  = scores.reshape(N, C)\n","\n","    best_idx = scores2.argmax(axis=1)\n","    details = {\n","        \"tox\": tox2,\n","        \"safety\": safety2,\n","        \"sim\": sims2,\n","        \"flu\": flus2,\n","        \"score\": scores2,\n","    }\n","    return best_idx, details\n"],"metadata":{"id":"dom7rBbguA2u"},"id":"dom7rBbguA2u","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Global reranking: combine toxicity, similarity, fluency\n","def rerank_candidates_global(\n","    sources,\n","    candidates,\n","    weights=(0.5, 0.3, 0.2),\n","    ppl_min=5.0,\n","    ppl_max=300.0,\n","):\n","    \"\"\"\n","    sources: list[str], length N\n","    candidates: list[list[str]], shape N x C\n","    weights: (w_T, w_S, w_F)\n","    \"\"\"\n","    w_T, w_S, w_F = weights\n","    N = len(sources)\n","    assert len(candidates) == N, \"candidates length mismatch\"\n","\n","    if N == 0:\n","        return np.array([], dtype=int), {}\n","\n","    C_list = [len(c) for c in candidates]\n","    assert len(set(C_list)) == 1, \"All inputs must have same num_candidates\"\n","    C = C_list[0]\n","    if C == 0:\n","        raise ValueError(\"num_candidates must be >= 1\")\n","\n","    flat_cands = []\n","    flat_src_idx = []\n","    for i, cand_list in enumerate(candidates):\n","        for cand in cand_list:\n","            flat_cands.append(cand)\n","            flat_src_idx.append(i)\n","    flat_src_idx = np.array(flat_src_idx, dtype=int)\n","\n","    tox = np.array(get_toxicity_scores(flat_cands), dtype=float)\n","    src_embs = get_labse_embeddings(sources)\n","    cand_embs = get_labse_embeddings(flat_cands)\n","\n","    src_embs = src_embs / np.clip(np.linalg.norm(src_embs, axis=1, keepdims=True), 1e-8, None)\n","    cand_embs = cand_embs / np.clip(np.linalg.norm(cand_embs, axis=1, keepdims=True), 1e-8, None)\n","\n","    sims = np.sum(cand_embs * src_embs[flat_src_idx], axis=1)\n","    sims = (sims + 1.0) / 2.0\n","\n","    ppls = np.array(get_gpt2_perplexities(flat_cands), dtype=float)\n","    flus = perplexity_to_fluency(ppls, p_min=ppl_min, p_max=ppl_max)\n","\n","    safety = 1.0 - tox\n","    scores = w_T * safety + w_S * sims + w_F * flus\n","\n","    tox2    = tox.reshape(N, C)\n","    safety2 = safety.reshape(N, C)\n","    sims2   = sims.reshape(N, C)\n","    flus2   = flus.reshape(N, C)\n","    scores2 = scores.reshape(N, C)\n","\n","    best_idx = scores2.argmax(axis=1)\n","    details = {\n","        \"tox\": tox2,\n","        \"safety\": safety2,\n","        \"sim\": sims2,\n","        \"flu\": flus2,\n","        \"score\": scores2,\n","    }\n","    return best_idx, details\n"],"metadata":{"id":"6FFAXYgJpgSs"},"id":"6FFAXYgJpgSs","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Masking + generation with global reranking (using LLM masks)\n","\n","def _bool2str(x: bool) -> str:\n","    return \"T\" if x else \"F\"\n","\n","def _build_gen_folder_name(\n","    alpha_a, alpha_e, alpha_b,\n","    base_type, antiexpert_type, expert_type,\n","    temperature, sample, top_k_gen, rep_penalty, filter_p, max_length, top_p\n","):\n","    return (\n","        \"aa\" + str(alpha_a) +\n","        \"_ae\" + str(alpha_e) +\n","        \"_ab\" + str(alpha_b) +\n","        \"_base\" + base_type[:5] +\n","        \"_anti\" + antiexpert_type[:5] +\n","        \"_expert\" + expert_type[:5] +\n","        \"_temp\" + str(temperature) +\n","        \"_sample\" + _bool2str(sample) +\n","        \"_topk\" + str(top_k_gen) +\n","        \"_reppenalty\" + str(rep_penalty) +\n","        \"_filterp\" + str(filter_p) +\n","        \"_maxlength\" + str(max_length) +\n","        \"_topp\" + str(top_p)\n","    )\n","\n","def _run_llm_masking_and_global_reranking_for_threshold(\n","    data_type,\n","    subset_path,\n","    thresh,               # kept for API symmetry, not used in folder naming\n","    base_out_rel,\n","    batch_size,\n","    alpha_a, alpha_e, alpha_b,\n","    temperature,\n","    rep_penalty,\n","    max_length,\n","    top_k_gen,\n","    top_p,\n","    filter_p,\n","    sample,\n","    num_candidates,\n","    weights,\n","    overwrite_gen=False,\n","    inputs=None,\n","):\n","    \"\"\"\n","    Run one LLM-masking + MaRCo + global-reranking pass.\n","\n","    `thresh` is only a dummy numeric label kept for compatibility; it does NOT\n","    affect behavior. All outputs go under a single folder: LLM_Masking.\n","    \"\"\"\n","    # Load inputs if not provided\n","    if inputs is None:\n","        args_data = _argparse.Namespace(data_type=data_type, data_path=subset_path)\n","        inputs = rx.get_data(args_data)\n","    print(f\"#inputs to detoxify: {len(inputs)}\")\n","\n","    # Paths: use LLM_Masking folder (no DecompX thresholds)\n","    mask_dir = \"LLM_Masking\"\n","    cur_rel = os.path.join(base_out_rel, data_type, mask_dir)\n","    cur_abs = os.path.join(REPO, cur_rel)\n","    _ensure_dir(cur_abs)\n","\n","    masked_file = os.path.join(cur_abs, \"masked_inputs.txt\")\n","\n","    # LLM masking (reuse cached file if available)\n","    if not os.path.exists(masked_file):\n","        print(\"Running LLM masking (Mistral) to create masked_inputs.txt ...\")\n","        decoded_mask_inputs = llm_mask_sentences(inputs)\n","        decoded_mask_inputs = [\n","            re.sub(r\"\\s+\", \" \", d).strip() for d in decoded_mask_inputs\n","        ]\n","        with open(masked_file, \"w\") as f:\n","            for d in decoded_mask_inputs:\n","                f.write(d + \"\\n\")\n","\n","        # Optional: free LLM to release memory before loading BART\n","        global _LLM_MODEL, _LLM_TOKENIZER\n","        del _LLM_MODEL\n","        del _LLM_TOKENIZER\n","        _LLM_MODEL = None\n","        _LLM_TOKENIZER = None\n","        if torch.cuda.is_available() and DEVICE_LLM.type == \"cuda\":\n","            torch.cuda.empty_cache()\n","    else:\n","        with open(masked_file, \"r\") as f:\n","            decoded_mask_inputs = [s.strip() for s in f.readlines()]\n","        print(\"Reusing existing masked_inputs.txt\")\n","\n","    assert len(decoded_mask_inputs) == len(inputs), \"Masked vs inputs mismatch\"\n","\n","    # Initialize Infiller (MaRCo: base + toxic antiexpert + non-toxic expert)\n","    rewriter = Infiller(\n","        seed=0,\n","        base_path=\"facebook/bart-base\",\n","        antiexpert_path=\"hallisky/bart-base-toxic-antiexpert\",\n","        expert_path=\"hallisky/bart-base-nontoxic-expert\",\n","        base_type=\"base\",\n","        antiexpert_type=\"antiexpert\",\n","        expert_type=\"expert\",\n","        tokenizer=\"facebook/bart-base\",\n","    )\n","\n","    base_type = \"base\"\n","    antiexpert_type = \"antiexpert\"\n","    expert_type = \"expert\"\n","    gen_folder = _build_gen_folder_name(\n","        alpha_a, alpha_e, alpha_b,\n","        base_type, antiexpert_type, expert_type,\n","        temperature, sample, top_k_gen, rep_penalty, filter_p, max_length, top_p\n","    )\n","    final_abs = os.path.join(cur_abs, gen_folder)\n","    gen_txt = os.path.join(final_abs, \"gen.txt\")\n","    orig_txt = os.path.join(final_abs, \"orig.txt\")\n","\n","    # If generation already exists and we are not overwriting, just load outputs\n","    if os.path.exists(gen_txt) and not overwrite_gen:\n","        print(\"Generation already exists at:\", gen_txt, \"— skipping generation.\")\n","        _ensure_dir(final_abs)\n","        if not os.path.exists(orig_txt):\n","            with open(orig_txt, \"w\") as f:\n","                for l in inputs:\n","                    f.write(re.sub(r\"\\s+\", \" \", l).strip() + \"\\n\")\n","\n","        # Load masked inputs (already ensured above)\n","        with open(masked_file, \"r\") as f:\n","            decoded_mask_inputs = [s.strip() for s in f.readlines()]\n","\n","        # Load final generations\n","        with open(gen_txt, \"r\") as f:\n","            best_generations = [s.strip() for s in f.readlines()]\n","\n","        return inputs, decoded_mask_inputs, best_generations, final_abs\n","\n","    _ensure_dir(final_abs)\n","\n","    # Generate multiple candidates per input\n","    all_candidates: List[List[str]] = [[] for _ in range(len(inputs))]\n","\n","    print(f\"Generating {num_candidates} candidates per input (sampling={sample})\")\n","    for c in range(num_candidates):\n","        outs, decoded = rewriter.generate(\n","            inputs,\n","            decoded_mask_inputs,\n","            alpha_a=alpha_a,\n","            alpha_e=alpha_e,\n","            alpha_b=alpha_b,\n","            temperature=temperature,\n","            verbose=False,\n","            max_length=max_length,\n","            repetition_penalty=rep_penalty,\n","            p=top_p,\n","            filter_p=filter_p,\n","            k=top_k_gen,\n","            batch_size=batch_size,\n","            sample=sample,\n","            ranking=False,          # no DecompX ranking\n","            ranking_eval_output=0,\n","        )\n","        for i, text in enumerate(decoded):\n","            all_candidates[i].append(re.sub(r\"\\s+\", \" \", text).strip())\n","\n","    # Free BART models before heavy scoring\n","    del rewriter\n","    if torch.cuda.is_available():\n","        torch.cuda.empty_cache()\n","\n","    # Global reranking\n","    print(\"Global reranking (toxicity + similarity + fluency)...\")\n","    best_idx, details = rerank_candidates_global(\n","        sources=inputs,\n","        candidates=all_candidates,\n","        weights=weights,\n","    )\n","    best_generations = [\n","        all_candidates[i][best_idx[i]] for i in range(len(inputs))\n","    ]\n","\n","    # Save orig + chosen gen\n","    with open(orig_txt, \"w\") as f:\n","        for l in inputs:\n","            f.write(re.sub(r\"\\s+\", \" \", l).strip() + \"\\n\")\n","    with open(gen_txt, \"w\") as f:\n","        for l in best_generations:\n","            f.write(re.sub(r\"\\s+\", \" \", l).strip() + \"\\n\")\n","\n","    print(\"Saved:\", orig_txt)\n","    print(\"Saved:\", gen_txt)\n","\n","    return inputs, decoded_mask_inputs, best_generations, final_abs\n"],"metadata":{"id":"U5oOUWRYuA6E"},"id":"U5oOUWRYuA6E","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title `detoxify()` — LLM masking + global reranking + optional eval\n","\n","def detoxify(\n","    data_type: str = \"paradetox\",\n","    output_folder: str = \"colab_run_llm_mask\",\n","    echo: bool = False,\n","    batch_size: int = 10,\n","    sample: bool = True,\n","    top_k_gen: int = 50,\n","    top_p: float = 0.95,\n","    filter_p: float = 1.0,\n","    max_length: int = 128,\n","    alpha_a: float = None,   # if None, take from data_configs\n","    alpha_e: float = None,   # if None, take from data_configs\n","    alpha_b: float = 1.0,\n","    temperature: float = None,  # if None, from data_configs\n","    rep_penalty: float = None,  # if None, from data_configs\n","    num_examples: int = 100,    # small-batch control; None = full dataset\n","    overwrite_gen: bool = False,\n","    run_eval: bool = False,\n","    overwrite_eval: bool = False,\n","    skip_ref_eval: bool = False,\n","    # global reranking:\n","    weights = (0.5, 0.3, 0.2),   # (w_T, w_S, w_F)\n","    num_candidates: int = 3,     # candidates per input\n","):\n","    \"\"\"\n","    Run XDetox with:\n","      - LLM masking (Mistral-7B-Instruct acting as a masked-span detector),\n","      - MaRCo generation (BART base / antiexpert / expert),\n","      - global reranking based on:\n","          - toxicity (XLM-R),\n","          - semantic similarity (LaBSE),\n","          - fluency (GPT-2 perplexity -> [0,1]),\n","      - evaluation via evaluation/evaluate_all.py (BLEU/BERTScore/MeaningBERT/PPL/Toxicity).\n","\n","    Notes:\n","    - There is no DecompX threshold here. The LLM masker decides which spans to\n","      replace with <mask>.\n","    - Outputs are stored under:\n","        data/model_outputs/{output_folder}/{data_type}/LLM_Masking/{run_folder}/\n","    - If echo=True, the function prints:\n","        * number of examples and dataset,\n","        * a few example inputs,\n","        * a few masked inputs,\n","        * a few detoxified outputs,\n","        * evaluation metrics for this specific run (if run_eval=True).\n","    \"\"\"\n","    assert data_type in data_configs, f\"Unknown data_type: {data_type}\"\n","    cfg = data_configs[data_type].copy()\n","\n","    if num_candidates < 1:\n","        raise ValueError(\"num_candidates must be >= 1\")\n","\n","    # fallbacks from data_configs\n","    if alpha_a is None:\n","        alpha_a = cfg[\"alpha_a\"]\n","    if alpha_e is None:\n","        alpha_e = cfg[\"alpha_e\"]\n","    if temperature is None:\n","        temperature = cfg[\"temperature\"]\n","    if rep_penalty is None:\n","        rep_penalty = cfg[\"rep_penalty\"]\n","\n","    # Use model_outputs instead of dexp_outputs\n","    base_out_rel = os.path.join(\"data\", \"model_outputs\", output_folder)\n","    base_out_abs = os.path.join(REPO, base_out_rel)\n","    _ensure_dir(base_out_abs)\n","\n","    # subset path (file)\n","    original_data_path = cfg[\"data_path\"]\n","    subset_dir = os.path.join(REPO, \"datasets\", \"_subsets\", data_type)\n","    _ensure_dir(subset_dir)\n","    subset_path = _subset_for_data_type(\n","        data_type, original_data_path, num_examples, subset_dir\n","    )\n","\n","    # Load inputs once here (so we can print and also reuse inside the runner)\n","    args_data = _argparse.Namespace(data_type=data_type, data_path=subset_path)\n","    inputs = rx.get_data(args_data)\n","    num_inputs = len(inputs)\n","\n","    if echo:\n","        print(\"=\" * 80)\n","        print(f\"[echo] Dataset: {data_type}\")\n","        print(f\"[echo] Subset path: {subset_path}\")\n","        print(f\"[echo] Output base: {base_out_abs}\")\n","        print(f\"[echo] Number of examples to detoxify: {num_inputs}\")\n","        print(f\"[echo] Weights (w_T, w_S, w_F): {weights}\")\n","        print(f\"[echo] num_candidates per input: {num_candidates}\")\n","        print(\"\\n[echo] Example inputs (first up to 3):\")\n","        for i, s in enumerate(inputs[:3]):\n","            print(f\"  input[{i}]: {s}\")\n","        print(\"=\" * 80)\n","\n","    # Dummy label kept only for compatibility with some utilities\n","    folder_label = 0.20\n","\n","    # Run one LLM-masking + MaRCo + global-reranking pass\n","    inputs, masked_inputs, best_generations, run_dir = _run_llm_masking_and_global_reranking_for_threshold(\n","        data_type=data_type,\n","        subset_path=subset_path,\n","        thresh=folder_label,        # label only, not used for naming\n","        base_out_rel=base_out_rel,\n","        batch_size=batch_size,\n","        alpha_a=alpha_a,\n","        alpha_e=alpha_e,\n","        alpha_b=alpha_b,\n","        temperature=temperature,\n","        rep_penalty=rep_penalty,\n","        max_length=max_length,\n","        top_k_gen=top_k_gen,\n","        top_p=top_p,\n","        filter_p=filter_p,\n","        sample=sample,\n","        num_candidates=num_candidates,\n","        weights=weights,\n","        overwrite_gen=overwrite_gen,\n","        inputs=inputs,\n","    )\n","\n","    if echo:\n","        print(\"\\n[echo] Example masked inputs (first up to 3):\")\n","        for i, m in enumerate(masked_inputs[:3]):\n","            print(f\"  masked[{i}]: {m}\")\n","\n","        print(\"\\n[echo] Example detoxified outputs (first up to 3):\")\n","        for i in range(min(3, len(best_generations))):\n","            print(f\"  detox[{i}]: {best_generations[i]}\")\n","\n","    # Optional evaluation (BLEU / BERTScore / MeaningBERT / PPL / Toxicity)\n","    if run_eval:\n","        mask_dir = \"LLM_Masking\"\n","        base_path = os.path.join(base_out_abs, data_type, mask_dir)\n","        _eval_with_toxicity(\n","            base_path,\n","            overwrite_eval=overwrite_eval,\n","            skip_ref=skip_ref_eval,\n","            tox_threshold=0.5,\n","            tox_batch_size=32,\n","        )\n","        _aggregate_eval_csv(\n","            output_folder,\n","            data_type,\n","            os.path.join(REPO, \"data\", \"model_outputs\", output_folder),\n","        )\n","\n","        # If echo, print metrics for THIS run from gen_stats.txt\n","        if echo:\n","            stats_path = os.path.join(run_dir, \"gen_stats.txt\")\n","            if os.path.exists(stats_path):\n","                stats = _read_stats_file(stats_path)\n","                print(\"\\n[echo] Evaluation metrics for this run:\")\n","                metric_keys = [\n","                    (\"bertscore\", \"BERTScore\"),\n","                    (\"meaningbert\", \"MeaningBERT\"),\n","                    (\"bleu4\", \"BLEU-4\"),\n","                    (\"perplexity gen\", \"Perplexity (gen)\"),\n","                    (\"perplexity orig\", \"Perplexity (orig)\"),\n","                    (\"toxicity gen\", \"Toxicity (gen)\"),\n","                    (\"toxicity orig\", \"Toxicity (orig)\"),\n","                ]\n","                for key, label in metric_keys:\n","                    val = stats.get(key, None)\n","                    if isinstance(val, float) and math.isnan(val):\n","                        continue\n","                    if val is None:\n","                        continue\n","                    print(f\"  {label}: {val:.4f}\")\n","            else:\n","                print(\"\\n[echo] gen_stats.txt not found for this run; no metrics to print.\")"],"metadata":{"id":"oqBLx5OSuA72"},"id":"oqBLx5OSuA72","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Example run — paradetox, LLM masking + global reranking\n","\n","# Example: small demo (adjust num_examples as you like)\n","# detoxify(\n","#     data_type=\"paradetox\",\n","#     output_folder=\"colab_run_llm_mask_demo\",\n","#     echo=True,\n","#     batch_size=8,\n","#     sample=True,\n","#     top_k_gen=50,\n","#     top_p=0.95,\n","#     max_length=96,\n","#     num_examples=50,             # small subset for testing\n","#     run_eval=True,               # BLEU/BERTScore/MeaningBERT/PPL/Toxicity\n","#     overwrite_gen=False,\n","#     overwrite_eval=True,\n","#     skip_ref_eval=False,\n","#     weights=(0.5, 0.3, 0.2),\n","#     num_candidates=10,\n","# )\n","\n","# Minimal example call (re-run eval only, assuming generations exist)\n","# detoxify(\n","#     data_type=\"paradetox\",\n","#     output_folder=\"colab_run_llm_mask_demo\",\n","#     echo=True,\n","#     batch_size=8,\n","#     sample=True,\n","#     top_k_gen=50,\n","#     top_p=0.95,\n","#     max_length=96,\n","#     num_examples=50,\n","#     run_eval=True,\n","#     overwrite_gen=False,\n","#     overwrite_eval=True,\n","#     skip_ref_eval=False,\n","#     weights=(0.5, 0.3, 0.2),\n","#     num_candidates=10,\n","# )"],"metadata":{"id":"u5LlySYquA9g","collapsed":true},"id":"u5LlySYquA9g","execution_count":null,"outputs":[]},{"cell_type":"code","source":["detoxify(\n","    data_type=\"paradetox\",\n","    output_folder=\"XDetox_w_LLM-Masking_Global-Reranking_Pipeline\",\n","    echo=True,\n","    batch_size=8,\n","    sample=True,\n","    top_k_gen=50,\n","    top_p=0.95,\n","    max_length=96,\n","    num_examples=1000,\n","    run_eval=True,             # BLEU/BERTScore/MeaningBERT/PPL/Toxicity\n","    overwrite_gen=True,\n","    overwrite_eval=True,\n","    skip_ref_eval=False,\n","    weights=(0.5, 0.3, 0.2),\n","    num_candidates=10,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":922,"referenced_widgets":["3b557a1651f34ece8902ea46d6e64dff","38dbaf51c8ae432e87712c85558ee7ee","6fbbd18b1e224e30b07fff9923cd9665","77cc919598f94441b4eb6575a576d074","04414ca4f3c446d9af7c7c4c7a23806d","2bd08e0957d849adb6389f37455cf884","164ff8b589ff4f93b72ead1ce28a87cb","3e645f483a2344f09153ff338264de9f","66d1dfeeec4141a29add821690a3c6b4","ed207152c87146429db0c9f552f2b0c7","73d7e4445aca43b28cbe260fc85dded5","39acf9142eff48acb3c0adc85409dadd","bda21c35a0604dc282bb992a4ae69d4f","37aa8247d42a40e79ecd3acaf5436af5","0623fca2c03a40deb7cfe139d95307a7","77a154dbce0f428184857bb7b1a5664b","5637a77ac62740b5a84af65f777f5e88","037ea79bec264c829a8353cd5aae7558","e38945e77935434a84c05847752f1e4d","4bdddbc9a775460ab7836a905c8edb5a","c8021c7650484cb387893b3c1d7f7c9c","3a7033a7cbfb436fab547b9a8a1e3cc5","c03848cd81c84c44ba3deb67c5d72f21","c7ca6aa7b6f1402c8a9a1660b933197c","2f47d1db2f424ac1b45e18a733337e0c","a3ff4721839e467aa9860575fcddb4eb","7e5c5619d01243f88a927c75e0983a09","db47a03a2d09419290606398f542eaec","17287c169e9b4437863b96c032f90446","608e2e9059284ad2a497394b87dfb069","abc14cb2fef547a58c11858d107375ed","f413726323d24cf2973b0744416cce93","163b984b74dd4d7595e8982ff54bad1a","dc7224f815cd41cf9047e5f554cf2c07","a2209ba2b76044fcb0566b3457b6a9e1","79f03e1fc7b5470eb5f5588a82578f23","4bb76d636d4e45ec9e63e8807bb26271","0f7f12f1020f4186a835b24a260c1a29","e1eda09817e2438ca9869a1c7d22cc59","786c34ed8f224e07adf8e80481c72b42","b0a74490673649f58a8dcaae2597f182","3e2f290466294c2fb84dd10bc39c93e5","9da9b96db59f4344b770cb6d3bfd5ac5","a33588ab36274d17961de9c15da6005a"]},"id":"O2S9fX-adatU","executionInfo":{"status":"ok","timestamp":1764494752543,"user_tz":480,"elapsed":1782234,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"1c1974f3-f975-4ab5-dd2c-1ea16bc3c036"},"id":"O2S9fX-adatU","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","[echo] Dataset: paradetox\n","[echo] Subset path: /content/drive/MyDrive/w266 - Project/XDetox/datasets/_subsets/paradetox/test_toxic_parallel.txt\n","[echo] Output base: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_Global-Reranking_Pipeline\n","[echo] Number of examples to detoxify: 671\n","[echo] Weights (w_T, w_S, w_F): (0.5, 0.3, 0.2)\n","[echo] num_candidates per input: 10\n","\n","[echo] Example inputs (first up to 3):\n","  input[0]: . or the loud ass one - thousand ton beast roaring towards you howling its horn .\n","  input[1]: mandated  and \" right fucking now \" would be good .\n","  input[2]: * neither * of my coworkers gave a shit when it came time to ditch mitch . ugh .\n","================================================================================\n","#inputs to detoxify: 671\n","Reusing existing masked_inputs.txt\n","Found 1 GPUS!\n","Generating 10 candidates per input (sampling=True)\n"]},{"output_type":"stream","name":"stderr","text":["Filling in masks: 100%|██████████| 84/84 [01:38<00:00,  1.17s/it]\n","Filling in masks: 100%|██████████| 84/84 [01:38<00:00,  1.18s/it]\n","Filling in masks: 100%|██████████| 84/84 [01:44<00:00,  1.25s/it]\n","Filling in masks: 100%|██████████| 84/84 [01:28<00:00,  1.05s/it]\n","Filling in masks: 100%|██████████| 84/84 [01:50<00:00,  1.31s/it]\n","Filling in masks: 100%|██████████| 84/84 [01:41<00:00,  1.21s/it]\n","Filling in masks: 100%|██████████| 84/84 [01:23<00:00,  1.01it/s]\n","Filling in masks: 100%|██████████| 84/84 [01:42<00:00,  1.23s/it]\n","Filling in masks: 100%|██████████| 84/84 [01:35<00:00,  1.14s/it]\n","Filling in masks: 100%|██████████| 84/84 [01:33<00:00,  1.11s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Global reranking (toxicity + similarity + fluency)...\n"]},{"output_type":"display_data","data":{"text/plain":["Toxicity:   0%|          | 0/210 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b557a1651f34ece8902ea46d6e64dff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["LaBSE embeddings:   0%|          | 0/21 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39acf9142eff48acb3c0adc85409dadd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["LaBSE embeddings:   0%|          | 0/210 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c03848cd81c84c44ba3deb67c5d72f21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["GPT-2 PPL:   0%|          | 0/6710 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc7224f815cd41cf9047e5f554cf2c07"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saved: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_Global-Reranking_Pipeline/paradetox/LLM_Masking/aa1.5_ae4.75_ab1.0_basebase_antiantie_expertexper_temp2.5_sampleT_topk50_reppenalty1.0_filterp1.0_maxlength96_topp0.95/orig.txt\n","Saved: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_Global-Reranking_Pipeline/paradetox/LLM_Masking/aa1.5_ae4.75_ab1.0_basebase_antiantie_expertexper_temp2.5_sampleT_topk50_reppenalty1.0_filterp1.0_maxlength96_topp0.95/gen.txt\n","\n","[echo] Example masked inputs (first up to 3):\n","  masked[0]: . or the loud <mask> one - thousand ton beast roaring towards you howling its horn .\n","  masked[1]: mandated and \"<mask> right <mask> now <mask> \" would be good .\n","  masked[2]: neither of my coworkers gave a <mask> when it came time to ditch mitch . ugh .\n","\n","[echo] Example detoxified outputs (first up to 3):\n","  detox[0]: . or the loud sound of a one- thousand ton beast roaring towards you howling its horn.\n","  detox[1]: mandated and \"right now\" would be good.\n","  detox[2]: neither of my coworkers gave a hoot about it when it came time to ditch mitch. ugh.\n","Eval: /usr/bin/python3 -m evaluation.evaluate_all --orig_path /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_Global-Reranking_Pipeline/paradetox/LLM_Masking/aa1.5_ae4.75_ab1.0_basebase_antiantie_expertexper_temp2.5_sampleT_topk50_reppenalty1.0_filterp1.0_maxlength96_topp0.95/orig.txt --gen_path /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_Global-Reranking_Pipeline/paradetox/LLM_Masking/aa1.5_ae4.75_ab1.0_basebase_antiantie_expertexper_temp2.5_sampleT_topk50_reppenalty1.0_filterp1.0_maxlength96_topp0.95/gen.txt --tox_threshold 0.5 --tox_batch_size 32\n","Wrote summary CSV: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_Global-Reranking_Pipeline/paradetox/paradetox.csv\n","\n","[echo] Evaluation metrics for this run:\n","  BERTScore: 0.9379\n","  MeaningBERT: 69.0240\n","  BLEU-4: 70.0470\n","  Perplexity (gen): 86.5870\n","  Perplexity (orig): 273.7500\n","  Toxicity (gen): 0.1593\n","  Toxicity (orig): 0.9253\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"VgZRbEBED8w4"},"id":"VgZRbEBED8w4","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"A100","machine_shape":"hm"},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"3b557a1651f34ece8902ea46d6e64dff":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_38dbaf51c8ae432e87712c85558ee7ee","IPY_MODEL_6fbbd18b1e224e30b07fff9923cd9665","IPY_MODEL_77cc919598f94441b4eb6575a576d074"],"layout":"IPY_MODEL_04414ca4f3c446d9af7c7c4c7a23806d"}},"38dbaf51c8ae432e87712c85558ee7ee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2bd08e0957d849adb6389f37455cf884","placeholder":"​","style":"IPY_MODEL_164ff8b589ff4f93b72ead1ce28a87cb","value":"Toxicity: 100%"}},"6fbbd18b1e224e30b07fff9923cd9665":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e645f483a2344f09153ff338264de9f","max":210,"min":0,"orientation":"horizontal","style":"IPY_MODEL_66d1dfeeec4141a29add821690a3c6b4","value":210}},"77cc919598f94441b4eb6575a576d074":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed207152c87146429db0c9f552f2b0c7","placeholder":"​","style":"IPY_MODEL_73d7e4445aca43b28cbe260fc85dded5","value":" 210/210 [00:08&lt;00:00, 27.88it/s]"}},"04414ca4f3c446d9af7c7c4c7a23806d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"2bd08e0957d849adb6389f37455cf884":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"164ff8b589ff4f93b72ead1ce28a87cb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3e645f483a2344f09153ff338264de9f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66d1dfeeec4141a29add821690a3c6b4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ed207152c87146429db0c9f552f2b0c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73d7e4445aca43b28cbe260fc85dded5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"39acf9142eff48acb3c0adc85409dadd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bda21c35a0604dc282bb992a4ae69d4f","IPY_MODEL_37aa8247d42a40e79ecd3acaf5436af5","IPY_MODEL_0623fca2c03a40deb7cfe139d95307a7"],"layout":"IPY_MODEL_77a154dbce0f428184857bb7b1a5664b"}},"bda21c35a0604dc282bb992a4ae69d4f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5637a77ac62740b5a84af65f777f5e88","placeholder":"​","style":"IPY_MODEL_037ea79bec264c829a8353cd5aae7558","value":"LaBSE embeddings:  95%"}},"37aa8247d42a40e79ecd3acaf5436af5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_e38945e77935434a84c05847752f1e4d","max":21,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4bdddbc9a775460ab7836a905c8edb5a","value":21}},"0623fca2c03a40deb7cfe139d95307a7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8021c7650484cb387893b3c1d7f7c9c","placeholder":"​","style":"IPY_MODEL_3a7033a7cbfb436fab547b9a8a1e3cc5","value":" 20/21 [00:00&lt;00:00, 65.24it/s]"}},"77a154dbce0f428184857bb7b1a5664b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"5637a77ac62740b5a84af65f777f5e88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"037ea79bec264c829a8353cd5aae7558":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e38945e77935434a84c05847752f1e4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4bdddbc9a775460ab7836a905c8edb5a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c8021c7650484cb387893b3c1d7f7c9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a7033a7cbfb436fab547b9a8a1e3cc5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c03848cd81c84c44ba3deb67c5d72f21":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c7ca6aa7b6f1402c8a9a1660b933197c","IPY_MODEL_2f47d1db2f424ac1b45e18a733337e0c","IPY_MODEL_a3ff4721839e467aa9860575fcddb4eb"],"layout":"IPY_MODEL_7e5c5619d01243f88a927c75e0983a09"}},"c7ca6aa7b6f1402c8a9a1660b933197c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db47a03a2d09419290606398f542eaec","placeholder":"​","style":"IPY_MODEL_17287c169e9b4437863b96c032f90446","value":"LaBSE embeddings:  96%"}},"2f47d1db2f424ac1b45e18a733337e0c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_608e2e9059284ad2a497394b87dfb069","max":210,"min":0,"orientation":"horizontal","style":"IPY_MODEL_abc14cb2fef547a58c11858d107375ed","value":210}},"a3ff4721839e467aa9860575fcddb4eb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f413726323d24cf2973b0744416cce93","placeholder":"​","style":"IPY_MODEL_163b984b74dd4d7595e8982ff54bad1a","value":" 202/210 [00:02&lt;00:00, 75.87it/s]"}},"7e5c5619d01243f88a927c75e0983a09":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"db47a03a2d09419290606398f542eaec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17287c169e9b4437863b96c032f90446":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"608e2e9059284ad2a497394b87dfb069":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abc14cb2fef547a58c11858d107375ed":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f413726323d24cf2973b0744416cce93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"163b984b74dd4d7595e8982ff54bad1a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc7224f815cd41cf9047e5f554cf2c07":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a2209ba2b76044fcb0566b3457b6a9e1","IPY_MODEL_79f03e1fc7b5470eb5f5588a82578f23","IPY_MODEL_4bb76d636d4e45ec9e63e8807bb26271"],"layout":"IPY_MODEL_0f7f12f1020f4186a835b24a260c1a29"}},"a2209ba2b76044fcb0566b3457b6a9e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e1eda09817e2438ca9869a1c7d22cc59","placeholder":"​","style":"IPY_MODEL_786c34ed8f224e07adf8e80481c72b42","value":"GPT-2 PPL: 100%"}},"79f03e1fc7b5470eb5f5588a82578f23":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_b0a74490673649f58a8dcaae2597f182","max":6710,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3e2f290466294c2fb84dd10bc39c93e5","value":6710}},"4bb76d636d4e45ec9e63e8807bb26271":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9da9b96db59f4344b770cb6d3bfd5ac5","placeholder":"​","style":"IPY_MODEL_a33588ab36274d17961de9c15da6005a","value":" 6709/6710 [01:15&lt;00:00, 90.92it/s]"}},"0f7f12f1020f4186a835b24a260c1a29":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"e1eda09817e2438ca9869a1c7d22cc59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"786c34ed8f224e07adf8e80481c72b42":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b0a74490673649f58a8dcaae2597f182":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e2f290466294c2fb84dd10bc39c93e5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9da9b96db59f4344b770cb6d3bfd5ac5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a33588ab36274d17961de9c15da6005a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}
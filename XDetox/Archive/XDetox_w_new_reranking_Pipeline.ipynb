{"cells":[{"cell_type":"markdown","id":"82d4549b","metadata":{"id":"82d4549b"},"source":["# XDetox Pipeline with Global Reranking\n","\n","This notebook mirrors `XDetox_Pipeline.ipynb` but replaces the **DecompX-based reranking** with a **global score**:\n","\n","$$\n","T'(c) = 1 - T(c) \\quad (\\text{safety})\n","$$\n","$$\n","\\text{Score}(c) = w_T \\cdot T'(c) + w_S \\cdot S(c) + w_F \\cdot F(c)\n","$$\n","\n","where:\n","- $T(c)$ = toxicity (from `xlmr-large-toxicity-classifier-v2`, in [0,1])\n","- $S(c)$ = semantic similarity (LaBSE cosine similarity, mapped to [0,1])\n","- $F(c)$ = fluency (GPT-2 perplexity mapped to [0,1], higher = more fluent)\n","\n","You control:\n","- **`weights=(w_T, w_S, w_F)`**\n","- **`num_candidates`** per input\n","\n","Masking (DecompX) and evaluation (BLEU/BERTScore/MeaningBERT/PPL/Toxicity) remain as before.\n"]},{"cell_type":"code","source":["#@title Mount Drive, Imports & locate XDetox\n","from google.colab import drive; drive.mount('/content/drive')\n","\n","import os, glob, re, sys, json, shutil, math\n","import numpy as np\n","import pandas as pd\n","from tqdm.auto import tqdm\n","from pathlib import Path\n","from subprocess import run, PIPE\n","import torch\n","import nltk\n","from typing import List\n","\n","# Try My Drive\n","candidate = \"/content/drive/MyDrive/w266 - Project/XDetox\"\n","print(\"Try MyDrive:\", candidate, \"->\", os.path.isdir(candidate))\n","\n","XDETOX_DIR = candidate\n","print(\"Using XDETOX_DIR:\", XDETOX_DIR)\n","assert os.path.isdir(XDETOX_DIR), f\"XDETOX_DIR does not exist: {XDETOX_DIR}\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kfBoQTrjtynY","executionInfo":{"status":"ok","timestamp":1763974093900,"user_tz":480,"elapsed":39486,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"bb87b453-cc0d-4920-be66-5790ec2f93e9"},"id":"kfBoQTrjtynY","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Try MyDrive: /content/drive/MyDrive/w266 - Project/XDetox -> True\n","Using XDETOX_DIR: /content/drive/MyDrive/w266 - Project/XDetox\n"]}]},{"cell_type":"code","source":["#@title Runtime setup (paths, cache, GPU)\n","# HuggingFace cache inside the repo (persists on Drive)\n","HF_CACHE = os.path.join(XDETOX_DIR, \"cache\")\n","os.makedirs(HF_CACHE, exist_ok=True)\n","os.environ[\"TRANSFORMERS_CACHE\"] = HF_CACHE\n","\n","# Add repo to PYTHONPATH\n","if XDETOX_DIR not in sys.path:\n","    sys.path.append(XDETOX_DIR)\n","\n","print(\"XDETOX_DIR:\", XDETOX_DIR)\n","print(\"TRANSFORMERS_CACHE:\", HF_CACHE)\n","print(\"CUDA available:\", torch.cuda.is_available())\n","if torch.cuda.is_available():\n","    print(\"GPU:\", torch.cuda.get_device_name(0))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ITPlTNBtzQx","executionInfo":{"status":"ok","timestamp":1763974103029,"user_tz":480,"elapsed":76,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"d43b8e5f-f47b-4324-fe23-900512ce8d0d"},"id":"7ITPlTNBtzQx","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["XDETOX_DIR: /content/drive/MyDrive/w266 - Project/XDetox\n","TRANSFORMERS_CACHE: /content/drive/MyDrive/w266 - Project/XDetox/cache\n","CUDA available: True\n","GPU: Tesla T4\n"]}]},{"cell_type":"code","source":["#@title Verify XDetox repo layout\n","for d in [\"rewrite\", \"evaluation\", \"datasets\"]:\n","    assert os.path.isdir(os.path.join(XDETOX_DIR, d)), f\"Missing folder: {d}\"\n","print(\"Repo folders OK.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MEy2TGYetzIb","executionInfo":{"status":"ok","timestamp":1763974151258,"user_tz":480,"elapsed":62,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"a0d979b8-238a-44e0-df25-9fa21827c4e4"},"id":"MEy2TGYetzIb","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Repo folders OK.\n"]}]},{"cell_type":"code","source":["#@title Install dependencies (restart runtime if major errors)\n","!pip -q install --upgrade pip setuptools wheel\n","!pip -q install \"transformers==4.41.2\" \"tokenizers==0.19.1\" \\\n","                \"datasets==2.19.0\" \"evaluate==0.4.1\" \\\n","                \"sacrebleu==2.4.1\" sacremoses ftfy nltk matplotlib pandas jedi\n","\n","# BERTScore dependency required by evaluation/bertscore.py\n","!pip -q install bert-score\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GeTzwxVDtzNn","executionInfo":{"status":"ok","timestamp":1763974129337,"user_tz":480,"elapsed":24712,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"07d1bfd4-4e86-438d-ebb3-307a72729e67"},"id":"GeTzwxVDtzNn","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.3.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["#@title Import from 'transformers'\n","from transformers import (\n","    AutoTokenizer, AutoModel, AutoModelForSequenceClassification,\n","    GPT2LMHeadModel, GPT2TokenizerFast,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tfnuR2YVCmW9","executionInfo":{"status":"ok","timestamp":1763974137346,"user_tz":480,"elapsed":5105,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"f1d29280-ed14-4114-a84a-15644a300e88"},"id":"tfnuR2YVCmW9","execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["#@title Import from 'rewrite'\n","from rewrite.mask_orig import Masker as Masker_single\n","from rewrite.generation import Infiller\n","from rewrite import rewrite_example as rx\n","import argparse as _argparse"],"metadata":{"id":"ccJxAWrjA8Qc","executionInfo":{"status":"ok","timestamp":1763974298786,"user_tz":480,"elapsed":23,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"ccJxAWrjA8Qc","execution_count":9,"outputs":[]},{"cell_type":"code","source":["#@title NLTK data\n","nltk.download(\"punkt\", quiet=True)\n","try:\n","    nltk.download(\"punkt_tab\", quiet=True)\n","except Exception:\n","    pass\n","print(\"NLTK ready\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y0Up7SKstzK9","executionInfo":{"status":"ok","timestamp":1763974148122,"user_tz":480,"elapsed":1457,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"fe315a72-5a6a-4a34-b688-09128ac6e523"},"id":"y0Up7SKstzK9","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["NLTK ready\n"]}]},{"cell_type":"code","source":["#@title Data configs\n","data_configs = {\n","    \"microagressions_val\": {\n","        \"data_path\": \"./datasets/microagressions/val.csv\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.25,\n","        \"temperature\": 2.5,\n","    },\n","    \"microagressions_test\": {\n","        \"data_path\": \"./datasets/microagressions/test.csv\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.25,\n","        \"temperature\": 2.5,\n","    },\n","    \"sbf_val\": {\n","        \"data_path\": \"./datasets/sbf/sbfdev.csv\",\n","        \"rep_penalty\": 1.5,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 5.0,\n","        \"temperature\": 2.9,\n","    },\n","    \"sbf_test\": {\n","        \"data_path\": \"./datasets/sbf/sbftst.csv\",\n","        \"rep_penalty\": 1.5,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 5.0,\n","        \"temperature\": 2.9,\n","    },\n","    \"dynabench_val\": {\n","        \"data_path\": \"./datasets/dynabench/db_dev.csv\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.75,\n","        \"temperature\": 2.5,\n","    },\n","    \"dynabench_test\": {\n","        \"data_path\": \"./datasets/dynabench/db_test.csv\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.75,\n","        \"temperature\": 2.5,\n","    },\n","    \"jigsaw_toxic\": {\n","        \"data_path\": \"./datasets/jigsaw_full_30/test_10k_toxic.txt\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.75,\n","        \"temperature\": 2.5,\n","    },\n","    \"paradetox\": {\n","        \"data_path\": \"./datasets/paradetox/test_toxic_parallel.txt\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.75,\n","        \"temperature\": 2.5,\n","    },\n","    \"appdia_original\": {\n","        \"data_path\": \"./datasets/appdia/original-annotated-data/original-test.tsv\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.75,\n","        \"temperature\": 2.5,\n","    },\n","    \"appdia_discourse\": {\n","        \"data_path\": \"./datasets/appdia/discourse-augmented-data/discourse-test.tsv\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.75,\n","        \"temperature\": 2.5,\n","    }\n","}\n","print(\"Datasets:\", \", \".join(data_configs.keys()))\n","\n","REPO = XDETOX_DIR\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7nBku39IuAgb","executionInfo":{"status":"ok","timestamp":1763974300061,"user_tz":480,"elapsed":30,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"dfdf671e-a65c-424d-ab76-0f8a6c2e604e"},"id":"7nBku39IuAgb","execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Datasets: microagressions_val, microagressions_test, sbf_val, sbf_test, dynabench_val, dynabench_test, jigsaw_toxic, paradetox, appdia_original, appdia_discourse\n"]}]},{"cell_type":"code","source":["#@title Helpers: subset data\n","def _abs_repo_path(rel: str) -> str:\n","    return os.path.join(REPO, rel.lstrip(\"./\"))\n","\n","def _ensure_dir(p: str):\n","    Path(p).mkdir(parents=True, exist_ok=True)\n","\n","def _subset_for_data_type(data_type, data_path, n, out_dir):\n","    \"\"\"\n","    Create a small subset file matching the expected format used by rewrite_example.get_data().\n","    Returns the path to the *new* subset file (or original path if n is None).\n","    \"\"\"\n","    if n is None or n <= 0:\n","        return data_path  # no subset\n","\n","    src = _abs_repo_path(data_path)\n","    _ensure_dir(out_dir)\n","\n","    if \"microagressions\" in data_path:\n","        df = pd.read_csv(src)\n","        sub = df.head(n)\n","        out = os.path.join(out_dir, os.path.basename(src))\n","        sub.to_csv(out, index=False)\n","        return out\n","\n","    if \"sbf\" in data_path:\n","        df = pd.read_csv(src)\n","        sub = df.head(n)\n","        out = os.path.join(out_dir, os.path.basename(src))\n","        sub.to_csv(out, index=False)\n","        return out\n","\n","    if \"dynabench\" in data_path:\n","        df = pd.read_csv(src)\n","        sub = df.head(n)\n","        out = os.path.join(out_dir, os.path.basename(src))\n","        sub.to_csv(out, index=False)\n","        return out\n","\n","    if any(k in data_path for k in [\"paradetox\", \"jigsaw\"]):\n","        # txt file\n","        if data_path.endswith(\".txt\"):\n","            with open(src, \"r\") as f:\n","                lines = [s.rstrip(\"\\n\") for s in f.readlines()]\n","            out = os.path.join(out_dir, os.path.basename(src))\n","            with open(out, \"w\") as g:\n","                for s in lines[:n]:\n","                    g.write(s + \"\\n\")\n","            return out\n","        elif data_path.endswith(\".csv\"):\n","            df = pd.read_csv(src).head(n)\n","            out = os.path.join(out_dir, os.path.basename(src))\n","            df.to_csv(out, index=False)\n","            return out\n","\n","    if \"appdia\" in data_path:\n","        # tsv\n","        df = pd.read_csv(src, sep=\"\\t\").head(n)\n","        out = os.path.join(out_dir, os.path.basename(src))\n","        df.to_csv(out, sep=\"\\t\", index=False)\n","        return out\n","\n","    # Fallback: copy original\n","    out = os.path.join(out_dir, os.path.basename(src))\n","    shutil.copy(src, out)\n","    return out\n"],"metadata":{"id":"ToytrY0SuAjr","executionInfo":{"status":"ok","timestamp":1763974300599,"user_tz":480,"elapsed":24,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"ToytrY0SuAjr","execution_count":11,"outputs":[]},{"cell_type":"code","source":["#@title Global scoring helpers: toxicity, similarity, fluency\n","\n","# Devices: Use GPU for scoring if available (much faster)\n","DEVICE_SCORE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Scoring models will use: {DEVICE_SCORE}\")\n","\n","# ---------- Toxicity model (textdetox/xlmr-large-toxicity-classifier-v2) ----------\n","_TOX_MODEL_NAME = \"textdetox/xlmr-large-toxicity-classifier-v2\"\n","_TOX_TOKENIZER = None\n","_TOX_MODEL = None\n","\n","def _lazy_load_tox():\n","    global _TOX_TOKENIZER, _TOX_MODEL\n","    if _TOX_TOKENIZER is None or _TOX_MODEL is None:\n","        _TOX_TOKENIZER = AutoTokenizer.from_pretrained(_TOX_MODEL_NAME)\n","        _TOX_MODEL = AutoModelForSequenceClassification.from_pretrained(\n","            _TOX_MODEL_NAME\n","        ).to(DEVICE_SCORE)\n","        _TOX_MODEL.eval()\n","\n","@torch.no_grad()\n","def get_toxicity_scores(texts, batch_size=32):\n","    \"\"\"\n","    Returns toxicity probabilities in [0,1] for each input text.\n","    (0 = non-toxic, 1 = very toxic)\n","    \"\"\"\n","    _lazy_load_tox()\n","    scores = []\n","    for i in tqdm(range(0, len(texts), batch_size), desc=\"Toxicity\", leave=False):\n","        batch = texts[i:i+batch_size]\n","        enc = _TOX_TOKENIZER(\n","            batch, return_tensors=\"pt\",\n","            truncation=True, max_length=512, padding=True\n","        ).to(DEVICE_SCORE)\n","        logits = _TOX_MODEL(**enc).logits\n","        probs = torch.softmax(logits, dim=-1)  # [..., 2]\n","        scores.extend(probs[:, 1].detach().cpu().tolist())  # toxic prob\n","    return scores\n","\n","# ---------- Semantic similarity (LaBSE) ----------\n","_LABSE_NAME = \"sentence-transformers/LaBSE\"\n","_LABSE_TOKENIZER = None\n","_LABSE_MODEL = None\n","\n","def _lazy_load_labse():\n","    global _LABSE_TOKENIZER, _LABSE_MODEL\n","    if _LABSE_TOKENIZER is None or _LABSE_MODEL is None:\n","        _LABSE_TOKENIZER = AutoTokenizer.from_pretrained(_LABSE_NAME)\n","        _LABSE_MODEL = AutoModel.from_pretrained(_LABSE_NAME).to(DEVICE_SCORE)\n","        _LABSE_MODEL.eval()\n","\n","@torch.no_grad()\n","def get_labse_embeddings(texts, batch_size=32):\n","    \"\"\"\n","    Returns a numpy array of shape (len(texts), hidden_dim).\n","    Mean-pooled LaBSE sentence embeddings.\n","    \"\"\"\n","    _lazy_load_labse()\n","    embs = []\n","    for i in tqdm(range(0, len(texts), batch_size), desc=\"LaBSE embeddings\", leave=False):\n","        batch = texts[i:i+batch_size]\n","        enc = _LABSE_TOKENIZER(\n","            batch, return_tensors=\"pt\",\n","            truncation=True, max_length=256, padding=True\n","        ).to(DEVICE_SCORE)\n","        outputs = _LABSE_MODEL(**enc)\n","        hidden = outputs.last_hidden_state  # [B, L, H]\n","        mask = enc[\"attention_mask\"].unsqueeze(-1)  # [B, L, 1]\n","        masked = hidden * mask\n","        summed = masked.sum(dim=1)  # [B, H]\n","        counts = mask.sum(dim=1).clamp(min=1e-6)  # [B,1]\n","        sent_emb = (summed / counts).cpu().numpy()\n","        embs.append(sent_emb)\n","    if not embs:\n","        return np.zeros((0, 768), dtype=np.float32)\n","    return np.vstack(embs)\n","\n","# ---------- Fluency via GPT-2 perplexity ----------\n","_GPT2_NAME = \"gpt2\"  # small model for speed; can switch to gpt2-medium if needed\n","_GPT2_TOKENIZER = None\n","_GPT2_MODEL = None\n","\n","def _lazy_load_gpt2():\n","    global _GPT2_TOKENIZER, _GPT2_MODEL\n","    if _GPT2_TOKENIZER is None or _GPT2_MODEL is None:\n","        _GPT2_TOKENIZER = GPT2TokenizerFast.from_pretrained(_GPT2_NAME)\n","        _GPT2_MODEL = GPT2LMHeadModel.from_pretrained(_GPT2_NAME).to(DEVICE_SCORE)\n","        _GPT2_MODEL.eval()\n","\n","@torch.no_grad()\n","def get_gpt2_perplexities(texts):\n","    \"\"\"\n","    Simple sentence-level perplexity using GPT-2.\n","    Returns a list of floats (one per text).\n","    \"\"\"\n","    import math as _math\n","    _lazy_load_gpt2()\n","    ppls = []\n","    for s in tqdm(texts, desc=\"GPT-2 PPL\", leave=False):\n","        enc = _GPT2_TOKENIZER(s, return_tensors=\"pt\").to(DEVICE_SCORE)\n","        out = _GPT2_MODEL(enc[\"input_ids\"], labels=enc[\"input_ids\"])\n","        ppl = _math.exp(out.loss.item())\n","        if ppl > 1e4:\n","            ppl = 1e4  # clip extreme\n","        ppls.append(float(ppl))\n","    return ppls\n","\n","def perplexity_to_fluency(ppls, p_min=5.0, p_max=300.0):\n","    \"\"\"\n","    Map perplexities to [0,1] fluency scores.\n","    Low perplexity -> high fluency.\n","    \"\"\"\n","    import math as _math\n","    ppls = np.asarray(ppls, dtype=float)\n","    p = np.clip(ppls, p_min, p_max)\n","    log_p = np.log(p)\n","    log_min = _math.log(p_min)\n","    log_max = _math.log(p_max)\n","    F = (log_max - log_p) / (log_max - log_min + 1e-8)\n","    F = np.clip(F, 0.0, 1.0)\n","    return F"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-MxQChqOuAnT","executionInfo":{"status":"ok","timestamp":1763974301162,"user_tz":480,"elapsed":32,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"9b5495bd-d9be-4f78-ea84-679cbb32a89e"},"id":"-MxQChqOuAnT","execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Scoring models will use: cuda\n"]}]},{"cell_type":"code","source":["#@title Global reranking: combine toxicity, similarity, fluency\n","def rerank_candidates_global(\n","    sources,\n","    candidates,\n","    weights=(0.5, 0.3, 0.2),\n","    ppl_min=5.0,\n","    ppl_max=300.0,\n","):\n","    \"\"\"\n","    sources: list[str], length N\n","    candidates: list[list[str]], shape N x C\n","    weights: (w_T, w_S, w_F)\n","    Returns:\n","        best_idx: np.ndarray of shape (N,), index of chosen candidate per source\n","        details: dict with matrices [N x C] for tox, safety, sim, flu, score\n","    \"\"\"\n","    w_T, w_S, w_F = weights\n","    N = len(sources)\n","    assert len(candidates) == N, \"candidates length mismatch\"\n","\n","    if N == 0:\n","        return np.array([], dtype=int), {}\n","\n","    C_list = [len(c) for c in candidates]\n","    assert len(set(C_list)) == 1, \"All inputs must have same num_candidates\"\n","    C = C_list[0]\n","    if C == 0:\n","        raise ValueError(\"num_candidates must be >= 1\")\n","\n","    # Flatten candidates and map to source indices\n","    flat_cands = []\n","    flat_src_idx = []\n","    for i, cand_list in enumerate(candidates):\n","        for cand in cand_list:\n","            flat_cands.append(cand)\n","            flat_src_idx.append(i)\n","    flat_src_idx = np.array(flat_src_idx, dtype=int)\n","\n","    # Toxicity\n","    tox = np.array(get_toxicity_scores(flat_cands), dtype=float)  # [N*C]\n","\n","    # Semantic similarity (LaBSE)\n","    src_embs = get_labse_embeddings(sources)  # [N, D]\n","    cand_embs = get_labse_embeddings(flat_cands)  # [N*C, D]\n","    # Normalize\n","    src_embs = src_embs / np.clip(np.linalg.norm(src_embs, axis=1, keepdims=True), 1e-8, None)\n","    cand_embs = cand_embs / np.clip(np.linalg.norm(cand_embs, axis=1, keepdims=True), 1e-8, None)\n","    # Cosine between each candidate and its source\n","    sims = np.sum(cand_embs * src_embs[flat_src_idx], axis=1)  # [-1,1]\n","    sims = (sims + 1.0) / 2.0  # -> [0,1]\n","\n","    # Fluency: GPT-2 PPL -> F in [0,1]\n","    ppls = np.array(get_gpt2_perplexities(flat_cands), dtype=float)\n","    flus = perplexity_to_fluency(ppls, p_min=ppl_min, p_max=ppl_max)\n","\n","    # Safety\n","    safety = 1.0 - tox\n","\n","    # Global score\n","    scores = w_T * safety + w_S * sims + w_F * flus\n","\n","    # Reshape to [N, C]\n","    tox2     = tox.reshape(N, C)\n","    safety2  = safety.reshape(N, C)\n","    sims2    = sims.reshape(N, C)\n","    flus2    = flus.reshape(N, C)\n","    scores2  = scores.reshape(N, C)\n","\n","    best_idx = scores2.argmax(axis=1)\n","    details = {\n","        \"tox\": tox2,\n","        \"safety\": safety2,\n","        \"sim\": sims2,\n","        \"flu\": flus2,\n","        \"score\": scores2,\n","    }\n","    return best_idx, details\n"],"metadata":{"id":"dom7rBbguA2u","executionInfo":{"status":"ok","timestamp":1763974302656,"user_tz":480,"elapsed":491,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"dom7rBbguA2u","execution_count":13,"outputs":[]},{"cell_type":"code","source":["#@title Evaluation helpers (reuse evaluate_all.py with toxicity)\n","def _parse_run_folder_name(folder_name):\n","    pattern = r\"aa(\\d+\\.\\d+)_ae(\\d+\\.\\d+)_ab(\\d+\\.\\d+)_base(.*?)_anti(.*?)_expert(.*?)_temp(\\d+\\.\\d+)_sample(.*?)_topk(\\d+)_reppenalty(\\d+\\.\\d+)_filterp(\\d+\\.\\d+)_maxlength(\\d+)_topp(\\d+\\.\\d+)\"\n","    m = re.match(pattern, folder_name)\n","    return bool(m)\n","\n","def _eval_with_toxicity(base_path, overwrite_eval=False, skip_ref=False, tox_threshold=0.5, tox_batch_size=32):\n","    \"\"\"\n","    Call evaluation.evaluate_all (the new one with local toxicity) on each gen folder.\n","    \"\"\"\n","    import sys as _sys, os as _os\n","    for folder in os.listdir(base_path):\n","        gen_dir = os.path.join(base_path, folder)\n","        if not os.path.isdir(gen_dir) or not _parse_run_folder_name(folder):\n","            continue\n","        orig_path = os.path.join(gen_dir, \"orig.txt\")\n","        gen_path  = os.path.join(gen_dir, \"gen.txt\")\n","        out_stats = os.path.join(gen_dir, \"gen_stats.txt\")\n","        if not (os.path.exists(orig_path) and os.path.exists(gen_path)):\n","            continue\n","        if os.path.exists(out_stats) and not overwrite_eval:\n","            continue\n","\n","        env = os.environ.copy()\n","        env[\"PYTHONPATH\"] = REPO + (\":\" + env.get(\"PYTHONPATH\",\"\") if env.get(\"PYTHONPATH\") else \"\")\n","        cmd = [\n","            _sys.executable, \"-m\", \"evaluation.evaluate_all\",\n","            \"--orig_path\", orig_path,\n","            \"--gen_path\",  gen_path,\n","            \"--tox_threshold\", str(tox_threshold),\n","            \"--tox_batch_size\", str(tox_batch_size),\n","        ]\n","        if skip_ref:\n","            cmd.append(\"--skip_ref\")\n","        print(\"Eval:\", \" \".join(cmd))\n","        res = run(cmd, cwd=REPO, env=env, stdout=PIPE, stderr=PIPE, text=True)\n","        if res.returncode != 0:\n","            print(res.stdout)\n","            print(res.stderr)\n","            res.check_returncode()\n","\n","def _safe_float(x):\n","    try:\n","        return float(x)\n","    except Exception:\n","        return float('nan')\n","\n","def _read_stats_file(path):\n","    \"\"\"\n","    Read gen_stats.txt into a dict of floats; tolerate '(skipped): None'.\n","    \"\"\"\n","    out = {}\n","    with open(path, \"r\") as f:\n","        for line in f:\n","            if \":\" not in line:\n","                continue\n","            k, v = line.strip().split(\": \", 1)\n","            k = k.replace(\"(skipped)\", \"\").strip().lower()\n","            out[k] = _safe_float(v)\n","    return out\n","\n","def _aggregate_eval_csv(output_folder, data_type, base_out_dir):\n","    rows = []\n","    for thresh in np.arange(0.15, 0.3, 0.05, dtype=np.float64):\n","        mask_dir = f\"DecompX{abs(thresh):g}\" if thresh != 0 else \"DecompX0.0\"\n","        base_path = os.path.join(base_out_dir, data_type, mask_dir)\n","        if not os.path.isdir(base_path):\n","            continue\n","        for folder in os.listdir(base_path):\n","            gen_dir = os.path.join(base_path, folder)\n","            stats_path = os.path.join(gen_dir, \"gen_stats.txt\")\n","            if not os.path.exists(stats_path):\n","                continue\n","            s = _read_stats_file(stats_path)\n","            rows.append({\n","                \"threshold\":        float(f\"{thresh:.2f}\"),\n","                \"folder\":           folder,\n","                \"bertscore\":        s.get(\"bertscore\", np.nan),\n","                \"meaningbert\":      s.get(\"meaningbert\", np.nan),   # <-- new\n","                \"bleu4\":            s.get(\"bleu4\", np.nan),\n","                \"perplexity_gen\":   s.get(\"perplexity gen\", np.nan),\n","                \"perplexity_orig\":  s.get(\"perplexity orig\", np.nan),\n","                \"toxicity_gen\":     s.get(\"toxicity gen\", np.nan),\n","                \"toxicity_orig\":    s.get(\"toxicity orig\", np.nan),\n","            })\n","\n","    if rows:\n","        cols = [\n","            \"threshold\", \"folder\",\n","            \"bertscore\", \"meaningbert\", \"bleu4\",   # <-- MeaningBERT between bertsore & bleu4\n","            \"perplexity_gen\", \"perplexity_orig\",\n","            \"toxicity_gen\", \"toxicity_orig\",\n","        ]\n","        df = pd.DataFrame(rows)\n","        df = df[cols]\n","        out_csv = os.path.join(base_out_dir, data_type, f\"{data_type}.csv\")\n","        df.to_csv(out_csv, index=False)\n","        print(\"Wrote summary CSV:\", out_csv)\n","    else:\n","        print(\"No evaluation files found to summarize.\")\n"],"metadata":{"id":"ybmqf84duA4a","executionInfo":{"status":"ok","timestamp":1763974303210,"user_tz":480,"elapsed":4,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"ybmqf84duA4a","execution_count":14,"outputs":[]},{"cell_type":"code","source":["#@title Masking + generation with global reranking\n","\n","\n","def _process_in_batches(masker, inputs, batch_size, thresh: float):\n","    batched_inputs = [\n","        inputs[i : i + batch_size] for i in range(0, len(inputs), batch_size)\n","    ]\n","    results = []\n","    for batch in tqdm(batched_inputs, desc=\"Masking (DecompX)\", leave=False):\n","        batch_result = masker.process_text(sentence=batch, threshold=thresh)\n","        results.append(batch_result)\n","    return results\n","\n","def _bool2str(x: bool) -> str:\n","    return \"T\" if x else \"F\"\n","\n","def _build_gen_folder_name(\n","    alpha_a, alpha_e, alpha_b,\n","    base_type, antiexpert_type, expert_type,\n","    temperature, sample, top_k_gen, rep_penalty, filter_p, max_length, top_p\n","):\n","    return (\n","        \"aa\" + str(alpha_a) +\n","        \"_ae\" + str(alpha_e) +\n","        \"_ab\" + str(alpha_b) +\n","        \"_base\" + base_type[:5] +\n","        \"_anti\" + antiexpert_type[:5] +\n","        \"_expert\" + expert_type[:5] +\n","        \"_temp\" + str(temperature) +\n","        \"_sample\" + _bool2str(sample) +\n","        \"_topk\" + str(top_k_gen) +\n","        \"_reppenalty\" + str(rep_penalty) +\n","        \"_filterp\" + str(filter_p) +\n","        \"_maxlength\" + str(max_length) +\n","        \"_topp\" + str(top_p)\n","    )\n","\n","def _run_global_reranking_for_threshold(\n","    data_type,\n","    subset_path,\n","    thresh,\n","    base_out_rel,\n","    batch_size,\n","    alpha_a, alpha_e, alpha_b,\n","    temperature,\n","    rep_penalty,\n","    max_length,\n","    top_k_gen,\n","    top_p,\n","    filter_p,\n","    sample,\n","    num_candidates,\n","    weights,\n","    overwrite_gen=False,\n","):\n","    \"\"\"\n","    For one threshold value:\n","      - load inputs using rewrite_example.get_data\n","      - mask with DecompX (Masker_single)\n","      - generate num_candidates samples per input with Infiller\n","      - rerank with global score\n","      - save orig.txt / gen.txt under the usual folder\n","    \"\"\"\n","    # Load inputs using original get_data logic\n","    args_data = _argparse.Namespace(data_type=data_type, data_path=subset_path)\n","    inputs = rx.get_data(args_data)\n","    print(f\"#inputs at thresh={thresh}: {len(inputs)}\")\n","\n","    # Paths\n","    mask_dir = f\"DecompX{abs(thresh):g}\" if thresh != 0 else \"DecompX0.0\"\n","    cur_rel = os.path.join(base_out_rel, data_type, mask_dir)\n","    cur_abs = os.path.join(REPO, cur_rel)\n","    _ensure_dir(cur_abs)\n","\n","    masked_file = os.path.join(cur_abs, \"masked_inputs.txt\")\n","\n","    # Masking (reuse if exists unless you want to overwrite)\n","    if not os.path.exists(masked_file):\n","        masker = Masker_single()\n","        decoded_masked_inputs_batches = _process_in_batches(\n","            masker, inputs, batch_size=batch_size, thresh=thresh\n","        )\n","        decoded_masked_inputs = [\n","            item for sublist in decoded_masked_inputs_batches for item in sublist\n","        ]\n","        decoded_mask_inputs = [\n","            d.replace(\"<s>\", \"\").replace(\"</s>\", \"\") for d in decoded_masked_inputs\n","        ]\n","        with open(masked_file, \"w\") as f:\n","            for d in decoded_mask_inputs:\n","                f.write(re.sub(r\"\\s+\", \" \", d) + \"\\n\")\n","        masker.release_model()\n","    else:\n","        with open(masked_file, \"r\") as f:\n","            decoded_mask_inputs = [s.strip() for s in f.readlines()]\n","        print(\"Reusing existing masked_inputs.txt\")\n","\n","    assert len(decoded_mask_inputs) == len(inputs), \"Masked vs inputs mismatch\"\n","\n","    # Initialize Infiller (same as in rewrite_example)\n","    rewriter = Infiller(\n","        seed=0,\n","        base_path=\"facebook/bart-base\",\n","        antiexpert_path=\"hallisky/bart-base-toxic-antiexpert\",\n","        expert_path=\"hallisky/bart-base-nontoxic-expert\",\n","        base_type=\"base\",\n","        antiexpert_type=\"antiexpert\",\n","        expert_type=\"expert\",\n","        tokenizer=\"facebook/bart-base\",\n","    )\n","\n","    # Build generation folder name\n","    base_type = \"base\"\n","    antiexpert_type = \"antiexpert\"\n","    expert_type = \"expert\"\n","    gen_folder = _build_gen_folder_name(\n","        alpha_a, alpha_e, alpha_b,\n","        base_type, antiexpert_type, expert_type,\n","        temperature, sample, top_k_gen, rep_penalty, filter_p, max_length, top_p\n","    )\n","    final_abs = os.path.join(cur_abs, gen_folder)\n","    gen_txt = os.path.join(final_abs, \"gen.txt\")\n","    orig_txt = os.path.join(final_abs, \"orig.txt\")\n","\n","    if os.path.exists(gen_txt) and not overwrite_gen:\n","        print(\"Generation already exists at:\", gen_txt, \"— skipping generation.\")\n","        return\n","\n","    _ensure_dir(final_abs)\n","\n","    # Generate multiple candidates per input\n","    all_candidates: List[List[str]] = [[] for _ in range(len(inputs))]\n","\n","    print(f\"Generating {num_candidates} candidates per input (sampling={sample})\")\n","    for c in range(num_candidates):\n","        outs, decoded = rewriter.generate(\n","            inputs,\n","            decoded_mask_inputs,\n","            alpha_a=alpha_a,\n","            alpha_e=alpha_e,\n","            alpha_b=alpha_b,\n","            temperature=temperature,\n","            verbose=False,\n","            max_length=max_length,\n","            repetition_penalty=rep_penalty,\n","            p=top_p,\n","            filter_p=filter_p,\n","            k=top_k_gen,\n","            batch_size=batch_size,\n","            sample=sample,\n","            ranking=False,          # <-- no DecompX reranking\n","            ranking_eval_output=0,\n","        )\n","        for i, text in enumerate(decoded):\n","            all_candidates[i].append(re.sub(r\"\\s+\", \" \", text).strip())\n","\n","    # --- Memory Cleanup before Scoring ---\n","    # We are done with the BART generation model.\n","    # Delete it and clear CUDA cache to make room for XLM-R (toxicity) and LaBSE (similarity).\n","    del rewriter\n","    if torch.cuda.is_available():\n","        torch.cuda.empty_cache()\n","    # -------------------------------------\n","\n","    # Global reranking\n","    print(\"Global reranking (toxicity + similarity + fluency)...\")\n","    best_idx, details = rerank_candidates_global(\n","        sources=inputs,\n","        candidates=all_candidates,\n","        weights=weights,\n","    )\n","    best_generations = [\n","        all_candidates[i][best_idx[i]] for i in range(len(inputs))\n","    ]\n","\n","    # Save orig + chosen gen\n","    with open(orig_txt, \"w\") as f:\n","        for l in inputs:\n","            f.write(re.sub(r\"\\s+\", \" \", l).strip() + \"\\n\")\n","    with open(gen_txt, \"w\") as f:\n","        for l in best_generations:\n","            f.write(re.sub(r\"\\s+\", \" \", l).strip() + \"\\n\")\n","\n","    print(\"Saved:\", orig_txt)\n","    print(\"Saved:\", gen_txt)"],"metadata":{"id":"U5oOUWRYuA6E","executionInfo":{"status":"ok","timestamp":1763974303217,"user_tz":480,"elapsed":3,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"U5oOUWRYuA6E","execution_count":15,"outputs":[]},{"cell_type":"code","source":["#@title `detoxify()` — masking + global reranking + optional eval\n","\n","def detoxify(\n","    data_type: str = \"paradetox\",\n","    output_folder: str = \"colab_run_global\",\n","    thresholds = (0.15, 0.20, 0.25),\n","    batch_size: int = 10,\n","    ranking: bool = True,   # kept for API symmetry; if False, you could adapt to skip reranking\n","    sample: bool = True,\n","    top_k_gen: int = 50,\n","    top_p: float = 0.95,\n","    filter_p: float = 1.0,\n","    max_length: int = 128,\n","    alpha_a: float = None,   # if None, take from data_configs\n","    alpha_e: float = None,   # if None, take from data_configs\n","    alpha_b: float = 1.0,\n","    temperature: float = None,  # if None, from data_configs\n","    rep_penalty: float = None,  # if None, from data_configs\n","    num_examples: int = 100,    # small-batch control; set None to use full dataset\n","    overwrite_gen: bool = False,\n","    run_eval: bool = False,\n","    overwrite_eval: bool = False,\n","    skip_ref_eval: bool = False,\n","    # NEW:\n","    weights = (0.5, 0.3, 0.2),   # (w_T, w_S, w_F)\n","    num_candidates: int = 3,     # candidates per input\n","):\n","    \"\"\"\n","    Run XDetox with DecompX masking + global reranking based on:\n","      - toxicity (XLM-R large)\n","      - semantic similarity (LaBSE)\n","      - fluency (GPT-2 perplexity -> [0,1])\n","\n","    Parameters match XDetox_Pipeline.ipynb plus:\n","      - weights: (w_T, w_S, w_F)\n","      - num_candidates: int, candidates per input\n","    \"\"\"\n","    assert data_type in data_configs, f\"Unknown data_type: {data_type}\"\n","    cfg = data_configs[data_type].copy()\n","\n","    if num_candidates < 1:\n","        raise ValueError(\"num_candidates must be >= 1\")\n","\n","    # fallbacks from data_configs\n","    if alpha_a is None: alpha_a = cfg[\"alpha_a\"]\n","    if alpha_e is None: alpha_e = cfg[\"alpha_e\"]\n","    if temperature is None: temperature = cfg[\"temperature\"]\n","    if rep_penalty is None: rep_penalty = cfg[\"rep_penalty\"]\n","\n","    base_out_rel = os.path.join(\"data\", \"dexp_outputs\", output_folder)\n","    base_out_abs = os.path.join(REPO, base_out_rel)\n","    _ensure_dir(base_out_abs)\n","\n","    # subset path (file)\n","    original_data_path = cfg[\"data_path\"]\n","    subset_dir = os.path.join(REPO, \"datasets\", \"_subsets\", data_type)\n","    _ensure_dir(subset_dir)\n","    subset_path = _subset_for_data_type(\n","        data_type, original_data_path, num_examples, subset_dir\n","    )\n","\n","    print(f\"Data type: {data_type}\")\n","    print(f\"Subset path: {subset_path}\")\n","    print(f\"Output base: {base_out_abs}\")\n","    print(f\"Weights (w_T, w_S, w_F): {weights}\")\n","    print(f\"num_candidates per input: {num_candidates}\")\n","\n","    # run thresholds\n","    for t in thresholds:\n","        print(\"=\" * 60)\n","        print(f\"Threshold (DecompX) = {t:.2f}\")\n","        _run_global_reranking_for_threshold(\n","            data_type=data_type,\n","            subset_path=subset_path,\n","            thresh=t,\n","            base_out_rel=base_out_rel,\n","            batch_size=batch_size,\n","            alpha_a=alpha_a,\n","            alpha_e=alpha_e,\n","            alpha_b=alpha_b,\n","            temperature=temperature,\n","            rep_penalty=rep_penalty,\n","            max_length=max_length,\n","            top_k_gen=top_k_gen,\n","            top_p=top_p,\n","            filter_p=filter_p,\n","            sample=sample,\n","            num_candidates=num_candidates,\n","            weights=weights,\n","            overwrite_gen=overwrite_gen,\n","        )\n","\n","        if run_eval:\n","            mask_dir = f\"DecompX{abs(t):g}\" if t != 0 else \"DecompX0.0\"\n","            base_path = os.path.join(base_out_abs, data_type, mask_dir)\n","            _eval_with_toxicity(\n","                base_path,\n","                overwrite_eval=overwrite_eval,\n","                skip_ref=skip_ref_eval,\n","                tox_threshold=0.5,\n","                tox_batch_size=32,\n","            )\n","\n","    # Summarize metrics across thresholds\n","    if run_eval:\n","        _aggregate_eval_csv(output_folder, data_type,\n","                            os.path.join(REPO, \"data\", \"dexp_outputs\", output_folder))\n"],"metadata":{"id":"oqBLx5OSuA72","executionInfo":{"status":"ok","timestamp":1763974303251,"user_tz":480,"elapsed":4,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"oqBLx5OSuA72","execution_count":16,"outputs":[]},{"cell_type":"code","source":["#@title Example run — paradetox, small subset with global reranking\n","\n","# detoxify(\n","#     data_type=\"paradetox\",\n","#     output_folder=\"colab_run_global_demo\",\n","#     thresholds=(0.20,),          # single threshold for quick test\n","#     batch_size=8,                # T4-friendly\n","#     ranking=True,                # flag kept for symmetry, global reranking is always used here\n","#     sample=True,\n","#     top_k_gen=50,\n","#     top_p=0.95,\n","#     max_length=96,\n","#     num_examples=50,             # small subset\n","#     run_eval=True,               # BLEU/BERTScore/PPL/Toxicity via evaluate_all.py\n","#     overwrite_gen=False,\n","#     overwrite_eval=True,\n","#     skip_ref_eval=False,\n","#     weights=(0.5, 0.3, 0.2),     # (w_T, w_S, w_F): safety, similarity, fluency\n","#     num_candidates=20             # candidates per input for reranking\n","# )\n"],"metadata":{"id":"u5LlySYquA9g","collapsed":true,"executionInfo":{"status":"ok","timestamp":1763974303257,"user_tz":480,"elapsed":3,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"u5LlySYquA9g","execution_count":17,"outputs":[]},{"cell_type":"code","source":["detoxify(\n","    data_type=\"paradetox\",\n","    output_folder=\"colab_run_global_demo\",\n","    thresholds=(0.20,),\n","    batch_size=8,\n","    ranking=True,\n","    sample=True,\n","    top_k_gen=50,\n","    top_p=0.95,\n","    max_length=96,\n","    num_examples=50,\n","    run_eval=True,        # <-- keep this True\n","    overwrite_gen=False,  # <-- do NOT touch generation\n","    overwrite_eval=True,  # <-- force recompute gen_stats.txt\n","    skip_ref_eval=False,\n","    weights=(0.5, 0.3, 0.2),\n","    num_candidates=20,\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O2S9fX-adatU","executionInfo":{"status":"ok","timestamp":1763976490989,"user_tz":480,"elapsed":403932,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"8f8d6497-8af8-4239-979d-fdb9d40fdb7c"},"id":"O2S9fX-adatU","execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Data type: paradetox\n","Subset path: /content/drive/MyDrive/w266 - Project/XDetox/datasets/_subsets/paradetox/test_toxic_parallel.txt\n","Output base: /content/drive/MyDrive/w266 - Project/XDetox/data/dexp_outputs/colab_run_global_demo\n","Weights (w_T, w_S, w_F): (0.5, 0.3, 0.2)\n","num_candidates per input: 20\n","============================================================\n","Threshold (DecompX) = 0.20\n","#inputs at thresh=0.2: 50\n","Reusing existing masked_inputs.txt\n","Found 1 GPUS!\n","Generation already exists at: /content/drive/MyDrive/w266 - Project/XDetox/data/dexp_outputs/colab_run_global_demo/paradetox/DecompX0.2/aa1.5_ae4.75_ab1.0_basebase_antiantie_expertexper_temp2.5_sampleT_topk50_reppenalty1.0_filterp1.0_maxlength96_topp0.95/gen.txt — skipping generation.\n","Eval: /usr/bin/python3 -m evaluation.evaluate_all --orig_path /content/drive/MyDrive/w266 - Project/XDetox/data/dexp_outputs/colab_run_global_demo/paradetox/DecompX0.2/aa1.5_ae4.75_ab1.0_basebase_antiantie_expertexper_temp2.5_sampleT_topk50_reppenalty1.0_filterp1.0_maxlength96_topp0.95/orig.txt --gen_path /content/drive/MyDrive/w266 - Project/XDetox/data/dexp_outputs/colab_run_global_demo/paradetox/DecompX0.2/aa1.5_ae4.75_ab1.0_basebase_antiantie_expertexper_temp2.5_sampleT_topk50_reppenalty1.0_filterp1.0_maxlength96_topp0.95/gen.txt --tox_threshold 0.5 --tox_batch_size 32\n","Wrote summary CSV: /content/drive/MyDrive/w266 - Project/XDetox/data/dexp_outputs/colab_run_global_demo/paradetox/paradetox.csv\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"VgZRbEBED8w4","executionInfo":{"status":"aborted","timestamp":1763974590493,"user_tz":480,"elapsed":11,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"VgZRbEBED8w4","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}
{"cells":[{"cell_type":"markdown","metadata":{"id":"dbKGwn6Gar5F"},"source":["# ProjectNotebook2Merged: T5-ParaDetox vs XDetox Pipeline Comparison\n","\n","This notebook compares two approaches to text detoxification:\n","1. **T5-ParaDetox**: Supervised fine-tuning of T5-base on ParaDetox dataset\n","2. **XDetox**: Product-of-Experts (DecompX) approach using base/expert/anti-expert models\n","\n","Both pipelines are evaluated on multiple test datasets with comprehensive metrics."]},{"cell_type":"markdown","metadata":{"id":"uUdibNUAar5J"},"source":["## Section 1: Setup & Dependencies"]},{"cell_type":"markdown","metadata":{"id":"s67Ve8n1ar5J"},"source":["### Mount Google Drive"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HX_JffXfar5K","executionInfo":{"status":"ok","timestamp":1763963490045,"user_tz":480,"elapsed":948,"user":{"displayName":"Benjamin Chen He","userId":"13010186992209467638"}},"outputId":"24670f78-d7d8-4a0b-939d-20cef3a74d67"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"9fshQh8Gar5K"},"source":["### Install Dependencies\n","\n","**Approach**: Use Colab's pre-installed packages (same as ProjectNotebook1).\n","\n","This matches what ProjectNotebook1 actually used in November 2024:\n","- `transformers` 4.57.1+ (Colab's latest)\n","- `datasets` 4.0.0+ (Colab's latest)\n","- No version pinning needed - Colab maintains compatible versions\n","\n","After running this cell, go to **Runtime â†’ Restart Runtime** to ensure clean imports."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8UI_Tm4bar5L","executionInfo":{"status":"ok","timestamp":1763963581196,"user_tz":480,"elapsed":91148,"user":{"displayName":"Benjamin Chen He","userId":"13010186992209467638"}},"outputId":"124299ce-88b6-4b17-ff9f-fcf7e3f01994"},"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","âœ“ All packages installed\n","======================================================================\n","\n","ðŸ” Verifying package compatibility...\n","\n","âœ“ transformers 4.37.2\n","âœ“ datasets 2.19.0\n","âœ“ sacremoses 0.1.1\n","âœ“ ftfy installed\n","âœ“ nltk installed\n","\n","âŒ Unexpected error: Failed to import transformers.trainer because of the following error (look up to see its traceback):\n","cannot import name 'EncoderDecoderCache' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)\n"]}],"source":["# Install required packages (use Colab's pre-installed versions where possible)\n","# This matches ProjectNotebook1's approach: let Colab provide compatible versions\n","\n","# Core packages (likely already installed in Colab)\n","!pip install -q transformers torch datasets\n","\n","# XDetox-specific dependencies\n","!pip install -q sacremoses ftfy nltk\n","\n","# Evaluation and training packages\n","!pip install -q evaluate sacrebleu\n","!pip install -q sentence-transformers\n","!pip install -q pandas numpy scikit-learn matplotlib\n","!pip install -q accelerate -U\n","\n","# Evaluation metrics\n","!pip install -q rouge_score\n","!pip install -q bert-score\n","\n","print(\"=\" * 70)\n","print(\"âœ“ All packages installed\")\n","print(\"=\" * 70)\n","\n","# Verify all critical imports for both pipelines\n","print(\"\\nðŸ” Verifying package compatibility...\\n\")\n","\n","try:\n","    # Check transformers and datasets versions\n","    import transformers\n","    import datasets\n","    print(f\"âœ“ transformers {transformers.__version__}\")\n","    print(f\"âœ“ datasets {datasets.__version__}\")\n","\n","    # Check XDetox dependencies\n","    import sacremoses\n","    import ftfy\n","    import nltk\n","    print(f\"âœ“ sacremoses {sacremoses.__version__}\")\n","    print(f\"âœ“ ftfy installed\")\n","    print(f\"âœ“ nltk installed\")\n","\n","    # Check T5-ParaDetox requirements (Trainer import)\n","    from transformers import (\n","        Trainer,\n","        TrainingArguments,\n","        T5Tokenizer,\n","        T5ForConditionalGeneration\n","    )\n","    print(f\"âœ“ Trainer imports successfully\")\n","    print(f\"âœ“ T5 models available\")\n","\n","    # Check XDetox requirements (apply_chunking_to_forward)\n","    from transformers.modeling_utils import apply_chunking_to_forward\n","    print(f\"âœ“ apply_chunking_to_forward available (XDetox needs this)\")\n","\n","    # Check evaluation requirements\n","    from sentence_transformers import SentenceTransformer\n","    from evaluate import load\n","    print(f\"âœ“ SentenceTransformer available\")\n","    print(f\"âœ“ evaluate library available\")\n","\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\"ðŸŽ‰ SUCCESS! All dependencies are compatible.\")\n","    print(\"Both T5-ParaDetox and XDetox pipelines should work correctly.\")\n","    print(\"=\" * 70)\n","\n","except ImportError as e:\n","    print(f\"\\nâŒ Import failed: {e}\")\n","    print(\"\\nâš ï¸  Troubleshooting:\")\n","    print(\"  1. Go to Runtime â†’ Restart Runtime\")\n","    print(\"  2. Re-run this cell\")\n","    print(\"  3. If error persists, check which specific import failed above\")\n","except Exception as e:\n","    print(f\"\\nâŒ Unexpected error: {e}\")"]},{"cell_type":"markdown","metadata":{"id":"7_m_h2d7ar5M"},"source":["### Import Libraries"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":426},"id":"3M6Rmuyuar5M","executionInfo":{"status":"error","timestamp":1763963581552,"user_tz":480,"elapsed":353,"user":{"displayName":"Benjamin Chen He","userId":"13010186992209467638"}},"outputId":"da4f6b5e-b990-449e-c885-61fe2b000f22"},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Failed to import transformers.trainer because of the following error (look up to see its traceback):\ncannot import name 'EncoderDecoderCache' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1363\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_peft_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpeft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m from .auto import (\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/auto.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m from .peft_model import (\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mPeftModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMSELoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDynamicCache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEncoderDecoderCache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_outputs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQuestionAnsweringModelOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequenceClassifierOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTokenClassifierOutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'EncoderDecoderCache' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3700709809.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Import transformers (same as ProjectNotebook1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m from transformers import (\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1352\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1354\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1366\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1367\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.trainer because of the following error (look up to see its traceback):\ncannot import name 'EncoderDecoderCache' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)"]}],"source":["import os\n","import sys\n","import re\n","import json\n","import shutil\n","import math\n","from pathlib import Path\n","from subprocess import run, PIPE\n","\n","import torch\n","import numpy as np\n","import pandas as pd\n","import nltk\n","from tqdm.auto import tqdm\n","from sklearn.model_selection import train_test_split\n","\n","# Import transformers (same as ProjectNotebook1)\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    T5Tokenizer,\n","    T5ForConditionalGeneration,\n","    GPT2LMHeadModel,\n","    GPT2Tokenizer,\n","    TrainingArguments,\n","    Trainer\n",")\n","from torch.utils.data import Dataset\n","from sentence_transformers import SentenceTransformer\n","from evaluate import load\n","\n","# Disable wandb logging\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\n","print(\"âœ“ Libraries imported\")\n","print(f\"âœ“ CUDA available: {torch.cuda.is_available()}\")\n","if torch.cuda.is_available():\n","    print(f\"âœ“ GPU: {torch.cuda.get_device_name(0)}\")"]},{"cell_type":"markdown","metadata":{"id":"udoXyLv1ar5N"},"source":["### Setup NLTK"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JQpPSKX0ar5N","executionInfo":{"status":"aborted","timestamp":1763963581633,"user_tz":480,"elapsed":37,"user":{"displayName":"Benjamin Chen He","userId":"13010186992209467638"}}},"outputs":[],"source":["nltk.download(\"punkt\", quiet=True)\n","try:\n","    nltk.download(\"punkt_tab\", quiet=True)\n","except Exception:\n","    pass\n","print(\"âœ“ NLTK ready\")"]},{"cell_type":"markdown","metadata":{"id":"IjHHL-tzar5N"},"source":["### Configure Paths\n","\n","**IMPORTANT**: Update these paths based on your Google Drive structure!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iXTz80rOar5N","executionInfo":{"status":"aborted","timestamp":1763963581637,"user_tz":480,"elapsed":92597,"user":{"displayName":"Benjamin Chen He","userId":"13010186992209467638"}}},"outputs":[],"source":["# ========================================\n","# CONFIGURE THESE PATHS FOR YOUR SETUP\n","# ========================================\n","\n","# Path to your project folder (contains ParaDetox data and model checkpoints)\n","# PLACEHOLDER - Update this!\n","PROJECT_BASE_PATH = \"/content/drive/MyDrive/ds266/w266 - Project\"\n","\n","# Path to XDetox repository (contains rewrite/, evaluation/, datasets/ folders)\n","# PLACEHOLDER - Update this!\n","XDETOX_DIR = \"/content/drive/MyDrive/ds266/w266 - Project/XDetox\"\n","\n","# ========================================\n","# Verify paths exist\n","# ========================================\n","assert os.path.isdir(PROJECT_BASE_PATH), f\"PROJECT_BASE_PATH not found: {PROJECT_BASE_PATH}\"\n","assert os.path.isdir(XDETOX_DIR), f\"XDETOX_DIR not found: {XDETOX_DIR}\"\n","\n","# Setup XDetox environment\n","HF_CACHE = os.path.join(XDETOX_DIR, \"cache\")\n","os.environ[\"TRANSFORMERS_CACHE\"] = HF_CACHE\n","if XDETOX_DIR not in sys.path:\n","    sys.path.append(XDETOX_DIR)\n","\n","# Verify XDetox repo structure\n","for d in [\"rewrite\", \"evaluation\", \"datasets\"]:\n","    assert os.path.isdir(os.path.join(XDETOX_DIR, d)), f\"Missing XDetox folder: {d}\"\n","\n","print(\"âœ“ PROJECT_BASE_PATH:\", PROJECT_BASE_PATH)\n","print(\"âœ“ XDETOX_DIR:\", XDETOX_DIR)\n","print(\"âœ“ HF_CACHE:\", HF_CACHE)\n","print(\"âœ“ All paths verified\")"]},{"cell_type":"markdown","metadata":{"id":"_8bWivy6ar5O"},"source":["### Dataset Configuration\n","\n","These are the available test datasets for both pipelines."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3PbAqDa1ar5O","executionInfo":{"status":"aborted","timestamp":1763963581640,"user_tz":480,"elapsed":92599,"user":{"displayName":"Benjamin Chen He","userId":"13010186992209467638"}}},"outputs":[],"source":["# XDetox dataset configurations (from XDetox_Pipeline.ipynb)\n","XDETOX_DATASETS = {\n","    \"paradetox\": {\n","        \"data_path\": \"./datasets/paradetox/test_toxic_parallel.txt\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.75,\n","        \"temperature\": 2.5,\n","    },\n","    \"microagressions_test\": {\n","        \"data_path\": \"./datasets/microagressions/test.csv\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.25,\n","        \"temperature\": 2.5,\n","    },\n","    \"sbf_test\": {\n","        \"data_path\": \"./datasets/sbf/sbftst.csv\",\n","        \"rep_penalty\": 1.5,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 5.0,\n","        \"temperature\": 2.9,\n","    },\n","    \"dynabench_test\": {\n","        \"data_path\": \"./datasets/dynabench/db_test.csv\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.75,\n","        \"temperature\": 2.5,\n","    },\n","    \"jigsaw_toxic\": {\n","        \"data_path\": \"./datasets/jigsaw_full_30/test_10k_toxic.txt\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.75,\n","        \"temperature\": 2.5,\n","    },\n","    \"appdia_original\": {\n","        \"data_path\": \"./datasets/appdia/original-annotated-data/original-test.tsv\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.75,\n","        \"temperature\": 2.5,\n","    },\n","    \"appdia_discourse\": {\n","        \"data_path\": \"./datasets/appdia/discourse-augmented-data/discourse-test.tsv\",\n","        \"rep_penalty\": 1.0,\n","        \"alpha_a\": 1.5,\n","        \"alpha_e\": 4.75,\n","        \"temperature\": 2.5,\n","    },\n","}\n","\n","print(f\"âœ“ {len(XDETOX_DATASETS)} test datasets configured:\")\n","for name in XDETOX_DATASETS.keys():\n","    print(f\"  - {name}\")"]},{"cell_type":"markdown","metadata":{"id":"f0xGCUVtar5O"},"source":["---\n","## Section 2: Shared Evaluation Metrics\n","\n","This evaluator class combines metrics from both pipelines:\n","- **Style Transfer Accuracy (STA)**: % of outputs classified as non-toxic\n","- **Content Similarity**: Cosine similarity between original and detoxified embeddings\n","- **BLEU**: Precision-based n-gram overlap\n","- **ROUGE-L**: Longest common subsequence\n","- **BERTScore**: Semantic similarity using BERT embeddings\n","- **Perplexity**: Language model fluency (GPT-2)\n","- **Toxicity Score**: Average toxicity of generated text\n","- **J-Score**: STA Ã— Content Similarity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OdXG-DT3ar5P","executionInfo":{"status":"aborted","timestamp":1763963581643,"user_tz":480,"elapsed":92601,"user":{"displayName":"Benjamin Chen He","userId":"13010186992209467638"}}},"outputs":[],"source":["class UnifiedDetoxEvaluator:\n","    \"\"\"\n","    Comprehensive evaluator combining metrics from both T5-ParaDetox and XDetox pipelines.\n","    \"\"\"\n","    def __init__(self):\n","        print(\"Initializing UnifiedDetoxEvaluator...\")\n","\n","        # Toxicity Classifier (for STA)\n","        self.toxicity_tokenizer = AutoTokenizer.from_pretrained(\"s-nlp/roberta_toxicity_classifier\")\n","        self.toxicity_model = AutoModelForSequenceClassification.from_pretrained(\"s-nlp/roberta_toxicity_classifier\")\n","        self.toxicity_model.eval()\n","\n","        # Sentence similarity model\n","        self.sim_model = SentenceTransformer('all-MiniLM-L6-v2')\n","\n","        # Metrics from HuggingFace evaluate\n","        self.bleu = load(\"sacrebleu\")\n","        self.rouge = load(\"rouge\")\n","        self.bertscore = load(\"bertscore\")\n","\n","        # GPT-2 for perplexity (using proper language model)\n","        self.perplexity_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n","        self.perplexity_model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n","        self.perplexity_model.eval()\n","\n","        # Set pad token for GPT-2\n","        if self.perplexity_tokenizer.pad_token is None:\n","            self.perplexity_tokenizer.pad_token = self.perplexity_tokenizer.eos_token\n","\n","        print(\"âœ“ Evaluator initialized\")\n","\n","    def style_transfer_accuracy(self, texts, batch_size=32):\n","        \"\"\"Measure how many texts are classified as non-toxic (STA)\"\"\"\n","        all_predictions = []\n","\n","        for i in range(0, len(texts), batch_size):\n","            batch = texts[i:i+batch_size]\n","            inputs = self.toxicity_tokenizer(\n","                batch,\n","                return_tensors=\"pt\",\n","                padding=True,\n","                truncation=True,\n","                max_length=512\n","            )\n","\n","            with torch.no_grad():\n","                outputs = self.toxicity_model(**inputs)\n","                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n","                # Label 0 = non-toxic, Label 1 = toxic\n","                non_toxic_probs = predictions[:, 0]\n","                all_predictions.extend(non_toxic_probs.tolist())\n","\n","        # STA = percentage classified as non-toxic\n","        non_toxic_count = sum(1 for p in all_predictions if p > 0.5)\n","        return non_toxic_count / len(all_predictions)\n","\n","    def toxicity_scores(self, texts, batch_size=32):\n","        \"\"\"Calculate average toxicity probability\"\"\"\n","        all_toxic_probs = []\n","\n","        for i in range(0, len(texts), batch_size):\n","            batch = texts[i:i+batch_size]\n","            inputs = self.toxicity_tokenizer(\n","                batch,\n","                return_tensors=\"pt\",\n","                padding=True,\n","                truncation=True,\n","                max_length=512\n","            )\n","\n","            with torch.no_grad():\n","                outputs = self.toxicity_model(**inputs)\n","                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n","                # Label 1 = toxic\n","                toxic_probs = predictions[:, 1]\n","                all_toxic_probs.extend(toxic_probs.tolist())\n","\n","        return np.mean(all_toxic_probs)\n","\n","    def content_similarity(self, original_texts, detoxified_texts):\n","        \"\"\"Measure semantic similarity using sentence embeddings\"\"\"\n","        original_embeddings = self.sim_model.encode(original_texts, show_progress_bar=False)\n","        detoxified_embeddings = self.sim_model.encode(detoxified_texts, show_progress_bar=False)\n","\n","        similarities = []\n","        for orig, detox in zip(original_embeddings, detoxified_embeddings):\n","            sim = np.dot(orig, detox) / (np.linalg.norm(orig) * np.linalg.norm(detox))\n","            similarities.append(sim)\n","\n","        return np.mean(similarities)\n","\n","    def calculate_perplexity(self, texts):\n","        \"\"\"Calculate average perplexity using GPT-2 language model\"\"\"\n","        perplexities = []\n","\n","        for text in texts:\n","            inputs = self.perplexity_tokenizer(\n","                text,\n","                return_tensors=\"pt\",\n","                truncation=True,\n","                max_length=512\n","            )\n","\n","            with torch.no_grad():\n","                # Get the loss from the language model\n","                outputs = self.perplexity_model(**inputs, labels=inputs[\"input_ids\"])\n","                loss = outputs.loss\n","                ppl = torch.exp(loss).item()\n","                perplexities.append(ppl)\n","\n","        return np.mean(perplexities) if perplexities else float('nan')\n","\n","    def evaluate_all(self, original_texts, detoxified_texts, reference_texts=None):\n","        \"\"\"\n","        Complete evaluation with all metrics.\n","\n","        Args:\n","            original_texts: List of toxic input texts\n","            detoxified_texts: List of model-generated detoxified texts\n","            reference_texts: Optional list of ground-truth detoxified texts\n","\n","        Returns:\n","            Dictionary of metric scores\n","        \"\"\"\n","        results = {}\n","\n","        # Style Transfer Accuracy\n","        results['STA'] = self.style_transfer_accuracy(detoxified_texts)\n","\n","        # Content Similarity\n","        results['Content_Similarity'] = self.content_similarity(original_texts, detoxified_texts)\n","\n","        # Toxicity scores\n","        results['Toxicity_Gen'] = self.toxicity_scores(detoxified_texts)\n","        results['Toxicity_Orig'] = self.toxicity_scores(original_texts)\n","\n","        # Perplexity (fluency)\n","        results['Perplexity_Gen'] = self.calculate_perplexity(detoxified_texts)\n","\n","        # If we have ground truth references, compute BLEU/ROUGE/BERTScore\n","        if reference_texts:\n","            # BLEU\n","            bleu_result = self.bleu.compute(\n","                predictions=detoxified_texts,\n","                references=[[ref] for ref in reference_texts]\n","            )\n","            results['BLEU'] = bleu_result['score'] / 100.0\n","\n","            # ROUGE\n","            rouge_result = self.rouge.compute(\n","                predictions=detoxified_texts,\n","                references=reference_texts\n","            )\n","            results['ROUGE-L'] = rouge_result['rougeL']\n","\n","            # BERTScore\n","            bert_result = self.bertscore.compute(\n","                predictions=detoxified_texts,\n","                references=reference_texts,\n","                lang=\"en\"\n","            )\n","            results['BERTScore_F1'] = np.mean(bert_result['f1'])\n","        else:\n","            # Use original texts as pseudo-references for BLEU/BERTScore\n","            bleu_result = self.bleu.compute(\n","                predictions=detoxified_texts,\n","                references=[[orig] for orig in original_texts]\n","            )\n","            results['BLEU'] = bleu_result['score'] / 100.0\n","\n","            bert_result = self.bertscore.compute(\n","                predictions=detoxified_texts,\n","                references=original_texts,\n","                lang=\"en\"\n","            )\n","            results['BERTScore_F1'] = np.mean(bert_result['f1'])\n","\n","            results['ROUGE-L'] = None  # Can't compute without references\n","\n","        # J-Score\n","        results['J-Score'] = results['STA'] * results['Content_Similarity']\n","\n","        return results"]},{"cell_type":"markdown","metadata":{"id":"vRYtCHD3ar5P"},"source":["---\n","## Section 3: T5-ParaDetox Pipeline\n","\n","This section implements supervised fine-tuning of T5-base on the ParaDetox dataset."]},{"cell_type":"markdown","metadata":{"id":"tHsDtMrMar5P"},"source":["### 3.1: Load ParaDetox Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tzdUXSARar5Q","executionInfo":{"status":"aborted","timestamp":1763963581646,"user_tz":480,"elapsed":92604,"user":{"displayName":"Benjamin Chen He","userId":"13010186992209467638"}}},"outputs":[],"source":["# Load the ParaDetox dataset\n","paradetox_data_path = os.path.join(PROJECT_BASE_PATH, 'paradetox.tsv')\n","df_paradetox = pd.read_csv(paradetox_data_path, sep='\\t')\n","\n","print(f\"âœ“ Loaded ParaDetox dataset: {len(df_paradetox)} examples\")\n","print(f\"âœ“ Columns: {df_paradetox.columns.tolist()}\")\n","df_paradetox.head(3)"]},{"cell_type":"markdown","metadata":{"id":"r-TBg94dar5Q"},"source":["### 3.2: Create Training Data with Augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4dQvVYQGar5Q","executionInfo":{"status":"aborted","timestamp":1763963581650,"user_tz":480,"elapsed":92607,"user":{"displayName":"Benjamin Chen He","userId":"13010186992209467638"}}},"outputs":[],"source":["def create_augmented_data(df):\n","    \"\"\"\n","    Create training data using ALL available neutral paraphrases.\n","    This augments the dataset by using neutral1, neutral2, neutral3 separately.\n","    \"\"\"\n","    training_data = []\n","    for _, row in df.iterrows():\n","        toxic_text = row['toxic']\n","        # Use ALL available neutral paraphrases\n","        for col in ['neutral1', 'neutral2', 'neutral3']:\n","            if pd.notna(row[col]) and str(row[col]).strip():\n","                training_data.append({\n","                    'input': f\"detoxify: {toxic_text}\",\n","                    'output': str(row[col]).strip()\n","                })\n","    return training_data\n","\n","# Create augmented training data\n","paradetox_training_data = create_augmented_data(df_paradetox)\n","print(f\"âœ“ Total training examples (after augmentation): {len(paradetox_training_data)}\")\n","\n","# Create train/val/test splits (80/10/10)\n","train_data, temp_data = train_test_split(paradetox_training_data, test_size=0.2, random_state=42)\n","val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n","\n","print(f\"âœ“ Train: {len(train_data)} | Val: {len(val_data)} | Test: {len(test_data)}\")"]},{"cell_type":"markdown","metadata":{"id":"1ZCFg7--ar5Q"},"source":["### 3.3: Dataset Class for T5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fZBixIlQar5Q","executionInfo":{"status":"aborted","timestamp":1763963581652,"user_tz":480,"elapsed":92608,"user":{"displayName":"Benjamin Chen He","userId":"13010186992209467638"}}},"outputs":[],"source":["class DetoxDataset(Dataset):\n","    \"\"\"\n","    PyTorch Dataset for T5 detoxification task.\n","    Input format: \"detoxify: {toxic_text}\"\n","    Output: neutral paraphrase\n","    \"\"\"\n","    def __init__(self, data, tokenizer, max_length=128):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        item = self.data[idx]\n","\n","        # Tokenize input\n","        input_encoding = self.tokenizer(\n","            item['input'],\n","            truncation=True,\n","            padding='max_length',\n","            max_length=self.max_length,\n","            return_tensors='pt'\n","        )\n","\n","        # Tokenize target\n","        target_encoding = self.tokenizer(\n","            item['output'],\n","            truncation=True,\n","            padding='max_length',\n","            max_length=self.max_length,\n","            return_tensors='pt'\n","        )\n","\n","        return {\n","            'input_ids': input_encoding['input_ids'].flatten(),\n","            'attention_mask': input_encoding['attention_mask'].flatten(),\n","            'labels': target_encoding['input_ids'].flatten()\n","        }"]},{"cell_type":"markdown","metadata":{"id":"Jggw26wzar5R"},"source":["### 3.4: Initialize T5 Model & Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ArLaTPopar5R","executionInfo":{"status":"aborted","timestamp":1763963581655,"user_tz":480,"elapsed":92611,"user":{"displayName":"Benjamin Chen He","userId":"13010186992209467638"}}},"outputs":[],"source":["# Initialize T5 tokenizer and model\n","model_name = \"t5-base\"\n","t5_tokenizer = T5Tokenizer.from_pretrained(model_name)\n","t5_model = T5ForConditionalGeneration.from_pretrained(model_name)\n","\n","# Create PyTorch datasets\n","train_dataset = DetoxDataset(train_data, t5_tokenizer)\n","val_dataset = DetoxDataset(val_data, t5_tokenizer)\n","test_dataset = DetoxDataset(test_data, t5_tokenizer)\n","\n","print(\"âœ“ T5 model and datasets initialized\")"]},{"cell_type":"markdown","metadata":{"id":"buAqXBciar5R"},"source":["### 3.5: Training Configuration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"unCIBqOiar5R","executionInfo":{"status":"aborted","timestamp":1763963581657,"user_tz":480,"elapsed":92612,"user":{"displayName":"Benjamin Chen He","userId":"13010186992209467638"}}},"outputs":[],"source":["# Training arguments (default config from ProjectNotebook1)\n","# NOTE: Training is optional - you can skip to cell 3.8 to load pre-trained checkpoint\n","\n","training_args = TrainingArguments(\n","    output_dir='./t5-detox-results',\n","    num_train_epochs=3,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n","    logging_steps=100,\n","    eval_strategy=\"steps\",\n","    eval_steps=500,\n","    save_strategy=\"steps\",\n","    save_steps=1000,\n","    load_best_model_at_end=True,\n",")\n","\n","# Initialize Trainer\n","trainer = Trainer(\n","    model=t5_model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    processing_class=t5_tokenizer,\n",")\n","\n","print(\"âœ“ Trainer initialized\")"]},{"cell_type":"markdown","metadata":{"id":"4EPpIwOrar5R"},"source":["### 3.6: Train the Model\n","\n","**Note**: Uncomment to actually train. Training takes significant time!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ppwrobdar5R","executionInfo":{"status":"aborted","timestamp":1763963581658,"user_tz":480,"elapsed":92612,"user":{"displayName":"Benjamin Chen He","userId":"13010186992209467638"}}},"outputs":[],"source":["# Uncomment to train:\n","# print(\"Starting training...\")\n","# trainer.train()\n","# print(\"âœ“ Training complete!\")"]},{"cell_type":"markdown","metadata":{"id":"ypK3obldar5R"},"source":["### 3.7: Save Checkpoint"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9mGwSM5Har5S","executionInfo":{"status":"aborted","timestamp":1763963581661,"user_tz":480,"elapsed":92615,"user":{"displayName":"Benjamin Chen He","userId":"13010186992209467638"}}},"outputs":[],"source":["# Save the fine-tuned model (PLACEHOLDER PATH - update this!)\n","t5_checkpoint_path = os.path.join(PROJECT_BASE_PATH, 't5-base-detox-model')\n","\n","# Uncomment to save after training:\n","# t5_model.save_pretrained(t5_checkpoint_path)\n","# t5_tokenizer.save_pretrained(t5_checkpoint_path)\n","# print(f\"âœ“ Model saved to {t5_checkpoint_path}\")"]},{"cell_type":"markdown","metadata":{"id":"h_FaD0yaar5S"},"source":["### 3.8: Load Checkpoint (for inference)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gTu_Kf3aar5S","executionInfo":{"status":"aborted","timestamp":1763963581695,"user_tz":480,"elapsed":0,"user":{"displayName":"Benjamin Chen He","userId":"13010186992209467638"}}},"outputs":[],"source":["# Load the trained model from checkpoint\n","t5_checkpoint_path = os.path.join(PROJECT_BASE_PATH, 't5-base-detox-model')\n","\n","print(f\"Loading T5 model from {t5_checkpoint_path}...\")\n","t5_tokenizer = T5Tokenizer.from_pretrained(t5_checkpoint_path)\n","t5_model = T5ForConditionalGeneration.from_pretrained(t5_checkpoint_path)\n","t5_model.eval()  # Set to evaluation mode\n","\n","print(\"âœ“ T5 model loaded from checkpoint\")"]},{"cell_type":"markdown","metadata":{"id":"MAQ6EKu7ar5S"},"source":["### 3.9: Inference Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rBsUg34uar5S","executionInfo":{"status":"aborted","timestamp":1763963581699,"user_tz":480,"elapsed":2,"user":{"displayName":"Benjamin Chen He","userId":"13010186992209467638"}}},"outputs":[],"source":["def t5_detoxify_text(text, model, tokenizer, max_length=128):\n","    \"\"\"\n","    Generate detoxified text using T5 model.\n","    \"\"\"\n","    device = next(model.parameters()).device\n","    input_text = f\"detoxify: {text}\"\n","    input_ids = tokenizer.encode(input_text, return_tensors='pt', max_length=max_length, truncation=True)\n","    input_ids = input_ids.to(device)\n","\n","    with torch.no_grad():\n","        outputs = model.generate(\n","            input_ids,\n","            max_length=max_length,\n","            num_beams=5,\n","            early_stopping=True,\n","            no_repeat_ngram_size=2\n","        )\n","\n","    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","# Test inference\n","test_text = \"This is a stupid idea\"\n","detoxified = t5_detoxify_text(test_text, t5_model, t5_tokenizer)\n","print(f\"Original: {test_text}\")\n","print(f\"Detoxified: {detoxified}\")"]},{"cell_type":"markdown","metadata":{"id":"h0iktMWkar5S"},"source":["### 3.10: Helper Function to Load Test Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TFWeHTk4ar5S","executionInfo":{"status":"aborted","timestamp":1763963581700,"user_tz":480,"elapsed":2,"user":{"displayName":"Benjamin Chen He","userId":"13010186992209467638"}}},"outputs":[],"source":["def load_test_dataset(dataset_name, max_examples=None):\n","    \"\"\"\n","    Load test data from various datasets.\n","    Returns (toxic_texts, reference_texts) where reference_texts may be None.\n","    \"\"\"\n","    if dataset_name not in XDETOX_DATASETS:\n","        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n","\n","    config = XDETOX_DATASETS[dataset_name]\n","    data_path = os.path.join(XDETOX_DIR, config[\"data_path\"].lstrip(\"./\"))\n","\n","    toxic_texts = []\n","    reference_texts = []\n","\n","    # Handle different file formats\n","    if data_path.endswith('.txt'):\n","        # Plain text file (no references)\n","        with open(data_path, 'r') as f:\n","            toxic_texts = [line.strip() for line in f if line.strip()]\n","        reference_texts = None\n","\n","    elif data_path.endswith('.csv'):\n","        df = pd.read_csv(data_path)\n","        # Try to find toxic and reference columns\n","        if 'text' in df.columns:\n","            toxic_texts = df['text'].tolist()\n","        elif 'toxic' in df.columns:\n","            toxic_texts = df['toxic'].tolist()\n","        else:\n","            toxic_texts = df.iloc[:, 0].tolist()  # First column\n","\n","        # Look for reference column\n","        if 'neutral' in df.columns:\n","            reference_texts = df['neutral'].tolist()\n","        else:\n","            reference_texts = None\n","\n","    elif data_path.endswith('.tsv'):\n","        df = pd.read_csv(data_path, sep='\\t')\n","        if 'text' in df.columns:\n","            toxic_texts = df['text'].tolist()\n","        else:\n","            toxic_texts = df.iloc[:, 0].tolist()\n","        reference_texts = None\n","\n","    # Limit number of examples if specified\n","    if max_examples:\n","        toxic_texts = toxic_texts[:max_examples]\n","        if reference_texts:\n","            reference_texts = reference_texts[:max_examples]\n","\n","    return toxic_texts, reference_texts\n","\n","print(\"âœ“ Test data loader defined\")"]},{"cell_type":"markdown","metadata":{"id":"2WFkxLJWar5T"},"source":["### 3.11: Evaluate T5-ParaDetox on Multiple Datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qLb6wobGar5T","executionInfo":{"status":"aborted","timestamp":1763963581702,"user_tz":480,"elapsed":92654,"user":{"displayName":"Benjamin Chen He","userId":"13010186992209467638"}}},"outputs":[],"source":["# Initialize evaluator\n","evaluator = UnifiedDetoxEvaluator()\n","\n","# Datasets to evaluate on\n","datasets_to_eval = [\"paradetox\", \"microagressions_test\", \"sbf_test\", \"dynabench_test\"]\n","max_examples_per_dataset = 200  # Adjust based on compute resources\n","\n","# Store results\n","t5_results = []\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"T5-PARADETOX EVALUATION\")\n","print(\"=\"*80)\n","\n","for dataset_name in datasets_to_eval:\n","    print(f\"\\n[{dataset_name}] Loading test data...\")\n","\n","    try:\n","        toxic_texts, reference_texts = load_test_dataset(dataset_name, max_examples=max_examples_per_dataset)\n","        print(f\"  âœ“ Loaded {len(toxic_texts)} examples\")\n","\n","        # Generate detoxified texts\n","        print(f\"  Generating detoxified outputs...\")\n","        detoxified_texts = []\n","        for text in tqdm(toxic_texts, desc=\"  T5 Inference\"):\n","            detox = t5_detoxify_text(text, t5_model, t5_tokenizer)\n","            detoxified_texts.append(detox)\n","\n","        # Evaluate\n","        print(f\"  Computing metrics...\")\n","        metrics = evaluator.evaluate_all(toxic_texts, detoxified_texts, reference_texts)\n","        metrics['dataset'] = dataset_name\n","        metrics['num_examples'] = len(toxic_texts)\n","        t5_results.append(metrics)\n","\n","        print(f\"  âœ“ Done!\")\n","\n","    except Exception as e:\n","        print(f\"  âœ— Error: {e}\")\n","        continue\n","\n","print(\"\\n\" + \"=\"*80)"]},{"cell_type":"markdown","metadata":{"id":"7cRHAJUlar5Y"},"source":["### 3.12: Display T5-ParaDetox Results Table"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n4yunSriar5Y","executionInfo":{"status":"aborted","timestamp":1763963581704,"user_tz":480,"elapsed":92655,"user":{"displayName":"Benjamin Chen He","userId":"13010186992209467638"}}},"outputs":[],"source":["# Create results DataFrame\n","t5_results_df = pd.DataFrame(t5_results)\n","\n","# Reorder columns for better readability\n","column_order = [\n","    'dataset', 'num_examples',\n","    'STA', 'Content_Similarity', 'J-Score',\n","    'BLEU', 'ROUGE-L', 'BERTScore_F1',\n","    'Toxicity_Gen', 'Toxicity_Orig', 'Perplexity_Gen'\n","]\n","t5_results_df = t5_results_df[[col for col in column_order if col in t5_results_df.columns]]\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"T5-PARADETOX RESULTS\")\n","print(\"=\"*80)\n","print(t5_results_df.to_string(index=False))\n","print(\"=\"*80)"]},{"cell_type":"markdown","metadata":{"id":"R08j2avIar5Y"},"source":["---\n","## Section 4: XDetox Pipeline (DecompX)\n","\n","This section runs the XDetox product-of-experts approach with masking and reranking."]},{"cell_type":"markdown","metadata":{"id":"C9Wi_Jzmar5Y"},"source":["### 4.1: XDetox Helper Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HA1JS6wHar5Z","executionInfo":{"status":"aborted","timestamp":1763963581709,"user_tz":480,"elapsed":3,"user":{"displayName":"Benjamin Chen He","userId":"13010186992209467638"}}},"outputs":[],"source":["# Helper functions from XDetox_Pipeline.ipynb\n","\n","def _abs_repo_path(rel):\n","    return os.path.join(XDETOX_DIR, rel.lstrip(\"./\"))\n","\n","def _ensure_dir(p):\n","    Path(p).mkdir(parents=True, exist_ok=True)\n","\n","def _subset_for_data_type(data_type, data_path, n, out_dir):\n","    \"\"\"Create a small subset file matching the expected format.\"\"\"\n","    if n is None or n <= 0:\n","        return data_path\n","\n","    src = _abs_repo_path(data_path)\n","    _ensure_dir(out_dir)\n","\n","    if \"microagressions\" in data_path:\n","        df = pd.read_csv(src)\n","        sub = df.head(n)\n","        out = os.path.join(out_dir, os.path.basename(src))\n","        sub.to_csv(out, index=False)\n","        return out\n","\n","    if \"sbf\" in data_path:\n","        df = pd.read_csv(src)\n","        sub = df.head(n)\n","        out = os.path.join(out_dir, os.path.basename(src))\n","        sub.to_csv(out, index=False)\n","        return out\n","\n","    if \"dynabench\" in data_path:\n","        df = pd.read_csv(src)\n","        sub = df.head(n)\n","        out = os.path.join(out_dir, os.path.basename(src))\n","        sub.to_csv(out, index=False)\n","        return out\n","\n","    if any(k in data_path for k in [\"paradetox\", \"jigsaw\"]):\n","        if data_path.endswith(\".txt\"):\n","            with open(src, \"r\") as f:\n","                lines = [s.rstrip(\"\\n\") for s in f.readlines()]\n","            out = os.path.join(out_dir, os.path.basename(src))\n","            with open(out, \"w\") as g:\n","                for s in lines[:n]:\n","                    g.write(s + \"\\n\")\n","            return out\n","        elif data_path.endswith(\".csv\"):\n","            df = pd.read_csv(src).head(n)\n","            out = os.path.join(out_dir, os.path.basename(src))\n","            df.to_csv(out, index=False)\n","            return out\n","\n","    if \"appdia\" in data_path:\n","        df = pd.read_csv(src, sep=\"\\t\").head(n)\n","        out = os.path.join(out_dir, os.path.basename(src))\n","        df.to_csv(out, sep=\"\\t\", index=False)\n","        return out\n","\n","    # Fallback\n","    out = os.path.join(out_dir, os.path.basename(src))\n","    shutil.copy(src, out)\n","    return out\n","\n","def _parse_run_folder_name(folder_name):\n","    pattern = r\"aa(\\d+\\.\\d+)_ae(\\d+\\.\\d+)_ab(\\d+\\.\\d+)_base(.*)_anti(.*)_expert(.*)_temp(\\d+\\.\\d+)_sample(.*)_topk(\\d+)_reppenalty(\\d+\\.\\d+)_filterp(\\d+\\.\\d+)_maxlength(\\d+)_topp(\\d+\\.\\d+)\"\n","    m = re.match(pattern, folder_name)\n","    return bool(m)\n","\n","def _eval_with_toxicity(base_path, overwrite_eval=False, skip_ref=False, tox_threshold=0.5, tox_batch_size=32):\n","    \"\"\"Run evaluation using XDetox's evaluation module.\"\"\"\n","    for folder in os.listdir(base_path):\n","        gen_dir = os.path.join(base_path, folder)\n","        if not os.path.isdir(gen_dir) or not _parse_run_folder_name(folder):\n","            continue\n","        orig_path = os.path.join(gen_dir, \"orig.txt\")\n","        gen_path  = os.path.join(gen_dir, \"gen.txt\")\n","        out_stats = os.path.join(gen_dir, \"gen_stats.txt\")\n","        if not (os.path.exists(orig_path) and os.path.exists(gen_path)):\n","            continue\n","        if os.path.exists(out_stats) and not overwrite_eval:\n","            continue\n","\n","        env = os.environ.copy()\n","        env[\"PYTHONPATH\"] = XDETOX_DIR + (\":\" + env.get(\"PYTHONPATH\",\"\") if env.get(\"PYTHONPATH\") else \"\")\n","        cmd = [\n","            sys.executable, \"-m\", \"evaluation.evaluate_all\",\n","            \"--orig_path\", orig_path,\n","            \"--gen_path\",  gen_path,\n","            \"--tox_threshold\", str(tox_threshold),\n","            \"--tox_batch_size\", str(tox_batch_size),\n","        ]\n","        if skip_ref:\n","            cmd.append(\"--skip_ref\")\n","        print(\"Eval:\", \" \".join(cmd))\n","        res = run(cmd, cwd=XDETOX_DIR, env=env, stdout=PIPE, stderr=PIPE, text=True)\n","        if res.returncode != 0:\n","            print(res.stdout)\n","            print(res.stderr)\n","            res.check_returncode()\n","\n","def _safe_float(x):\n","    try:\n","        return float(x)\n","    except Exception:\n","        return float('nan')\n","\n","def _read_stats_file(path):\n","    \"\"\"Read gen_stats.txt into a dict of floats.\"\"\"\n","    out = {}\n","    with open(path, \"r\") as f:\n","        for line in f:\n","            if \":\" not in line:\n","                continue\n","            k, v = line.strip().split(\": \", 1)\n","            k = k.replace(\"(skipped)\", \"\").strip().lower()\n","            out[k] = _safe_float(v)\n","    return out\n","\n","def _aggregate_eval_csv(output_folder, data_type, base_out_dir):\n","    \"\"\"Aggregate evaluation results across thresholds.\"\"\"\n","    rows = []\n","    for thresh in np.arange(0.15, 0.3, 0.05, dtype=np.float64):\n","        mask_dir = f\"DecompX{abs(thresh):g}\" if thresh != 0 else \"DecompX0.0\"\n","        base_path = os.path.join(base_out_dir, data_type, mask_dir)\n","        if not os.path.isdir(base_path):\n","            continue\n","        for folder in os.listdir(base_path):\n","            gen_dir = os.path.join(base_path, folder)\n","            stats_path = os.path.join(gen_dir, \"gen_stats.txt\")\n","            if not os.path.exists(stats_path):\n","                continue\n","            s = _read_stats_file(stats_path)\n","            rows.append({\n","                \"threshold\": float(f\"{thresh:.2f}\"),\n","                \"folder\": folder,\n","                \"bertscore\":        s.get(\"bertscore\", np.nan),\n","                \"bleu4\":            s.get(\"bleu4\", np.nan),\n","                \"perplexity_gen\":   s.get(\"perplexity gen\", np.nan),\n","                \"perplexity_orig\":  s.get(\"perplexity orig\", np.nan),\n","                \"toxicity_gen\":     s.get(\"toxicity gen\", np.nan),\n","                \"toxicity_orig\":    s.get(\"toxicity orig\", np.nan),\n","            })\n","\n","    if rows:\n","        cols = [\n","            \"threshold\", \"folder\",\n","            \"bertscore\", \"bleu4\",\n","            \"perplexity_gen\", \"perplexity_orig\",\n","            \"toxicity_gen\", \"toxicity_orig\",\n","        ]\n","        df = pd.DataFrame(rows)\n","        df = df[cols]\n","        out_csv = os.path.join(base_out_dir, data_type, f\"{data_type}.csv\")\n","        df.to_csv(out_csv, index=False)\n","        print(\"Wrote summary CSV:\", out_csv)\n","        return df\n","    else:\n","        print(\"No evaluation files found to summarize.\")\n","        return None\n","\n","print(\"âœ“ XDetox helper functions loaded\")"]},{"cell_type":"markdown","metadata":{"id":"vZ731Nh2ar5Z"},"source":["### 4.2: XDetox `detoxify()` Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K3chWoacar5Z","executionInfo":{"status":"aborted","timestamp":1763963581711,"user_tz":480,"elapsed":4,"user":{"displayName":"Benjamin Chen He","userId":"13010186992209467638"}}},"outputs":[],"source":["def xdetox_detoxify(\n","    data_type: str = \"paradetox\",\n","    output_folder: str = \"merged_notebook_run\",\n","    thresholds = (0.20,),  # Single threshold for simplicity\n","    batch_size: int = 10,\n","    ranking: bool = True,\n","    sample: bool = True,\n","    top_k_gen: int = 50,\n","    top_p: float = 0.95,\n","    filter_p: float = 1.0,\n","    max_length: int = 128,\n","    alpha_a: float = None,\n","    alpha_e: float = None,\n","    alpha_b: float = 1.0,\n","    temperature: float = None,\n","    rep_penalty: float = None,\n","    num_examples: int = 200,\n","    overwrite_gen: bool = False,\n","    run_eval: bool = True,\n","    overwrite_eval: bool = False,\n","    skip_ref_eval: bool = False,\n","):\n","    \"\"\"\n","    Run the XDetox pipeline (DecompX with product-of-experts).\n","    \"\"\"\n","    assert data_type in XDETOX_DATASETS, f\"Unknown data_type: {data_type}\"\n","    cfg = XDETOX_DATASETS[data_type].copy()\n","\n","    # Fallbacks from config\n","    if alpha_a is None: alpha_a = cfg[\"alpha_a\"]\n","    if alpha_e is None: alpha_e = cfg[\"alpha_e\"]\n","    if temperature is None: temperature = cfg[\"temperature\"]\n","    if rep_penalty is None: rep_penalty = cfg[\"rep_penalty\"]\n","\n","    base_out_dir = os.path.join(\"data\", \"dexp_outputs\", output_folder)\n","    abs_base_out_dir = os.path.join(XDETOX_DIR, base_out_dir)\n","    _ensure_dir(abs_base_out_dir)\n","\n","    # Create subset\n","    original_data_path = cfg[\"data_path\"]\n","    subset_dir = os.path.join(XDETOX_DIR, \"datasets\", \"_subsets\", data_type)\n","    _ensure_dir(subset_dir)\n","    subset_path = _subset_for_data_type(data_type, original_data_path, num_examples, subset_dir)\n","\n","    # Run thresholds\n","    for t in thresholds:\n","        mask_dir = f\"DecompX{abs(t):g}\" if t != 0 else \"DecompX0.0\"\n","        mix_out_dir = os.path.join(abs_base_out_dir, data_type, mask_dir)\n","        _ensure_dir(mix_out_dir)\n","\n","        # Build command\n","        cmd = [\n","            sys.executable, \"-m\", \"rewrite.rewrite_example\",\n","            \"--output_dir\", base_out_dir,\n","            \"--data_type\", data_type,\n","            \"--data_path\", subset_path.replace(XDETOX_DIR + \"/\", \"./\"),\n","            \"--rep_penalty\", str(rep_penalty),\n","            \"--alpha_a\", str(alpha_a),\n","            \"--alpha_e\", str(alpha_e),\n","            \"--temperature\", str(temperature),\n","            \"--alpha_b\", str(alpha_b),\n","            \"--max_length\", str(max_length),\n","            \"--batch_size\", str(batch_size),\n","            \"--top_k_gen\", str(top_k_gen),\n","            \"--top_p\", str(top_p),\n","            \"--filter_p\", str(filter_p),\n","            \"--thresh\", f\"{t:.2f}\",\n","        ]\n","        if ranking:\n","            cmd.append(\"--ranking\")\n","        if sample:\n","            cmd.append(\"--sample\")\n","        if overwrite_gen:\n","            cmd.append(\"--overwrite_gen\")\n","\n","        print(\"Run:\", \" \".join(cmd))\n","\n","        # Run with better error handling\n","        result = run(cmd, cwd=XDETOX_DIR, capture_output=True, text=True)\n","        if result.returncode != 0:\n","            print(f\"\\nâŒ XDetox command failed with exit code {result.returncode}\")\n","            print(f\"\\nðŸ“‹ STDOUT:\")\n","            print(result.stdout)\n","            print(f\"\\nðŸ“‹ STDERR:\")\n","            print(result.stderr)\n","            raise RuntimeError(f\"XDetox pipeline failed for {data_type}\")\n","\n","        # Optional evaluation\n","        if run_eval:\n","            base_path = os.path.join(abs_base_out_dir, data_type, mask_dir)\n","            _eval_with_toxicity(\n","                base_path,\n","                overwrite_eval=overwrite_eval,\n","                skip_ref=skip_ref_eval,\n","                tox_threshold=0.5,\n","                tox_batch_size=32\n","            )\n","\n","    # Summarize\n","    if run_eval:\n","        return _aggregate_eval_csv(output_folder, data_type, os.path.join(XDETOX_DIR, \"data\", \"dexp_outputs\", output_folder))\n","\n","    return None\n","\n","print(\"âœ“ XDetox detoxify function defined\")"]},{"cell_type":"markdown","metadata":{"id":"bzC2ofh_ar5a"},"source":["### 4.3: Run XDetox on Multiple Datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bAl6yOnKar5a","executionInfo":{"status":"aborted","timestamp":1763963581712,"user_tz":480,"elapsed":4,"user":{"displayName":"Benjamin Chen He","userId":"13010186992209467638"}}},"outputs":[],"source":["# Datasets to evaluate\n","xdetox_datasets_to_eval = [\"paradetox\", \"microagressions_test\", \"sbf_test\", \"dynabench_test\"]\n","xdetox_num_examples = 200\n","\n","# Store aggregated results\n","xdetox_results = {}\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"XDETOX (DecompX) EVALUATION\")\n","print(\"=\"*80)\n","\n","for dataset_name in xdetox_datasets_to_eval:\n","    print(f\"\\n[{dataset_name}] Running XDetox pipeline...\")\n","\n","    try:\n","        result_df = xdetox_detoxify(\n","            data_type=dataset_name,\n","            output_folder=\"merged_notebook_run\",\n","            thresholds=(0.20,),  # Single threshold\n","            batch_size=8,\n","            ranking=True,\n","            sample=True,\n","            num_examples=xdetox_num_examples,\n","            run_eval=True,\n","            overwrite_eval=False,\n","            skip_ref_eval=False\n","        )\n","\n","        if result_df is not None:\n","            xdetox_results[dataset_name] = result_df\n","            print(f\"  âœ“ {dataset_name} complete!\")\n","        else:\n","            print(f\"  âœ— No results generated for {dataset_name}\")\n","\n","    except Exception as e:\n","        print(f\"  âœ— Error: {e}\")\n","        continue\n","\n","print(\"\\n\" + \"=\"*80)"]},{"cell_type":"markdown","metadata":{"id":"R90vw8liar5a"},"source":["### 4.4: Display XDetox Results Table"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TAUZA9Yuar5a","executionInfo":{"status":"aborted","timestamp":1763963581714,"user_tz":480,"elapsed":6,"user":{"displayName":"Benjamin Chen He","userId":"13010186992209467638"}}},"outputs":[],"source":["print(\"\\n\" + \"=\"*80)\n","print(\"XDETOX (DecompX) RESULTS\")\n","print(\"=\"*80)\n","\n","for dataset_name, df in xdetox_results.items():\n","    print(f\"\\n[{dataset_name}]\")\n","    print(df.to_string(index=False))\n","    print(\"-\" * 80)\n","\n","print(\"=\"*80)"]},{"cell_type":"markdown","metadata":{"id":"52mZ_QFRar5a"},"source":["---\n","## Section 5: Summary & Comparison\n","\n","Compare the two approaches side-by-side."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hCM_EHMKar5a","executionInfo":{"status":"aborted","timestamp":1763963581715,"user_tz":480,"elapsed":92663,"user":{"displayName":"Benjamin Chen He","userId":"13010186992209467638"}}},"outputs":[],"source":["print(\"\\n\" + \"=\"*80)\n","print(\"FINAL COMPARISON: T5-ParaDetox vs XDetox\")\n","print(\"=\"*80)\n","\n","print(\"\\n--- T5-PARADETOX RESULTS ---\")\n","if len(t5_results) > 0:\n","    print(t5_results_df.to_string(index=False))\n","else:\n","    print(\"No T5 results available.\")\n","\n","print(\"\\n--- XDETOX RESULTS ---\")\n","if len(xdetox_results) > 0:\n","    for dataset_name, df in xdetox_results.items():\n","        print(f\"\\n[{dataset_name}]\")\n","        print(df.to_string(index=False))\n","else:\n","    print(\"No XDetox results available.\")\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"âœ“ Evaluation complete!\")\n","print(\"=\"*80)"]},{"cell_type":"markdown","metadata":{"id":"W31Vak2Rar5b"},"source":["---\n","## Notes\n","\n","### Key Differences Between Approaches:\n","\n","**T5-ParaDetox:**\n","- Single fine-tuned T5-base model\n","- Supervised training on parallel toxic/neutral pairs\n","- Simple inference (beam search)\n","- Fast and straightforward\n","\n","**XDetox (DecompX):**\n","- Product-of-Experts with 3 models (base, expert, anti-expert)\n","- Masking-based approach with threshold tuning\n","- Sampling + reranking for diversity\n","- More complex but potentially more controllable\n","\n","### Evaluation Metrics:\n","- **STA**: Higher = more non-toxic outputs\n","- **Content Similarity**: Higher = better preservation of meaning\n","- **J-Score**: STA Ã— Similarity (balance between detoxification and preservation)\n","- **BLEU/ROUGE/BERTScore**: Measure against references (when available)\n","- **Perplexity**: Lower = more fluent text\n","- **Toxicity**: Lower = better detoxification"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}
{"cells":[{"cell_type":"markdown","metadata":{"id":"X1rhtOAceoHn"},"source":["# T5-ParaDetox Pipeline with DecompX Reranking\n","\n","This notebook combines:\n","- **T5-base** fine-tuned on ParaDetox for detoxification\n","- **DecompX reranking** to select the least toxic candidate from multiple generations\n","\n","## Pipeline\n","\n","1. Generate `num_candidates` detoxified texts per input using T5 sampling\n","2. Score each candidate using DecompX toxicity attribution (RoBERTa-based)\n","3. Select candidate with lowest toxicity score\n","4. Evaluate with BLEU, BERTScore, MeaningBERT, Perplexity, Toxicity\n","\n","---\n","\n","## `detoxify()` API\n","\n","```python\n","def detoxify(\n","    data_type: str = \"paradetox\",\n","    output_folder: str = \"T5_w_DecompX-Reranking\",\n","    batch_size: int = 8,\n","    max_length: int = 128,\n","    num_examples: int = 100,\n","    num_candidates: int = 10,\n","    temperature: float = 1.0,\n","    top_k: int = 50,\n","    top_p: float = 0.95,\n","    overwrite_gen: bool = False,\n","    run_eval: bool = True,\n","    overwrite_eval: bool = False,\n","    echo: bool = False,\n",")\n","```\n","\n","### Key Arguments\n","\n","- `data_type`: Dataset key (paradetox, microagressions_test, sbf_test, dynabench_test, jigsaw_toxic, appdia_original, appdia_discourse)\n","- `output_folder`: Folder under `data/model_outputs/` for results\n","- `num_candidates`: Number of candidates to generate per input for reranking\n","- `temperature`: Sampling temperature for diversity (higher = more diverse)\n","- `echo`: If True, print example inputs, candidates, and outputs"]},{"cell_type":"markdown","metadata":{"id":"bYc9JrJaeoHp"},"source":["## Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":263},"id":"4W_N0aereoHp","executionInfo":{"status":"error","timestamp":1764832544613,"user_tz":480,"elapsed":28102,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"f135dd8e-1a9a-4860-8039-966276587086"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","PROJECT_BASE: /content/drive/MyDrive/ds266/w266 - Project\n","XDETOX_DIR: /content/drive/MyDrive/ds266/w266 - Project/XDetox\n","T5_CHECKPOINT: /content/drive/MyDrive/ds266/w266 - Project/t5-base-detox-model\n"]},{"output_type":"error","ename":"AssertionError","evalue":"PROJECT_BASE does not exist: /content/drive/MyDrive/ds266/w266 - Project","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2119870918.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"T5_CHECKPOINT:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT5_CHECKPOINT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPROJECT_BASE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"PROJECT_BASE does not exist: {PROJECT_BASE}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXDETOX_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"XDETOX_DIR does not exist: {XDETOX_DIR}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: PROJECT_BASE does not exist: /content/drive/MyDrive/ds266/w266 - Project"]}],"source":["#@title Mount Drive & locate project\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, sys, torch\n","\n","# Set your project base path\n","PROJECT_BASE = \"/content/drive/MyDrive/ds266/w266 - Project\"\n","XDETOX_DIR = os.path.join(PROJECT_BASE, \"XDetox\")\n","T5_CHECKPOINT = os.path.join(PROJECT_BASE, \"t5-base-detox-model\")\n","DATASET_BASE = XDETOX_DIR\n","\n","# Add XDetox to path for DecompX imports\n","if XDETOX_DIR not in sys.path:\n","    sys.path.append(XDETOX_DIR)\n","\n","print(\"PROJECT_BASE:\", PROJECT_BASE)\n","print(\"XDETOX_DIR:\", XDETOX_DIR)\n","print(\"T5_CHECKPOINT:\", T5_CHECKPOINT)\n","\n","assert os.path.isdir(PROJECT_BASE), f\"PROJECT_BASE does not exist: {PROJECT_BASE}\"\n","assert os.path.isdir(XDETOX_DIR), f\"XDETOX_DIR does not exist: {XDETOX_DIR}\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oGzBZlJ3eoHq","executionInfo":{"status":"ok","timestamp":1764820200542,"user_tz":480,"elapsed":32,"user":{"displayName":"Benjamin Chen He","userId":"13010186992209467638"}},"outputId":"105d75e2-dfe6-4842-c629-eb3a3cb8d3e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["TRANSFORMERS_CACHE: /content/drive/MyDrive/ds266/w266 - Project/cache\n","CUDA available: True\n","GPU: Tesla T4\n"]}],"source":["#@title Runtime setup (cache, GPU)\n","HF_CACHE = os.path.join(PROJECT_BASE, \"cache\")\n","os.environ[\"TRANSFORMERS_CACHE\"] = HF_CACHE\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\n","print(\"TRANSFORMERS_CACHE:\", HF_CACHE)\n","print(\"CUDA available:\", torch.cuda.is_available())\n","if torch.cuda.is_available():\n","    print(\"GPU:\", torch.cuda.get_device_name(0))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QUMJhQb_eoHq","executionInfo":{"status":"ok","timestamp":1764820233211,"user_tz":480,"elapsed":32666,"user":{"displayName":"Benjamin Chen He","userId":"13010186992209467638"}},"outputId":"4dc6b8b6-6989-4875-df05-628842b1c9d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["#@title Install dependencies\n","!pip install -q transformers torch datasets\n","!pip install -q evaluate sacrebleu bert-score\n","!pip install -q sentence-transformers accelerate -U\n","!pip install -q rouge_score pandas numpy scikit-learn matplotlib nltk"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NGDUUoabeoHr"},"outputs":[],"source":["#@title NLTK data\n","import nltk\n","nltk.download(\"punkt\", quiet=True)\n","try:\n","    nltk.download(\"punkt_tab\", quiet=True)\n","except Exception:\n","    pass\n","print(\"NLTK ready\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"784CUqYSeoHr"},"outputs":[],"source":["#@title Import libraries\n","import glob, re, json, shutil, math\n","import numpy as np\n","import pandas as pd\n","from tqdm.auto import tqdm\n","from pathlib import Path\n","from typing import List\n","\n","from transformers import (\n","    T5Tokenizer, T5ForConditionalGeneration,\n","    AutoTokenizer, AutoModelForSequenceClassification,\n","    GPT2Tokenizer, GPT2LMHeadModel\n",")\n","from sentence_transformers import SentenceTransformer\n","from evaluate import load\n","\n","# DecompX compatibility fix: apply_chunking_to_forward moved from\n","# transformers.modeling_utils to transformers.pytorch_utils in newer versions\n","import transformers.modeling_utils as modeling_utils\n","try:\n","    from transformers.modeling_utils import apply_chunking_to_forward\n","except ImportError:\n","    from transformers.pytorch_utils import apply_chunking_to_forward\n","    modeling_utils.apply_chunking_to_forward = apply_chunking_to_forward\n","\n","# DecompX imports (for reranking)\n","from rewrite.mask_orig import Masker\n","\n","print(\"Libraries imported\")"]},{"cell_type":"markdown","metadata":{"id":"mJ6WLuymeoHr"},"source":["## Dataset Configuration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I6TEERFXeoHr"},"outputs":[],"source":["#@title Data configs (matching XDetox datasets)\n","data_configs = {\n","    \"paradetox\": {\n","        \"data_path\": \"./datasets/paradetox/test_toxic_parallel.txt\",\n","        \"format\": \"txt\",\n","    },\n","    \"microagressions_test\": {\n","        \"data_path\": \"./datasets/microagressions/test.csv\",\n","        \"format\": \"csv\",\n","    },\n","    \"sbf_test\": {\n","        \"data_path\": \"./datasets/sbf/sbftst.csv\",\n","        \"format\": \"csv\",\n","    },\n","    \"dynabench_test\": {\n","        \"data_path\": \"./datasets/dynabench/db_test.csv\",\n","        \"format\": \"csv\",\n","    },\n","    \"jigsaw_toxic\": {\n","        \"data_path\": \"./datasets/jigsaw_full_30/test_10k_toxic.txt\",\n","        \"format\": \"txt\",\n","    },\n","    \"appdia_original\": {\n","        \"data_path\": \"./datasets/appdia/original-annotated-data/original-test.tsv\",\n","        \"format\": \"tsv\",\n","    },\n","    \"appdia_discourse\": {\n","        \"data_path\": \"./datasets/appdia/discourse-augmented-data/discourse-test.tsv\",\n","        \"format\": \"tsv\",\n","    },\n","}\n","\n","print(f\"{len(data_configs)} datasets configured:\")\n","for name in data_configs.keys():\n","    print(f\"  - {name}\")"]},{"cell_type":"markdown","metadata":{"id":"fExrd6_JeoHs"},"source":["## Helper Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qNg4xbyHeoHs"},"outputs":[],"source":["#@title Helper functions\n","\n","def _ensure_dir(p):\n","    Path(p).mkdir(parents=True, exist_ok=True)\n","\n","def load_test_data(data_type, num_examples=None):\n","    \"\"\"\n","    Load test data from various formats (.txt, .csv, .tsv).\n","    Returns list of toxic texts as strings.\n","    \"\"\"\n","    if data_type not in data_configs:\n","        raise ValueError(f\"Unknown data_type: {data_type}\")\n","\n","    cfg = data_configs[data_type]\n","    data_path = os.path.join(DATASET_BASE, cfg[\"data_path\"].lstrip(\"./\"))\n","\n","    texts = []\n","\n","    if cfg[\"format\"] == \"txt\":\n","        with open(data_path, 'r', encoding='utf-8') as f:\n","            texts = [line.strip() for line in f if line.strip()]\n","\n","    elif cfg[\"format\"] == \"csv\":\n","        df = pd.read_csv(data_path)\n","        if 'text' in df.columns:\n","            texts = df['text'].tolist()\n","        elif 'toxic' in df.columns:\n","            texts = df['toxic'].tolist()\n","        else:\n","            texts = df.iloc[:, 0].tolist()\n","\n","    elif cfg[\"format\"] == \"tsv\":\n","        df = pd.read_csv(data_path, sep='\\t')\n","        if 'text' in df.columns:\n","            texts = df['text'].tolist()\n","        else:\n","            texts = df.iloc[:, 0].tolist()\n","\n","    # Clean and convert to strings\n","    cleaned_texts = []\n","    for text in texts:\n","        if pd.isna(text):\n","            continue\n","        text_str = str(text).strip()\n","        if text_str:\n","            cleaned_texts.append(text_str)\n","\n","    if num_examples and num_examples > 0:\n","        cleaned_texts = cleaned_texts[:num_examples]\n","\n","    return cleaned_texts\n","\n","def _safe_float(x):\n","    try:\n","        return float(x)\n","    except Exception:\n","        return float('nan')\n","\n","def _read_stats_file(path):\n","    \"\"\"Read gen_stats.txt into dict.\"\"\"\n","    out = {}\n","    with open(path, \"r\") as f:\n","        for line in f:\n","            if \":\" not in line:\n","                continue\n","            k, v = line.strip().split(\": \", 1)\n","            k = k.replace(\"(skipped)\", \"\").strip().lower()\n","            out[k] = _safe_float(v)\n","    return out\n","\n","print(\"Helper functions loaded\")"]},{"cell_type":"markdown","metadata":{"id":"1alT9KsbeoHs"},"source":["## T5 Model Loading"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t1drYIvieoHs"},"outputs":[],"source":["#@title Load T5 model\n","print(f\"Loading T5 model from {T5_CHECKPOINT}...\")\n","\n","t5_tokenizer = T5Tokenizer.from_pretrained(T5_CHECKPOINT)\n","t5_model = T5ForConditionalGeneration.from_pretrained(T5_CHECKPOINT)\n","t5_model.eval()\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","t5_model = t5_model.to(device)\n","\n","print(f\"T5 model loaded on {device}\")"]},{"cell_type":"markdown","metadata":{"id":"HhWSNC5jeoHs"},"source":["## T5 Multi-Candidate Generation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8LtjFBnaeoHs"},"outputs":[],"source":["#@title T5 multi-candidate generation functions\n","\n","def t5_generate_candidates(text, model, tokenizer, num_candidates,\n","                           temperature=1.0, top_k=50, top_p=0.95,\n","                           max_length=128, device=\"cuda\"):\n","    \"\"\"\n","    Generate num_candidates different outputs via sampling.\n","    \"\"\"\n","    input_text = f\"detoxify: {text}\"\n","    input_ids = tokenizer.encode(input_text, return_tensors='pt',\n","                                  max_length=max_length, truncation=True)\n","    input_ids = input_ids.to(device)\n","\n","    with torch.no_grad():\n","        outputs = model.generate(\n","            input_ids,\n","            max_length=max_length,\n","            num_return_sequences=num_candidates,\n","            do_sample=True,\n","            temperature=temperature,\n","            top_k=top_k,\n","            top_p=top_p,\n","            no_repeat_ngram_size=2\n","        )\n","\n","    candidates = [tokenizer.decode(out, skip_special_tokens=True) for out in outputs]\n","    return candidates\n","\n","def t5_generate_candidates_batch(texts, model, tokenizer, num_candidates,\n","                                  temperature=1.0, top_k=50, top_p=0.95,\n","                                  max_length=128, device=\"cuda\"):\n","    \"\"\"\n","    Batch generation of candidates for multiple inputs.\n","    \"\"\"\n","    all_candidates = []\n","    for text in tqdm(texts, desc=\"T5 Generation\"):\n","        candidates = t5_generate_candidates(\n","            text, model, tokenizer, num_candidates,\n","            temperature, top_k, top_p, max_length, device\n","        )\n","        all_candidates.append(candidates)\n","    return all_candidates\n","\n","# Test\n","test_text = \"This is a stupid idea\"\n","candidates = t5_generate_candidates(test_text, t5_model, t5_tokenizer,\n","                                     num_candidates=3, device=device)\n","print(f\"Input: {test_text}\")\n","print(f\"Candidates:\")\n","for i, c in enumerate(candidates):\n","    print(f\"  [{i}]: {c}\")"]},{"cell_type":"markdown","metadata":{"id":"7EBMYfvEeoHs"},"source":["## DecompX Reranking"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NvlGUHVZeoHs"},"outputs":[],"source":["#@title DecompX reranking functions\n","\n","# Global masker instance (lazy loaded)\n","_decompx_masker = None\n","\n","def _get_decompx_masker():\n","    global _decompx_masker\n","    if _decompx_masker is None:\n","        _decompx_masker = Masker()\n","    return _decompx_masker\n","\n","def decompx_toxicity_score(texts: List[str]) -> List[float]:\n","    \"\"\"\n","    Get toxicity scores using DecompX attribution.\n","    Returns sum of positive toxicity attributions for each text.\n","    Lower score = less toxic.\n","    \"\"\"\n","    masker = _get_decompx_masker()\n","    scores = []\n","\n","    for text in texts:\n","        # Process text with high threshold (no masking, just get attribution)\n","        result = masker.process_text(sentence=[text], threshold=1.0)\n","        # Get the toxicity attribution sum\n","        # Higher positive attribution = more toxic\n","        attr = masker.get_last_attributions()\n","        if attr is not None and len(attr) > 0:\n","            # Sum positive attributions\n","            pos_attr = sum(a for a in attr[0] if a > 0)\n","            scores.append(pos_attr)\n","        else:\n","            scores.append(0.0)\n","\n","    return scores\n","\n","def rerank_with_decompx(sources: List[str], candidates: List[List[str]]) -> List[str]:\n","    \"\"\"\n","    Rerank candidates using DecompX toxicity scores.\n","    Select candidate with lowest toxicity for each source.\n","\n","    Args:\n","        sources: List of original toxic texts (N)\n","        candidates: List of candidate lists (N x C)\n","\n","    Returns:\n","        List of best candidates (N)\n","    \"\"\"\n","    N = len(sources)\n","    assert len(candidates) == N\n","\n","    best_outputs = []\n","\n","    for i, cand_list in enumerate(tqdm(candidates, desc=\"DecompX Reranking\")):\n","        if len(cand_list) == 1:\n","            best_outputs.append(cand_list[0])\n","            continue\n","\n","        # Score all candidates for this input\n","        scores = decompx_toxicity_score(cand_list)\n","\n","        # Select candidate with lowest toxicity score\n","        best_idx = np.argmin(scores)\n","        best_outputs.append(cand_list[best_idx])\n","\n","    return best_outputs\n","\n","def release_decompx():\n","    \"\"\"Release DecompX model to free GPU memory.\"\"\"\n","    global _decompx_masker\n","    if _decompx_masker is not None:\n","        _decompx_masker.release_model()\n","        _decompx_masker = None\n","    if torch.cuda.is_available():\n","        torch.cuda.empty_cache()\n","\n","print(\"DecompX reranking functions loaded\")"]},{"cell_type":"markdown","metadata":{"id":"1ntgpKgpeoHt"},"source":["## Evaluation Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cvnm89jUeoHt"},"outputs":[],"source":["#@title Load evaluation models\n","print(\"Loading evaluation models...\")\n","\n","# Toxicity classifier\n","tox_tokenizer = AutoTokenizer.from_pretrained(\"s-nlp/roberta_toxicity_classifier\")\n","tox_model = AutoModelForSequenceClassification.from_pretrained(\"s-nlp/roberta_toxicity_classifier\")\n","tox_model.eval()\n","tox_model = tox_model.to(device)\n","\n","# Perplexity model (GPT-2)\n","ppl_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n","ppl_model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n","ppl_model.eval()\n","ppl_model = ppl_model.to(device)\n","if ppl_tokenizer.pad_token is None:\n","    ppl_tokenizer.pad_token = ppl_tokenizer.eos_token\n","\n","# Sentence embeddings for MeaningBERT\n","sim_model = SentenceTransformer('all-MiniLM-L6-v2')\n","\n","# Metrics\n","bleu_metric = load(\"sacrebleu\")\n","bertscore_metric = load(\"bertscore\")\n","\n","print(\"Evaluation models loaded\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NIjzb143eoHt"},"outputs":[],"source":["#@title Evaluation functions\n","\n","def compute_toxicity(texts, tokenizer, model, device=\"cuda\", batch_size=32):\n","    \"\"\"\n","    Compute average toxicity score.\n","    \"\"\"\n","    all_scores = []\n","\n","    for i in range(0, len(texts), batch_size):\n","        batch = texts[i:i+batch_size]\n","        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n","        inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n","            toxic_probs = predictions[:, 1]  # Label 1 = toxic\n","            all_scores.extend(toxic_probs.cpu().tolist())\n","\n","    return np.mean(all_scores)\n","\n","def compute_perplexity(texts, tokenizer, model, device=\"cuda\"):\n","    \"\"\"\n","    Compute average perplexity.\n","    \"\"\"\n","    perplexities = []\n","\n","    for text in texts:\n","        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n","        inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","        with torch.no_grad():\n","            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n","            loss = outputs.loss\n","            ppl = torch.exp(loss).item()\n","            perplexities.append(ppl)\n","\n","    return np.mean(perplexities)\n","\n","def compute_bertscore(predictions, references):\n","    \"\"\"\n","    Compute BERTScore.\n","    \"\"\"\n","    result = bertscore_metric.compute(predictions=predictions, references=references, lang=\"en\")\n","    return np.mean(result['f1'])\n","\n","def compute_bleu(predictions, references):\n","    \"\"\"\n","    Compute BLEU score.\n","    \"\"\"\n","    formatted_refs = [[ref] for ref in references]\n","    result = bleu_metric.compute(predictions=predictions, references=formatted_refs)\n","    return result['score']\n","\n","def compute_meaningbert(predictions, references):\n","    \"\"\"\n","    Compute MeaningBERT score using sentence-transformers.\n","    Returns cosine similarity scaled to 0-100.\n","    \"\"\"\n","    pred_embs = sim_model.encode(predictions, convert_to_tensor=True)\n","    ref_embs = sim_model.encode(references, convert_to_tensor=True)\n","    cosine_scores = torch.nn.functional.cosine_similarity(pred_embs, ref_embs)\n","    return cosine_scores.mean().item() * 100\n","\n","def evaluate_all(orig_texts, gen_texts, device=\"cuda\"):\n","    \"\"\"\n","    Run all evaluations including MeaningBERT.\n","    \"\"\"\n","    results = {}\n","\n","    print(\"  Computing toxicity scores...\")\n","    results['toxicity_gen'] = compute_toxicity(gen_texts, tox_tokenizer, tox_model, device)\n","    results['toxicity_orig'] = compute_toxicity(orig_texts, tox_tokenizer, tox_model, device)\n","\n","    print(\"  Computing perplexity...\")\n","    results['perplexity_gen'] = compute_perplexity(gen_texts, ppl_tokenizer, ppl_model, device)\n","    results['perplexity_orig'] = compute_perplexity(orig_texts, ppl_tokenizer, ppl_model, device)\n","\n","    print(\"  Computing BERTScore...\")\n","    results['bertscore'] = compute_bertscore(gen_texts, orig_texts)\n","\n","    print(\"  Computing MeaningBERT...\")\n","    results['meaningbert'] = compute_meaningbert(gen_texts, orig_texts)\n","\n","    print(\"  Computing BLEU...\")\n","    results['bleu4'] = compute_bleu(gen_texts, orig_texts)\n","\n","    return results\n","\n","print(\"Evaluation functions defined\")"]},{"cell_type":"markdown","metadata":{"id":"rlufDviseoHt"},"source":["## Main Pipeline Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pe0JESJ0eoHt"},"outputs":[],"source":["#@title detoxify() - T5 + DecompX reranking pipeline\n","\n","def detoxify(\n","    data_type: str = \"paradetox\",\n","    output_folder: str = \"T5_w_DecompX-Reranking\",\n","    batch_size: int = 8,\n","    max_length: int = 128,\n","    num_examples: int = 100,\n","    num_candidates: int = 10,\n","    temperature: float = 1.0,\n","    top_k: int = 50,\n","    top_p: float = 0.95,\n","    overwrite_gen: bool = False,\n","    run_eval: bool = True,\n","    overwrite_eval: bool = False,\n","    echo: bool = False,\n","):\n","    \"\"\"\n","    T5-ParaDetox pipeline with DecompX reranking.\n","\n","    1. Generate num_candidates detoxified texts per input using T5 sampling\n","    2. Score each candidate using DecompX toxicity attribution\n","    3. Select candidate with lowest toxicity score\n","    4. Evaluate with BLEU, BERTScore, MeaningBERT, Perplexity, Toxicity\n","    \"\"\"\n","    assert data_type in data_configs, f\"Unknown data_type: {data_type}\"\n","\n","    # Output paths\n","    base_out_dir = os.path.join(XDETOX_DIR, \"data\", \"model_outputs\", output_folder)\n","    data_out_dir = os.path.join(base_out_dir, data_type)\n","    _ensure_dir(data_out_dir)\n","\n","    orig_path = os.path.join(data_out_dir, \"orig.txt\")\n","    gen_path = os.path.join(data_out_dir, \"gen.txt\")\n","    stats_path = os.path.join(data_out_dir, \"gen_stats.txt\")\n","\n","    # Load data\n","    print(f\"\\n[{data_type}] Loading data...\")\n","    orig_texts = load_test_data(data_type, num_examples)\n","    print(f\"  Loaded {len(orig_texts)} examples\")\n","\n","    if echo:\n","        print(f\"\\n[echo] Example inputs (first 3):\")\n","        for i, s in enumerate(orig_texts[:3]):\n","            print(f\"  input[{i}]: {s}\")\n","\n","    # Generate or load\n","    if overwrite_gen or not os.path.exists(gen_path):\n","        print(f\"  Generating {num_candidates} candidates per input...\")\n","        all_candidates = t5_generate_candidates_batch(\n","            orig_texts, t5_model, t5_tokenizer, num_candidates,\n","            temperature, top_k, top_p, max_length, device\n","        )\n","\n","        if echo:\n","            print(f\"\\n[echo] Example candidates for input[0]:\")\n","            for j, c in enumerate(all_candidates[0][:3]):\n","                print(f\"    candidate[{j}]: {c}\")\n","\n","        print(f\"  Reranking with DecompX...\")\n","        gen_texts = rerank_with_decompx(orig_texts, all_candidates)\n","        release_decompx()  # Free GPU memory\n","\n","        if echo:\n","            print(f\"\\n[echo] Selected outputs (first 3):\")\n","            for i, g in enumerate(gen_texts[:3]):\n","                print(f\"  output[{i}]: {g}\")\n","\n","        # Save outputs\n","        with open(orig_path, 'w') as f:\n","            for t in orig_texts:\n","                f.write(re.sub(r\"\\s+\", \" \", t).strip() + '\\n')\n","        with open(gen_path, 'w') as f:\n","            for t in gen_texts:\n","                f.write(re.sub(r\"\\s+\", \" \", t).strip() + '\\n')\n","\n","        print(f\"  Saved outputs to {data_out_dir}\")\n","    else:\n","        print(f\"  Loading existing outputs...\")\n","        with open(orig_path, 'r') as f:\n","            orig_texts = [l.strip() for l in f]\n","        with open(gen_path, 'r') as f:\n","            gen_texts = [l.strip() for l in f]\n","        print(f\"  Loaded {len(gen_texts)} examples\")\n","\n","    # Evaluate\n","    if run_eval and (overwrite_eval or not os.path.exists(stats_path)):\n","        print(f\"  Running evaluation...\")\n","        results = evaluate_all(orig_texts, gen_texts, device)\n","\n","        with open(stats_path, 'w') as f:\n","            for k, v in results.items():\n","                f.write(f\"{k}: {v}\\n\")\n","\n","        if echo:\n","            print(f\"\\n[echo] Evaluation metrics:\")\n","            for k, v in results.items():\n","                print(f\"  {k}: {v:.4f}\")\n","\n","        print(f\"  Saved stats to {stats_path}\")\n","        return results\n","\n","    elif run_eval:\n","        print(f\"  Loading existing stats...\")\n","        results = _read_stats_file(stats_path)\n","        return results\n","\n","    return None\n","\n","print(\"detoxify() function defined\")"]},{"cell_type":"markdown","metadata":{"id":"9DbDFQYxeoHt"},"source":["## Run Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KYhL4PZoeoHt"},"outputs":[],"source":["#@title Example run on paradetox\n","detoxify(\n","    data_type=\"paradetox\",\n","    output_folder=\"T5_w_DecompX-Reranking\",\n","    num_examples=1000,\n","    num_candidates=10,\n","    temperature=1.0,\n","    run_eval=True,\n","    overwrite_gen=True,\n","    overwrite_eval=True,\n","    echo=True,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wSlQBk1AeoHt"},"outputs":[],"source":["#@title Run on multiple datasets\n","\n","datasets_to_eval = [\"paradetox\", \"microagressions_test\", \"sbf_test\", \"dynabench_test\"]\n","num_examples = 200\n","output_folder = \"T5_w_DecompX-Reranking\"\n","\n","all_results = {}\n","\n","print(\"=\" * 80)\n","print(\"T5-PARADETOX + DECOMPX RERANKING PIPELINE\")\n","print(\"=\" * 80)\n","\n","for dataset_name in datasets_to_eval:\n","    try:\n","        results = detoxify(\n","            data_type=dataset_name,\n","            output_folder=output_folder,\n","            batch_size=8,\n","            max_length=128,\n","            num_examples=num_examples,\n","            num_candidates=10,\n","            temperature=1.0,\n","            overwrite_gen=False,\n","            run_eval=True,\n","            overwrite_eval=False,\n","            echo=False,\n","        )\n","\n","        if results:\n","            all_results[dataset_name] = results\n","            print(f\"  {dataset_name} complete!\")\n","\n","    except Exception as e:\n","        print(f\"  Error on {dataset_name}: {e}\")\n","        import traceback\n","        traceback.print_exc()\n","        continue\n","\n","print(\"\\n\" + \"=\" * 80)"]},{"cell_type":"markdown","metadata":{"id":"gSES3bjJeoHt"},"source":["## Results Summary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mIafj9wReoHt"},"outputs":[],"source":["#@title Display results table\n","\n","if all_results:\n","    rows = []\n","    for dataset_name, results in all_results.items():\n","        row = {'dataset': dataset_name}\n","        row.update(results)\n","        rows.append(row)\n","\n","    df = pd.DataFrame(rows)\n","\n","    col_order = [\n","        'dataset',\n","        'bertscore',\n","        'meaningbert',\n","        'bleu4',\n","        'perplexity_gen',\n","        'perplexity_orig',\n","        'toxicity_gen',\n","        'toxicity_orig',\n","    ]\n","    df = df[[col for col in col_order if col in df.columns]]\n","\n","    # Save to CSV\n","    summary_csv = os.path.join(XDETOX_DIR, \"data\", \"model_outputs\", output_folder, \"t5_decompx_summary.csv\")\n","    df.to_csv(summary_csv, index=False)\n","    print(f\"Saved summary to {summary_csv}\\n\")\n","\n","    # Display\n","    print(\"=\" * 80)\n","    print(\"T5-PARADETOX + DECOMPX RERANKING RESULTS\")\n","    print(\"=\" * 80)\n","    print(df.to_string(index=False))\n","    print(\"=\" * 80)\n","else:\n","    print(\"No results available.\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.0"}},"nbformat":4,"nbformat_minor":0}
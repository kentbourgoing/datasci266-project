{"cells":[{"cell_type":"markdown","metadata":{"id":"lz_UXLl0xV-f"},"source":["# T5-XDetox Pipeline (DecompX Masking + T5 Ensemble + DecompX Reranking)\n","\n","This notebook runs the T5-XDetox pipeline with:\n","\n","1. **DecompX masking** – token-level toxicity attribution on RoBERTa to decide which tokens to mask with `<extra_id_0>`.\n","2. **T5 Ensemble generation** – T5 base + expert (non-toxic) + anti-expert (toxic) with logits combination.\n","3. **Optional DecompX-based reranking** – generate several candidates and pick the **least toxic** one.\n","4. **Evaluation** – BLEU, BERTScore, perplexity, and toxicity, plus a summary CSV per dataset.\n","\n","---\n","\n","## What this pipeline does\n","\n","For each chosen dataset:\n","\n","1. **Masking with DecompX**: Use RoBERTa toxicity classifier with gradient attribution to get **per-token toxicity importance**. Tokens that contribute to toxicity are **replaced with `<extra_id_0>`**.\n","\n","2. **Generation with T5 Ensemble**: For each masked input, use ensemble of T5 models:\n","   - **Base** model (your trained t5-base-detox-model)\n","   - **Expert** model (trained on non-toxic text)\n","   - **Anti-expert** model (trained on toxic text)\n","   \n","   During generation, logits are combined:\n","   $$\\text{logits}_{\\text{ens}} = \\alpha_b \\cdot \\text{logits}_{\\text{base}} + \\alpha_e \\cdot \\text{logits}_{\\text{expert}} - \\alpha_a \\cdot \\text{logits}_{\\text{anti}}$$\n","\n","3. **DecompX-based reranking**: When `ranking=True`, generate `num_candidates` candidates and choose the one with **lowest toxicity importance**.\n","\n","4. **Evaluation**: Compute BLEU, BERTScore, perplexity, toxicity for each threshold.\n","\n","---\n","\n","## Setup Requirements\n","\n","- **Jigsaw dataset**: Download from [Kaggle](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data)\n","- **T5 base model**: Pre-trained at `{PROJECT_BASE}/t5-base-detox-model`\n","- **Expert/Anti-expert**: Train using cells below (2-3 hours each on A100)"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QUGRsdDCxV-g","executionInfo":{"status":"ok","timestamp":1764747809919,"user_tz":480,"elapsed":1002,"user":{"displayName":"Ben He","userId":"03559440280945504495"}},"outputId":"6b707eff-6894-4411-b522-313ef8bda827"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","PROJECT_BASE: /content/drive/MyDrive/ds266/w266 - Project\n","T5_BASE_CHECKPOINT: /content/drive/MyDrive/ds266/w266 - Project/t5-base-detox-model\n","DATASET_BASE: /content/drive/MyDrive/ds266/w266 - Project/XDetox\n"]}],"source":["#@title Mount Drive & Setup Paths\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import sys\n","\n","# Base paths (matching T5-ParaDetox)\n","PROJECT_BASE = \"/content/drive/MyDrive/ds266/w266 - Project\"\n","HF_CACHE = os.path.join(PROJECT_BASE, \"cache\")\n","T5_BASE_CHECKPOINT = os.path.join(PROJECT_BASE, \"t5-base-detox-model\")\n","DATASET_BASE = os.path.join(PROJECT_BASE, \"XDetox\")\n","\n","# Expert/Anti-expert paths\n","T5_EXPERT_CHECKPOINT = os.path.join(PROJECT_BASE, \"t5-expert-nontoxic\")\n","T5_ANTIEXPERT_CHECKPOINT = os.path.join(PROJECT_BASE, \"t5-antiexpert-toxic\")\n","\n","os.environ[\"TRANSFORMERS_CACHE\"] = HF_CACHE\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\n","print(f\"PROJECT_BASE: {PROJECT_BASE}\")\n","print(f\"T5_BASE_CHECKPOINT: {T5_BASE_CHECKPOINT}\")\n","print(f\"DATASET_BASE: {DATASET_BASE}\")"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Kz-39RhxV-h","executionInfo":{"status":"ok","timestamp":1764747811504,"user_tz":480,"elapsed":1586,"user":{"displayName":"Ben He","userId":"03559440280945504495"}},"outputId":"7e1f4d5c-eea5-4ec3-bc6a-7e4d944f0b1c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n","GPU: NVIDIA L4\n","GPU Memory: 23.80 GB\n"]}],"source":["#@title Runtime setup (GPU check)\n","import torch\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Device: {device}\")\n","if torch.cuda.is_available():\n","    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n","    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n","else:\n","    print(\"⚠️ No GPU - training will be very slow!\")"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KApE4jZpxV-h","executionInfo":{"status":"ok","timestamp":1764747833674,"user_tz":480,"elapsed":22169,"user":{"displayName":"Ben He","userId":"03559440280945504495"}},"outputId":"373fdabf-bcbc-4e4f-f360-5ac5a3a9eb3e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: transformers 4.41.2\n","Uninstalling transformers-4.41.2:\n","  Successfully uninstalled transformers-4.41.2\n","Found existing installation: tokenizers 0.19.1\n","Uninstalling tokenizers-0.19.1:\n","  Successfully uninstalled tokenizers-0.19.1\n","Found existing installation: datasets 2.19.0\n","Uninstalling datasets-2.19.0:\n","  Successfully uninstalled datasets-2.19.0\n","Found existing installation: evaluate 0.4.1\n","Uninstalling evaluate-0.4.1:\n","  Successfully uninstalled evaluate-0.4.1\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m193.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtune 0.6.1 requires datasets, which is not installed.\u001b[0m\u001b[31m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m139.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m199.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m403.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m312.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h============================================================\n","✓ Package installation complete!\n","============================================================\n","\n","⚠️ CRITICAL NEXT STEPS:\n","1. Click 'Runtime' -> 'Restart runtime' NOW!\n","2. After restart, run the NEXT cell to verify versions\n","3. Then continue from cell 5\n"]}],"source":["#@title Install dependencies - RESTART RUNTIME AFTER THIS COMPLETES!\n","\n","# Step 1: Completely uninstall transformers and related packages\n","!pip uninstall -y transformers tokenizers datasets evaluate\n","\n","# Step 2: Install tokenizers SEPARATELY with --only-binary flag (to avoid build errors)\n","!pip install -q --no-cache-dir --only-binary=tokenizers \"tokenizers==0.19.1\"\n","\n","# Step 3: Install core transformers packages (WITHOUT --only-binary)\n","!pip install -q --no-cache-dir \\\n","    \"transformers==4.41.2\" \"datasets==2.19.0\" \"evaluate==0.4.1\" \\\n","    \"sacrebleu==2.4.1\" sacremoses ftfy nltk matplotlib pandas\n","\n","# Step 4: Install other dependencies (these won't upgrade transformers)\n","!pip install -q bert-score sentence-transformers accelerate scikit-learn\n","\n","print(\"=\"*60)\n","print(\"✓ Package installation complete!\")\n","print(\"=\"*60)\n","print(\"\\n⚠️ CRITICAL NEXT STEPS:\")\n","print(\"1. Click 'Runtime' -> 'Restart runtime' NOW!\")\n","print(\"2. After restart, run the NEXT cell to verify versions\")\n","print(\"3. Then continue from cell 5\")"]},{"cell_type":"code","source":["#@title Verify installed versions (run AFTER restart)\n","import transformers, tokenizers, datasets, evaluate\n","\n","print(\"=\"*60)\n","print(\"INSTALLED VERSIONS:\")\n","print(f\"✓ transformers: {transformers.__version__}\")\n","print(f\"✓ tokenizers: {tokenizers.__version__}\")\n","print(f\"✓ datasets: {datasets.__version__}\")\n","print(f\"✓ evaluate: {evaluate.__version__}\")\n","print(\"=\"*60)\n","\n","# Check if correct versions\n","if transformers.__version__ == \"4.41.2\":\n","    print(\"\\n✅ Correct transformers version installed!\")\n","else:\n","    print(f\"\\n⚠️ WARNING: Expected transformers 4.41.2, got {transformers.__version__}\")"],"metadata":{"id":"cKOs9m1bxV-h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764747844871,"user_tz":480,"elapsed":11195,"user":{"displayName":"Ben He","userId":"03559440280945504495"}},"outputId":"3ddf328d-403a-4e09-feb8-08675ed1a418"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["============================================================\n","INSTALLED VERSIONS:\n","✓ transformers: 4.41.2\n","✓ tokenizers: 0.19.1\n","✓ datasets: 2.19.0\n","✓ evaluate: 0.4.1\n","============================================================\n","\n","✅ Correct transformers version installed!\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"id":"5mPkfj5BxV-h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764747846499,"user_tz":480,"elapsed":1627,"user":{"displayName":"Ben He","userId":"03559440280945504495"}},"outputId":"14e98db4-3ce8-47cd-9268-299b67c7caf2"},"outputs":[{"output_type":"stream","name":"stdout","text":["NLTK ready\n"]}],"source":["#@title NLTK data\n","import nltk\n","nltk.download(\"punkt\", quiet=True)\n","try:\n","    nltk.download(\"punkt_tab\", quiet=True)\n","except:\n","    pass\n","print(\"NLTK ready\")"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"nEOFLHt5xV-i","colab":{"base_uri":"https://localhost:8080/","height":426},"executionInfo":{"status":"error","timestamp":1764747846960,"user_tz":480,"elapsed":458,"user":{"displayName":"Ben He","userId":"03559440280945504495"}},"outputId":"11a5810b-6c63-460b-fe8a-9d8d524dbf4c"},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Failed to import transformers.trainer because of the following error (look up to see its traceback):\ncannot import name 'EncoderDecoderCache' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1534\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1535\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1536\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_peft_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpeft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m from .auto import (\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/auto.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m from .peft_model import (\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mPeftModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMSELoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDynamicCache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEncoderDecoderCache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_outputs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQuestionAnsweringModelOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequenceClassifierOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTokenClassifierOutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'EncoderDecoderCache' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2990857713.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Core ML libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m from transformers import (\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mT5Tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT5ForConditionalGeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1523\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1525\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1526\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1535\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1538\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.trainer because of the following error (look up to see its traceback):\ncannot import name 'EncoderDecoderCache' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)"]}],"source":["#@title Import libraries\n","import os\n","import sys\n","import json\n","import torch\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","from tqdm.auto import tqdm\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Core ML libraries\n","from transformers import (\n","    T5Tokenizer, T5ForConditionalGeneration,\n","    AutoTokenizer, AutoModelForSequenceClassification,\n","    GPT2Tokenizer, GPT2LMHeadModel,\n","    Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",")\n","from torch.utils.data import Dataset\n","from sklearn.model_selection import train_test_split\n","from evaluate import load\n","import torch.nn.functional as F\n","\n","print(f\"PyTorch version: {torch.__version__}\")\n","print(f\"CUDA available: {torch.cuda.is_available()}\")\n","print(\"✓ Libraries imported\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YoVj0Y3pxV-i","executionInfo":{"status":"aborted","timestamp":1764747846959,"user_tz":480,"elapsed":38223,"user":{"displayName":"Ben He","userId":"03559440280945504495"}}},"outputs":[],"source":["#@title Data configs (matching XDetox datasets)\n","data_configs = {\n","    \"paradetox\": {\n","        \"data_path\": os.path.join(DATASET_BASE, \"datasets/paradetox/test_toxic_parallel.txt\"),\n","        \"alpha_a\": 0.5,\n","        \"alpha_e\": 2.5,\n","        \"temperature\": 1.0,\n","    },\n","    \"microagressions_test\": {\n","        \"data_path\": os.path.join(DATASET_BASE, \"datasets/microagressions/test.csv\"),\n","        \"alpha_a\": 0.5,\n","        \"alpha_e\": 2.75,\n","        \"temperature\": 1.0,\n","    },\n","    \"sbf_test\": {\n","        \"data_path\": os.path.join(DATASET_BASE, \"datasets/sbf/sbftst.csv\"),\n","        \"alpha_a\": 0.5,\n","        \"alpha_e\": 3.0,\n","        \"temperature\": 1.0,\n","    },\n","    \"dynabench_test\": {\n","        \"data_path\": os.path.join(DATASET_BASE, \"datasets/dynabench/db_test.csv\"),\n","        \"alpha_a\": 0.5,\n","        \"alpha_e\": 2.75,\n","        \"temperature\": 1.0,\n","    },\n","    \"jigsaw_toxic\": {\n","        \"data_path\": os.path.join(DATASET_BASE, \"datasets/jigsaw_full_30/test_10k_toxic.txt\"),\n","        \"alpha_a\": 0.5,\n","        \"alpha_e\": 2.75,\n","        \"temperature\": 1.0,\n","    }\n","}\n","\n","print(\"Datasets:\", \", \".join(data_configs.keys()))"]},{"cell_type":"markdown","metadata":{"id":"DkjuAj_PxV-i"},"source":["## Helper Functions\n","\n","Core functions for T5-XDetox pipeline:\n","- Model loading\n","- DecompX masking\n","- T5 ensemble generation\n","- DecompX reranking\n","- Evaluation\n","- Data loading"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_iecxxZvxV-j","executionInfo":{"status":"aborted","timestamp":1764747846993,"user_tz":480,"elapsed":38256,"user":{"displayName":"Ben He","userId":"03559440280945504495"}}},"outputs":[],"source":["#@title Helper Functions (DecompX masking, T5 ensemble, evaluation)\n","\n","# ============================================================================\n","# MODEL LOADING\n","# ============================================================================\n","def load_models():\n","    \"\"\"Load all required models for T5-XDetox pipeline\"\"\"\n","    global t5_base, t5_expert, t5_antiexpert, t5_tokenizer\n","    global decompx_model, decompx_tokenizer\n","    global toxicity_model, toxicity_tokenizer\n","    global gpt2_model, gpt2_tokenizer\n","    global bleu_metric, bertscore_metric\n","\n","    print(\"=\"*80)\n","    print(\"LOADING MODELS\")\n","    print(\"=\"*80)\n","\n","    # T5 models\n","    print(\"Loading T5 base...\")\n","    t5_tokenizer = T5Tokenizer.from_pretrained(T5_BASE_CHECKPOINT)\n","    t5_base = T5ForConditionalGeneration.from_pretrained(T5_BASE_CHECKPOINT).to(device).eval()\n","\n","    # Expert/Anti-expert\n","    if os.path.exists(T5_EXPERT_CHECKPOINT):\n","        print(\"Loading T5 expert...\")\n","        t5_expert = T5ForConditionalGeneration.from_pretrained(T5_EXPERT_CHECKPOINT).to(device).eval()\n","    else:\n","        print(\"⚠️ T5 expert not found - using base model\")\n","        t5_expert = t5_base\n","\n","    if os.path.exists(T5_ANTIEXPERT_CHECKPOINT):\n","        print(\"Loading T5 anti-expert...\")\n","        t5_antiexpert = T5ForConditionalGeneration.from_pretrained(T5_ANTIEXPERT_CHECKPOINT).to(device).eval()\n","    else:\n","        print(\"⚠️ T5 anti-expert not found - using base model\")\n","        t5_antiexpert = t5_base\n","\n","    # DecompX classifier\n","    print(\"Loading DecompX classifier...\")\n","    decompx_tokenizer = AutoTokenizer.from_pretrained(\"martin-ha/toxic-comment-model\")\n","    decompx_model = AutoModelForSequenceClassification.from_pretrained(\"martin-ha/toxic-comment-model\").to(device).eval()\n","\n","    # Toxicity classifier for evaluation\n","    print(\"Loading toxicity classifier...\")\n","    toxicity_tokenizer = AutoTokenizer.from_pretrained(\"unitary/toxic-bert\")\n","    toxicity_model = AutoModelForSequenceClassification.from_pretrained(\"unitary/toxic-bert\").to(device).eval()\n","\n","    # GPT-2 for perplexity\n","    print(\"Loading GPT-2 for perplexity...\")\n","    gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","    gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device).eval()\n","    gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n","\n","    # Evaluation metrics\n","    bleu_metric = load('bleu')\n","    bertscore_metric = load('bertscore')\n","\n","    print(\"=\"*80)\n","    print(\"✓ ALL MODELS LOADED\")\n","    print(\"=\"*80)\n","\n","# ============================================================================\n","# DECOMPX MASKING\n","# ============================================================================\n","def get_token_toxicity_scores(text):\n","    \"\"\"Get per-token toxicity scores using gradient attribution\"\"\"\n","    inputs = decompx_tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n","    inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","    # Enable gradients on embeddings\n","    embeddings = decompx_model.get_input_embeddings()(inputs['input_ids'])\n","    embeddings.requires_grad = True\n","\n","    outputs = decompx_model(inputs_embeds=embeddings, attention_mask=inputs['attention_mask'])\n","    probabilities = F.softmax(outputs.logits, dim=-1)\n","    toxicity_prob = probabilities[0, 1]\n","\n","    toxicity_prob.backward()\n","    token_importance = embeddings.grad.abs().sum(dim=-1).squeeze().cpu().numpy()\n","    tokens = decompx_tokenizer.convert_ids_to_tokens(inputs['input_ids'].squeeze())\n","\n","    return tokens, token_importance, toxicity_prob.item()\n","\n","def mask_toxic_tokens(text, threshold=0.20):\n","    \"\"\"Mask toxic tokens with <extra_id_0> for T5\"\"\"\n","    try:\n","        tokens, importance, _ = get_token_toxicity_scores(text)\n","\n","        if len(importance) > 0 and np.mean(importance) > 0:\n","            normalized = importance / (np.mean(importance) + 1e-8)\n","        else:\n","            normalized = importance\n","\n","        mask_indices = normalized > threshold\n","        masked_tokens = []\n","        num_masked = 0\n","\n","        for token, should_mask in zip(tokens, mask_indices):\n","            if token in ['<s>', '</s>', '<pad>', '<unk>', '[CLS]', '[SEP]']:\n","                masked_tokens.append(token)\n","            elif should_mask:\n","                masked_tokens.append('<extra_id_0>')\n","                num_masked += 1\n","            else:\n","                masked_tokens.append(token)\n","\n","        masked_text = decompx_tokenizer.convert_tokens_to_string(masked_tokens)\n","        masked_text = masked_text.replace('</s>', '').replace('<s>', '').strip()\n","\n","        return masked_text, num_masked\n","    except:\n","        return text, 0\n","\n","# ============================================================================\n","# T5 ENSEMBLE GENERATION\n","# ============================================================================\n","def generate_with_ensemble(masked_text, alpha_b=1.0, alpha_e=2.5, alpha_a=0.5,\n","                           max_length=128, temperature=1.0, top_p=0.95, top_k=50):\n","    \"\"\"Generate using T5 ensemble with TRUE logits combination\"\"\"\n","    input_text = f\"detoxify: {masked_text}\"\n","    input_ids = t5_tokenizer.encode(input_text, return_tensors='pt').to(device)\n","    generated_ids = input_ids.clone()\n","\n","    with torch.no_grad():\n","        for _ in range(max_length):\n","            # Get logits from all three models\n","            base_out = t5_base(input_ids=generated_ids)\n","            expert_out = t5_expert(input_ids=generated_ids)\n","            anti_out = t5_antiexpert(input_ids=generated_ids)\n","\n","            # Ensemble combination\n","            ensemble_logits = (\n","                alpha_b * base_out.logits[:, -1, :] +\n","                alpha_e * expert_out.logits[:, -1, :] -\n","                alpha_a * anti_out.logits[:, -1, :]\n","            ) / temperature\n","\n","            # Top-k filtering\n","            if top_k > 0:\n","                indices_to_remove = ensemble_logits < torch.topk(ensemble_logits, top_k)[0][..., -1, None]\n","                ensemble_logits[indices_to_remove] = float('-inf')\n","\n","            # Top-p filtering\n","            if top_p < 1.0:\n","                sorted_logits, sorted_indices = torch.sort(ensemble_logits, descending=True)\n","                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n","                sorted_indices_to_remove = cumulative_probs > top_p\n","                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n","                sorted_indices_to_remove[..., 0] = 0\n","                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n","                ensemble_logits[indices_to_remove] = float('-inf')\n","\n","            # Sample next token\n","            probs = F.softmax(ensemble_logits, dim=-1)\n","            next_token = torch.multinomial(probs, num_samples=1)\n","            generated_ids = torch.cat([generated_ids, next_token], dim=1)\n","\n","            if next_token.item() == t5_tokenizer.eos_token_id:\n","                break\n","\n","    text = t5_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n","    return text[9:].strip() if text.startswith('detoxify:') else text\n","\n","# ============================================================================\n","# DECOMPX RERANKING\n","# ============================================================================\n","def get_toxicity_importance(text):\n","    \"\"\"Get summed token-level toxicity importance for reranking\"\"\"\n","    try:\n","        _, importance, _ = get_token_toxicity_scores(text)\n","        return np.sum(np.abs(importance))\n","    except:\n","        return float('inf')\n","\n","def rerank_candidates(candidates):\n","    \"\"\"Rerank candidates, return best (lowest toxicity)\"\"\"\n","    if len(candidates) == 1:\n","        return candidates[0], [0.0]\n","\n","    scores = [get_toxicity_importance(c) for c in candidates]\n","    best_idx = np.argmin(scores)\n","    return candidates[best_idx], scores\n","\n","# ============================================================================\n","# EVALUATION\n","# ============================================================================\n","def evaluate_toxicity(texts):\n","    \"\"\"Evaluate average toxicity\"\"\"\n","    scores = []\n","    for text in tqdm(texts, desc=\"Toxicity\", leave=False):\n","        inputs = toxicity_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n","        inputs = {k: v.to(device) for k, v in inputs.items()}\n","        with torch.no_grad():\n","            outputs = toxicity_model(**inputs)\n","            probs = F.softmax(outputs.logits, dim=-1)\n","            scores.append(probs[0, 1].item())\n","    return np.mean(scores)\n","\n","def evaluate_perplexity(texts):\n","    \"\"\"Evaluate average perplexity\"\"\"\n","    total_loss, total_tokens = 0, 0\n","    for text in tqdm(texts, desc=\"Perplexity\", leave=False):\n","        inputs = gpt2_tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n","        inputs = {k: v.to(device) for k, v in inputs.items()}\n","        with torch.no_grad():\n","            outputs = gpt2_model(**inputs, labels=inputs['input_ids'])\n","            total_loss += outputs.loss.item() * inputs['input_ids'].shape[1]\n","            total_tokens += inputs['input_ids'].shape[1]\n","    return np.exp(total_loss / total_tokens) if total_tokens > 0 else float('inf')\n","\n","def evaluate_all(orig_texts, gen_texts):\n","    \"\"\"Run all evaluations\"\"\"\n","    print(\"  Evaluating...\")\n","    results = {}\n","    results['toxicity_orig'] = evaluate_toxicity(orig_texts)\n","    results['toxicity_gen'] = evaluate_toxicity(gen_texts)\n","    results['perplexity_orig'] = evaluate_perplexity(orig_texts)\n","    results['perplexity_gen'] = evaluate_perplexity(gen_texts)\n","    results['bleu4'] = bleu_metric.compute(predictions=gen_texts, references=[[t] for t in orig_texts])['bleu']\n","    results['bertscore'] = np.mean(bertscore_metric.compute(predictions=gen_texts, references=orig_texts, lang='en')['f1'])\n","    return results\n","\n","# ============================================================================\n","# DATA LOADING\n","# ============================================================================\n","def load_dataset(data_type, num_examples=None):\n","    \"\"\"Load dataset from file\"\"\"\n","    if data_type not in data_configs:\n","        raise ValueError(f\"Unknown data_type: {data_type}\")\n","\n","    data_path = data_configs[data_type][\"data_path\"]\n","\n","    if not os.path.exists(data_path):\n","        print(f\"⚠️ Dataset not found: {data_path}\")\n","        return [\"You are such an idiot.\", \"This is terrible.\"] * 50\n","\n","    if data_path.endswith('.txt'):\n","        with open(data_path, 'r') as f:\n","            texts = [line.strip() for line in f if line.strip()]\n","    elif data_path.endswith('.csv'):\n","        df = pd.read_csv(data_path)\n","        col = 'text' if 'text' in df.columns else df.columns[0]\n","        texts = df[col].tolist()\n","    else:\n","        df = pd.read_csv(data_path, sep='\\t')\n","        col = 'text' if 'text' in df.columns else df.columns[0]\n","        texts = df[col].tolist()\n","\n","    texts = [str(t).strip() for t in texts if pd.notna(t)]\n","    return texts[:num_examples] if num_examples else texts\n","\n","print(\"✓ Helper functions loaded\")"]},{"cell_type":"markdown","metadata":{"id":"ONophxTWxV-j"},"source":["## Training T5 Expert/Anti-Expert Models (Optional)\n","\n","Run these cells to train expert and anti-expert models on Jigsaw data.\n","\n","**Requirements:**\n","- Download `train.csv` from [Kaggle Jigsaw](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data)\n","- Upload to: `{DATASET_BASE}/jigsaw/train.csv`\n","\n","**Training time:** ~2-3 hours per model on A100 GPU\n","\n","Set `SKIP_TRAINING = False` below to run training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UwTwnePWxV-j","executionInfo":{"status":"aborted","timestamp":1764747847005,"user_tz":480,"elapsed":38268,"user":{"displayName":"Ben He","userId":"03559440280945504495"}}},"outputs":[],"source":["#@title Jigsaw Data Preparation & Training Code\n","SKIP_TRAINING = True  # Set to False to actually train\n","\n","# Jigsaw data paths\n","JIGSAW_CSV = os.path.join(DATASET_BASE, \"jigsaw/train.csv\")\n","JIGSAW_SPLITS_DIR = os.path.join(DATASET_BASE, \"jigsaw_splits\")\n","\n","# ============================================================================\n","# JIGSAW DATA PREPARATION\n","# ============================================================================\n","def prepare_jigsaw_splits(jigsaw_csv_path, toxic_threshold=0.5, output_dir=None):\n","    \"\"\"\n","    Split Jigsaw dataset into toxic and non-toxic subsets for training.\n","\n","    Args:\n","        jigsaw_csv_path: Path to Jigsaw train.csv from Kaggle\n","        toxic_threshold: Threshold for toxicity binary classification\n","        output_dir: Where to save splits (default: DATASET_BASE/jigsaw_splits)\n","\n","    Returns:\n","        (toxic_path, nontoxic_path): Paths to created CSV files\n","    \"\"\"\n","    if output_dir is None:\n","        output_dir = JIGSAW_SPLITS_DIR\n","\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    print(f\"Loading Jigsaw data from {jigsaw_csv_path}...\")\n","    df = pd.read_csv(jigsaw_csv_path)\n","\n","    # Jigsaw has columns: id, comment_text, toxic, severe_toxic, obscene, threat, insult, identity_hate\n","    # Combine all toxicity columns\n","    toxicity_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n","    df['is_toxic'] = (df[toxicity_cols].sum(axis=1) >= toxic_threshold).astype(int)\n","\n","    # Split\n","    toxic_df = df[df['is_toxic'] == 1][['comment_text']].copy()\n","    nontoxic_df = df[df['is_toxic'] == 0][['comment_text']].copy()\n","\n","    # Rename column\n","    toxic_df.columns = ['text']\n","    nontoxic_df.columns = ['text']\n","\n","    # Save\n","    toxic_path = os.path.join(output_dir, 'toxic.csv')\n","    nontoxic_path = os.path.join(output_dir, 'nontoxic.csv')\n","\n","    toxic_df.to_csv(toxic_path, index=False)\n","    nontoxic_df.to_csv(nontoxic_path, index=False)\n","\n","    print(f\"✓ Created splits:\")\n","    print(f\"  Toxic: {len(toxic_df)} examples → {toxic_path}\")\n","    print(f\"  Non-toxic: {len(nontoxic_df)} examples → {nontoxic_path}\")\n","\n","    return toxic_path, nontoxic_path\n","\n","# ============================================================================\n","# TRAINING DATASET CLASS\n","# ============================================================================\n","class T5DetoxDataset(Dataset):\n","    \"\"\"Dataset for T5 detoxification training\"\"\"\n","    def __init__(self, texts, tokenizer, max_length=128):\n","        self.texts = texts\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = str(self.texts[idx]).strip()\n","\n","        # Input: \"detoxify: <text>\"\n","        input_text = f\"detoxify: {text}\"\n","        # Target: same text (self-supervised)\n","        target_text = text\n","\n","        # Tokenize\n","        inputs = self.tokenizer(\n","            input_text,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","\n","        targets = self.tokenizer(\n","            target_text,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","\n","        return {\n","            'input_ids': inputs['input_ids'].squeeze(),\n","            'attention_mask': inputs['attention_mask'].squeeze(),\n","            'labels': targets['input_ids'].squeeze()\n","        }\n","\n","# ============================================================================\n","# TRAINING FUNCTIONS\n","# ============================================================================\n","def train_t5_model(data_csv, output_dir, base_model_path, num_train=10000,\n","                   num_epochs=3, batch_size=8, learning_rate=5e-5):\n","    \"\"\"\n","    Train T5 model on given data.\n","\n","    Args:\n","        data_csv: Path to CSV with 'text' column\n","        output_dir: Where to save trained model\n","        base_model_path: Path to T5 base model to start from\n","        num_train: Number of training examples (default 10k)\n","        num_epochs: Number of training epochs\n","        batch_size: Batch size\n","        learning_rate: Learning rate\n","    \"\"\"\n","    print(f\"\\n{'='*80}\")\n","    print(f\"Training T5 model → {output_dir}\")\n","    print(f\"{'='*80}\")\n","\n","    # Load data\n","    df = pd.read_csv(data_csv)\n","    texts = df['text'].tolist()[:num_train]\n","    print(f\"Loaded {len(texts)} training examples\")\n","\n","    # Split train/val\n","    train_texts, val_texts = train_test_split(texts, test_size=0.1, random_state=42)\n","    print(f\"Train: {len(train_texts)}, Val: {len(val_texts)}\")\n","\n","    # Load tokenizer and model\n","    tokenizer = T5Tokenizer.from_pretrained(base_model_path)\n","    model = T5ForConditionalGeneration.from_pretrained(base_model_path)\n","\n","    # Create datasets\n","    train_dataset = T5DetoxDataset(train_texts, tokenizer)\n","    val_dataset = T5DetoxDataset(val_texts, tokenizer)\n","\n","    # Training arguments\n","    training_args = TrainingArguments(\n","        output_dir=output_dir,\n","        num_train_epochs=num_epochs,\n","        per_device_train_batch_size=batch_size,\n","        per_device_eval_batch_size=batch_size,\n","        learning_rate=learning_rate,\n","        weight_decay=0.01,\n","        logging_steps=100,\n","        eval_steps=500,\n","        save_steps=500,\n","        evaluation_strategy=\"steps\",\n","        save_total_limit=2,\n","        fp16=torch.cuda.is_available(),\n","        load_best_model_at_end=True,\n","        metric_for_best_model=\"eval_loss\",\n","        report_to=\"none\"\n","    )\n","\n","    # Data collator\n","    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n","\n","    # Trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=val_dataset,\n","        data_collator=data_collator\n","    )\n","\n","    # Train\n","    print(\"\\nStarting training...\")\n","    trainer.train()\n","\n","    # Save\n","    print(f\"\\nSaving to {output_dir}...\")\n","    trainer.save_model(output_dir)\n","    tokenizer.save_pretrained(output_dir)\n","\n","    print(f\"✓ Training complete: {output_dir}\")\n","\n","# ============================================================================\n","# MAIN TRAINING EXECUTION\n","# ============================================================================\n","if not SKIP_TRAINING:\n","    # Step 1: Prepare Jigsaw splits\n","    if os.path.exists(JIGSAW_CSV):\n","        print(f\"Found Jigsaw dataset at {JIGSAW_CSV}\")\n","        toxic_path, nontoxic_path = prepare_jigsaw_splits(JIGSAW_CSV)\n","    else:\n","        print(f\"⚠️ Jigsaw dataset not found at {JIGSAW_CSV}\")\n","        print(\"Please download train.csv from Kaggle and upload to Google Drive\")\n","        print(\"https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data\")\n","        toxic_path = os.path.join(JIGSAW_SPLITS_DIR, 'toxic.csv')\n","        nontoxic_path = os.path.join(JIGSAW_SPLITS_DIR, 'nontoxic.csv')\n","\n","    # Step 2: Train Anti-Expert (on toxic data)\n","    if os.path.exists(toxic_path):\n","        print(\"\\n\" + \"=\"*80)\n","        print(\"TRAINING ANTI-EXPERT (on toxic data)\")\n","        print(\"=\"*80)\n","        train_t5_model(\n","            data_csv=toxic_path,\n","            output_dir=T5_ANTIEXPERT_CHECKPOINT,\n","            base_model_path=T5_BASE_CHECKPOINT,\n","            num_train=10000,\n","            num_epochs=3,\n","            batch_size=8\n","        )\n","    else:\n","        print(f\"⚠️ Toxic data not found at {toxic_path}\")\n","\n","    # Step 3: Train Expert (on non-toxic data)\n","    if os.path.exists(nontoxic_path):\n","        print(\"\\n\" + \"=\"*80)\n","        print(\"TRAINING EXPERT (on non-toxic data)\")\n","        print(\"=\"*80)\n","        train_t5_model(\n","            data_csv=nontoxic_path,\n","            output_dir=T5_EXPERT_CHECKPOINT,\n","            base_model_path=T5_BASE_CHECKPOINT,\n","            num_train=10000,\n","            num_epochs=3,\n","            batch_size=8\n","        )\n","    else:\n","        print(f\"⚠️ Non-toxic data not found at {nontoxic_path}\")\n","\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"✓ ALL TRAINING COMPLETE\")\n","    print(\"=\"*80)\n","else:\n","    print(\"SKIP_TRAINING = True, skipping training cells\")\n","    print(\"Models will be loaded from existing checkpoints\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"201nd-FyxV-k","executionInfo":{"status":"aborted","timestamp":1764747847006,"user_tz":480,"elapsed":38269,"user":{"displayName":"Ben He","userId":"03559440280945504495"}}},"outputs":[],"source":["#@title Main T5-XDetox Pipeline Function\n","def t5_detoxify(data_type=\"paradetox\",\n","                thresholds=[0.10, 0.15, 0.20, 0.25, 0.30],\n","                alpha_b=1.0,\n","                alpha_e=None,  # Will use data_config default if None\n","                alpha_a=None,  # Will use data_config default if None\n","                temperature=None,  # Will use data_config default if None\n","                num_examples=None,\n","                ranking=True,\n","                num_candidates=5,\n","                output_folder=\"default\",\n","                max_length=128,\n","                top_p=0.95,\n","                top_k=50):\n","    \"\"\"\n","    Run T5-XDetox pipeline: DecompX masking → T5 ensemble → DecompX reranking\n","\n","    Args:\n","        data_type: Dataset name from data_configs\n","        thresholds: List of DecompX masking thresholds\n","        alpha_b: Base model weight (default 1.0)\n","        alpha_e: Expert weight (uses data_config if None)\n","        alpha_a: Anti-expert weight (uses data_config if None)\n","        temperature: Sampling temperature (uses data_config if None)\n","        num_examples: Limit number of examples (None = all)\n","        ranking: Whether to use DecompX reranking\n","        num_candidates: Number of candidates for reranking\n","        output_folder: Output subdirectory name\n","        max_length: Max generation length\n","        top_p: Nucleus sampling parameter\n","        top_k: Top-k sampling parameter\n","\n","    Returns:\n","        results_df: DataFrame with all results\n","    \"\"\"\n","    print(\"\\n\" + \"=\"*80)\n","    print(f\"T5-XDETOX PIPELINE: {data_type}\")\n","    print(\"=\"*80)\n","\n","    # Get config\n","    if data_type not in data_configs:\n","        raise ValueError(f\"Unknown data_type: {data_type}\")\n","\n","    config = data_configs[data_type]\n","    alpha_e = alpha_e if alpha_e is not None else config['alpha_e']\n","    alpha_a = alpha_a if alpha_a is not None else config['alpha_a']\n","    temperature = temperature if temperature is not None else config['temperature']\n","\n","    print(f\"Config: alpha_b={alpha_b}, alpha_e={alpha_e}, alpha_a={alpha_a}, temp={temperature}\")\n","    print(f\"Thresholds: {thresholds}\")\n","    print(f\"Ranking: {ranking} (candidates={num_candidates})\")\n","\n","    # Load data\n","    print(f\"\\nLoading dataset: {data_type}\")\n","    texts = load_dataset(data_type, num_examples)\n","    print(f\"Loaded {len(texts)} examples\")\n","\n","    # Output directory\n","    output_base = os.path.join(PROJECT_BASE, \"data/t5_xdetox_outputs\", output_folder, data_type)\n","    os.makedirs(output_base, exist_ok=True)\n","\n","    # Results storage\n","    all_results = []\n","\n","    # Process each threshold\n","    for threshold in thresholds:\n","        print(f\"\\n{'='*80}\")\n","        print(f\"Processing threshold: {threshold}\")\n","        print(f\"{'='*80}\")\n","\n","        # Output paths\n","        threshold_str = f\"DecompX{threshold}\"\n","        output_dir = os.path.join(output_base, threshold_str)\n","        os.makedirs(output_dir, exist_ok=True)\n","\n","        masked_file = os.path.join(output_dir, \"masked.txt\")\n","        output_file = os.path.join(output_dir, \"detoxified.txt\")\n","\n","        # Step 1: Mask toxic tokens\n","        print(f\"\\n[1/3] Masking toxic tokens (threshold={threshold})...\")\n","        masked_texts = []\n","        for text in tqdm(texts, desc=\"Masking\"):\n","            masked, num_masked = mask_toxic_tokens(text, threshold=threshold)\n","            masked_texts.append(masked)\n","\n","        # Save masked\n","        with open(masked_file, 'w') as f:\n","            f.write('\\n'.join(masked_texts))\n","        print(f\"  Saved: {masked_file}\")\n","\n","        # Step 2: Generate with T5 ensemble\n","        print(f\"\\n[2/3] Generating with T5 ensemble...\")\n","        generated_texts = []\n","\n","        for masked_text in tqdm(masked_texts, desc=\"Generating\"):\n","            if ranking:\n","                # Generate multiple candidates\n","                candidates = []\n","                for _ in range(num_candidates):\n","                    gen = generate_with_ensemble(\n","                        masked_text,\n","                        alpha_b=alpha_b,\n","                        alpha_e=alpha_e,\n","                        alpha_a=alpha_a,\n","                        temperature=temperature,\n","                        max_length=max_length,\n","                        top_p=top_p,\n","                        top_k=top_k\n","                    )\n","                    candidates.append(gen)\n","\n","                # Step 3: Rerank candidates\n","                best, scores = rerank_candidates(candidates)\n","                generated_texts.append(best)\n","            else:\n","                # No reranking - single generation\n","                gen = generate_with_ensemble(\n","                    masked_text,\n","                    alpha_b=alpha_b,\n","                    alpha_e=alpha_e,\n","                    alpha_a=alpha_a,\n","                    temperature=temperature,\n","                    max_length=max_length,\n","                    top_p=top_p,\n","                    top_k=top_k\n","                )\n","                generated_texts.append(gen)\n","\n","        # Save outputs\n","        with open(output_file, 'w') as f:\n","            f.write('\\n'.join(generated_texts))\n","        print(f\"  Saved: {output_file}\")\n","\n","        # Step 4: Evaluate\n","        metrics = evaluate_all(texts, generated_texts)\n","\n","        # Store results\n","        result = {\n","            'data_type': data_type,\n","            'threshold': threshold,\n","            'alpha_b': alpha_b,\n","            'alpha_e': alpha_e,\n","            'alpha_a': alpha_a,\n","            'temperature': temperature,\n","            'ranking': ranking,\n","            'num_candidates': num_candidates if ranking else 1,\n","            **metrics\n","        }\n","        all_results.append(result)\n","\n","        print(f\"\\n  Results:\")\n","        print(f\"    Toxicity: {metrics['toxicity_orig']:.4f} → {metrics['toxicity_gen']:.4f}\")\n","        print(f\"    Perplexity: {metrics['perplexity_orig']:.2f} → {metrics['perplexity_gen']:.2f}\")\n","        print(f\"    BLEU: {metrics['bleu4']:.4f}\")\n","        print(f\"    BERTScore: {metrics['bertscore']:.4f}\")\n","\n","    # Save summary\n","    results_df = pd.DataFrame(all_results)\n","    summary_file = os.path.join(output_base, \"summary.csv\")\n","    results_df.to_csv(summary_file, index=False)\n","    print(f\"\\n{'='*80}\")\n","    print(f\"✓ PIPELINE COMPLETE\")\n","    print(f\"  Summary saved: {summary_file}\")\n","    print(f\"{'='*80}\")\n","\n","    return results_df\n","\n","print(\"✓ Main pipeline function loaded\")\n"]},{"cell_type":"markdown","metadata":{"id":"iyOjWNMKxV-k"},"source":["## Example Run\n","\n","Below are examples of running the T5-XDetox pipeline.\n","\n","**Small test:** Quick test on 50 examples\n","**Full run:** Complete evaluation on entire dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qCcsyDx2xV-k","executionInfo":{"status":"aborted","timestamp":1764747847006,"user_tz":480,"elapsed":38269,"user":{"displayName":"Ben He","userId":"03559440280945504495"}}},"outputs":[],"source":["#@title Example: Run T5-XDetox Pipeline\n","# Load models first\n","load_models()\n","\n","# Quick test on small subset\n","print(\"\\nRunning small test (50 examples)...\")\n","results_test = t5_detoxify(\n","    data_type=\"paradetox\",\n","    thresholds=[0.15, 0.20],\n","    num_examples=50,\n","    ranking=True,\n","    num_candidates=3,\n","    output_folder=\"test_run\"\n",")\n","\n","print(\"\\nTest Results:\")\n","print(results_test)\n","\n","# Uncomment below for full evaluation:\n","# results_full = t5_detoxify(\n","#     data_type=\"paradetox\",\n","#     thresholds=[0.10, 0.15, 0.20, 0.25, 0.30],\n","#     ranking=True,\n","#     num_candidates=5,\n","#     output_folder=\"full_eval\"\n","# )\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
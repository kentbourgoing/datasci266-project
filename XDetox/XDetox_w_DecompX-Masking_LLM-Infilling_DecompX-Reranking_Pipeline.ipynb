{"cells":[{"cell_type":"markdown","id":"82d4549b","metadata":{"id":"82d4549b"},"source":["# XDetox with DecompX Masking, LLM Infilling, and DecompX Reranking\n","\n","This notebook runs an XDetox variant with:\n","\n","1. **DecompX masking** (token-level toxicity attribution on RoBERTa).\n","2. **LLM infilling** using Mistral-7B-Instruct  \n","   (`mistralai/Mistral-7B-Instruct-v0.2`).\n","3. **DecompX-based reranking** of multiple LLM candidates, following the\n","   reranking strategy described in *“XDetox: Text Detoxification with Token-Level Toxicity Explanations”*.\n","\n","For each toxic input sentence, the goal is to pick **one final detoxified candidate** that:\n","\n","- Has the **lowest DecompX toxicity importance**,\n","- While being produced by an LLM that sees both the **raw toxic sentence** and the **DecompX-masked sentence**.\n","\n","Compared to the **DecompX-masking + LLM infilling + global reranking** pipeline, this notebook:\n","\n","- Uses the **same DecompX masking** and **same LLM infilling** setup,\n","- But replaces global reranking (toxicity + similarity + fluency) with **pure DecompX toxicity-based reranking**.\n","\n","---\n","\n","## Scoring: DecompX-Based Reranking\n","\n","The reranking stage follows the idea described in Section 2.3 of the XDetox paper:\n","\n","1. For each input, we generate multiple candidate detoxified sentences $s_j$ with the LLM.\n","2. For each candidate sentence $s_j$, we run **DecompX** and compute **token-level importance scores** with respect to toxicity.\n","3. For each candidate, we sum the importance scores of its tokens:\n","\n","   - Let $t_{i,j}$ be the $i$-th token in candidate sentence $s_j$.\n","   - Let $\\text{Importance}(t_{i,j})$ be its DecompX importance score (toxicity contribution).\n","\n","   The **total toxicity importance** of candidate $s_j$ is:\n","\n","   $$\n","   \\sum_{i=1}^{N_j} \\text{Importance}(t_{i,j})\n","   $$\n","\n","4. We choose the candidate with the **lowest total importance**:\n","\n","   $$\n","   s^* = \\arg\\min_{s_j} \\sum_{i=1}^{N_j} \\text{Importance}(t_{i,j})\n","   $$\n","\n","Intuitively:\n","\n","- A **lower sum** of token-level importance scores means **lower overall toxicity**.\n","- Reranking selects the candidate with the **minimum DecompX toxicity** among all candidates for that input.\n","\n","In the implementation, this is encapsulated in a helper like:\n","\n","- `rerank_candidates_decompx(sources, candidates, threshold, batch_size_mask)`\n","\n","which:\n","\n","- Flattens all candidates,\n","- Runs DecompX on each candidate sentence,\n","- Computes a DecompX-based toxicity score per candidate,\n","- Reshapes scores back to `[num_inputs, num_candidates]`,\n","- Picks the **index of the candidate with the lowest score** for each input.\n","\n","There is **no XLM-R / LaBSE / GPT-2 global scoring** in this notebook.  \n","All reranking is done by **DecompX**.\n","\n","---\n","\n","## DecompX Masking\n","\n","Masking uses the original XDetox **DecompX masker**:\n","\n","- Implementation: `rewrite.mask_orig.Masker`.\n","- Backend: RoBERTa with DecompX token-level toxicity attribution.\n","\n","For each input sentence:\n","\n","1. DecompX computes an **importance score** for each token based on its contribution to toxicity.\n","2. If the importance score of a token exceeds a **threshold** $t$, that token is considered **toxic**.\n","3. Such tokens are replaced by the `<mask>` token.\n","\n","For a given dataset and DecompX threshold $t$:\n","\n","- Inputs are loaded via `rewrite_example.get_data`.\n","- Masked outputs are written to:\n","\n","```text\n","  data/model_outputs/{output_folder}/{data_type}/DecompX_LLM_DecompX{t}/masked_inputs.txt\n","```\n","\n","(where the exact directory name may encode the DecompX threshold and the fact that this is a DecompX-masking + LLM pipeline.)\n","\n","These masked sentences are later fed into the LLM infiller.\n","\n","---\n","\n","## LLM Infilling (Mistral-7B-Instruct)\n","\n","After DecompX masking, we use **Mistral-7B-Instruct** as an **infilling model**.\n","\n","### Inputs to the LLM\n","\n","For each example we provide **both**:\n","\n","* **Toxic Sentence**: the original toxic sentence, unchanged.\n","* **Masked Sentence**: the DecompX-masked sentence, where toxic spans have been replaced by `<mask>`.\n","\n","The LLM prompt is structured along the lines of:\n","\n","```text\n","You are a helpful assistant trained to make toxic or offensive sentences more polite and respectful\n","while keeping their original meaning. ...\n","\n","Toxic Sentence: {raw_toxic}\n","Masked Sentence: {masked_by_DecompX}\n","Final Output:\n","```\n","\n","The instructions emphasize:\n","\n","* **Only fill in the `<mask>` tokens** in the Masked Sentence.\n","* Keep **all non-masked parts** of the Masked Sentence as close as possible to their original form.\n","* Preserve the **meaning and intent** of the Toxic Sentence.\n","* Use the **same language** as the Toxic Sentence.\n","* Return **only** the final detoxified sentence **inside one pair of square brackets**:\n","\n","```text\n","[Detoxified sentence here.]\n","```\n","\n","### Candidate generation\n","\n","For each $(\\text{toxic}, \\text{masked})$ pair, we ask Mistral for `num_candidates` completions:\n","\n","* The generation parameters include:\n","\n","  * `llm_sample` (sampling vs greedy),\n","  * `llm_temperature`,\n","  * `llm_top_p`,\n","  * `max_new_tokens`.\n","\n","* For each completion, we:\n","\n","  1. **Extract content inside the first `[ ... ]` block.**\n","  2. **Strip any remaining outer brackets.**\n","  3. **Normalize whitespace.**\n","\n","If a cleaned candidate is empty, we fall back to using the Masked Sentence.\n","\n","The result: for each input sentence, we obtain a list of `num_candidates` **LLM-generated candidates** to be reranked by DecompX.\n","\n","---\n","\n","## End-to-End Flow: Masking, LLM Infilling, DecompX Reranking\n","\n","For each dataset:\n","\n","1. **Subset selection**\n","\n","   * The script can run on the full dataset or only the first `num_examples` instances.\n","   * A subset file is written under:\n","\n","```text\n","datasets/_subsets/{data_type}/\n","```\n","\n","2. **DecompX masking (per threshold)**\n","\n","   * For each threshold $t$ in `thresholds`, we run DecompX masking.\n","   * Masked sentences are saved to:\n","\n","```text\n","data/model_outputs/{output_folder}/{data_type}/DecompX_LLM_DecompX{t}/masked_inputs.txt\n","```\n","\n","3. **LLM infilling (Mistral-7B-Instruct)**\n","\n","   * For each masked sentence and its corresponding toxic input, we call Mistral with the prompt described above.\n","   * We generate `num_candidates` candidates per input.\n","   * The raw LLM outputs are post-processed (bracket extraction, whitespace cleanup).\n","\n","4. **DecompX-based reranking**\n","\n","   * For each threshold $t$ and each input sentence, we apply DecompX to **all LLM candidates**.\n","\n","   * For each candidate $s_j$, we compute the total toxicity importance:\n","\n","$$\n","\\sum_{i=1}^{N_j} \\text{Importance}(t_{i,j})\n","$$\n","\n","   * We choose the candidate with the **lowest** total importance as the final output.\n","\n","   * For each run, we write:\n","\n","```text\n","data/model_outputs/{output_folder}/{data_type}/DecompX_LLM_DecompX{t}/{run_folder}/orig.txt\n","data/model_outputs/{output_folder}/{data_type}/DecompX_LLM_DecompX{t}/{run_folder}/gen.txt\n","```\n","\n","   where:\n","\n","   * `orig.txt`: original toxic inputs (one per line),\n","   * `gen.txt`: selected (reranked) LLM outputs (one per line),\n","   * `{run_folder}` encodes LLM generation hyperparameters.\n","\n","---\n","\n","## Evaluation\n","\n","If `run_eval=True`, the pipeline calls `evaluation.evaluate_all` to compute:\n","\n","* BERTScore (F1),\n","* MeaningBERT,\n","* BLEU-4,\n","* Toxicity (orig / gen),\n","* Perplexity (orig / gen).\n","\n","For each `(threshold, run_folder)` we write:\n","\n","```text\n","data/model_outputs/{output_folder}/{data_type}/DecompX_LLM_DecompX{t}/{run_folder}/gen_stats.txt\n","```\n","\n","The notebook also builds a **summary CSV per dataset** by scanning all `DecompX_LLM_DecompX*` directories:\n","\n","```text\n","data/model_outputs/{output_folder}/{data_type}/{data_type}.csv\n","```\n","\n","This CSV aggregates:\n","\n","* `threshold` (DecompX masking / reranking threshold),\n","* `folder` (run folder name),\n","* `bertscore`, `meaningbert`, `bleu4`,\n","* `perplexity_gen`, `perplexity_orig`,\n","* `toxicity_gen`, `toxicity_orig`.\n","\n","---\n","\n","## How to Use `detoxify()`\n","\n","A typical function signature for this notebook looks like:\n","\n","```python\n","def detoxify(\n","    data_type: str = \"paradetox\",\n","    output_folder: str = \"colab_run_decompx_mask_llm_decompx\",\n","    thresholds = (0.20,),\n","    echo: bool = False,\n","    num_examples: int = 100,        # None = full dataset\n","    overwrite_gen: bool = False,\n","    run_eval: bool = False,\n","    overwrite_eval: bool = False,\n","    skip_ref_eval: bool = False,\n","    # DecompX masking\n","    mask_batch_size: int = 10,\n","    # LLM infilling\n","    llm_sample: bool = True,\n","    llm_temperature: float = 0.7,\n","    llm_top_p: float = 0.95,\n","    max_new_tokens: int = 64,\n","    num_candidates: int = 3,        # LLM candidates per input\n",")\n","```\n","\n","### Key arguments\n","\n","#### Core I/O\n","\n","* `data_type`:\n","\n","  * Key in `data_configs`, for example:\n","\n","    * `\"paradetox\"`, `\"dynabench_val\"`, `\"dynabench_test\"`,\n","    * `\"jigsaw_toxic\"`, `\"microagressions_val\"`, `\"sbf_val\"`,\n","    * `\"appdia_original\"`, `\"appdia_discourse\"`, etc.\n","\n","* `output_folder`:\n","\n","  * Top-level directory under:\n","\n","    ```text\n","    data/model_outputs/{output_folder}/{data_type}/...\n","    ```\n","\n","* `num_examples`:\n","\n","  * `None`: use the full dataset.\n","  * Integer: run on the first `num_examples` examples.\n","\n","* `overwrite_gen`:\n","\n","  * `False`: if `gen.txt` already exists for a given `(threshold, run_folder)`, reuse existing generations.\n","  * `True`: regenerate and overwrite `gen.txt`.\n","\n","* `echo`:\n","\n","  * If `True`, print:\n","\n","    * Basic dataset info,\n","    * Example inputs,\n","    * Example masked sentences,\n","    * Example final outputs,\n","    * Per-run metrics (if `run_eval=True`).\n","\n","#### DecompX masking and thresholds\n","\n","* `thresholds`:\n","\n","  * Tuple of DecompX thresholds (e.g. `(0.15, 0.20, 0.25)`).\n","  * For each $t$, we:\n","\n","    * Run DecompX masking with threshold $t$,\n","    * Run LLM infilling,\n","    * Apply DecompX-based reranking (using the same DecompX mechanism).\n","\n","* `mask_batch_size`:\n","\n","  * Batch size used when running DecompX masking over inputs.\n","\n","#### LLM infilling (Mistral)\n","\n","* `llm_sample`:\n","\n","  * `True`: sampling.\n","  * `False`: deterministic decoding.\n","\n","* `llm_temperature`:\n","\n","  * Sampling temperature for Mistral (used when `llm_sample=True`).\n","\n","* `llm_top_p`:\n","\n","  * Top-p nucleus sampling cutoff.\n","\n","* `max_new_tokens`:\n","\n","  * Maximum number of new tokens generated per candidate.\n","\n","* `num_candidates`:\n","\n","  * Number of LLM candidates per input to be reranked by DecompX.\n","\n","#### Evaluation\n","\n","* `run_eval`:\n","\n","  * If `True`, compute evaluation metrics and write `gen_stats.txt` files.\n","\n","* `overwrite_eval`:\n","\n","  * If `True`, recompute metrics even if `gen_stats.txt` already exists.\n","\n","* `skip_ref_eval`:\n","\n","  * If `True`, skip reference-based evaluation (for example, perplexity on reference outputs).\n","\n","---\n","\n","## Example Calls\n","\n","### Quick sanity check (single threshold, small subset)\n","\n","```python\n","detoxify(\n","    data_type=\"paradetox\",\n","    output_folder=\"colab_run_decompx_mask_llm_decompx_demo_50_ex\",\n","    thresholds=(0.20,),\n","    echo=True,\n","    num_examples=50,          # small subset\n","    overwrite_gen=True,\n","    run_eval=True,            # BLEU / BERTScore / MeaningBERT / PPL / Toxicity\n","    overwrite_eval=True,\n","    skip_ref_eval=False,\n","    mask_batch_size=8,\n","    llm_sample=True,\n","    llm_temperature=0.7,\n","    llm_top_p=0.95,\n","    max_new_tokens=64,\n","    num_candidates=10,\n",")\n","```\n","\n","### Larger run (multiple thresholds, full dataset)\n","\n","```python\n","detoxify(\n","    data_type=\"paradetox\",\n","    output_folder=\"paradetox_decompx_mask_llm_decompx_full\",\n","    thresholds=(0.15, 0.20, 0.25),\n","    echo=True,\n","    num_examples=None,        # full dataset\n","    overwrite_gen=False,\n","    run_eval=True,\n","    overwrite_eval=False,\n","    skip_ref_eval=False,\n","    mask_batch_size=8,\n","    llm_sample=True,\n","    llm_temperature=0.7,\n","    llm_top_p=0.95,\n","    max_new_tokens=64,\n","    num_candidates=10,\n",")\n","```\n","\n","After running `detoxify`, you can inspect:\n","\n","* Per-threshold, per-run outputs:\n","\n","```text\n","data/model_outputs/{output_folder}/{data_type}/DecompX_LLM_DecompX{t}/{run_folder}/orig.txt\n","data/model_outputs/{output_folder}/{data_type}/DecompX_LLM_DecompX{t}/{run_folder}/gen.txt\n","data/model_outputs/{output_folder}/{data_type}/DecompX_LLM_DecompX{t}/{run_folder}/gen_stats.txt\n","```\n","\n","* Aggregated metrics:\n","\n","```text\n","data/model_outputs/{output_folder}/{data_type}/{data_type}.csv\n","```\n","\n","This notebook lets you compare:\n","\n","* **DecompX masking + LLM infilling + DecompX reranking** (this pipeline),\n","* Against:\n","\n","  * **DecompX masking + LLM infilling + global reranking**, and\n","  * The original **DecompX masking + MaRCo + DecompX/global reranking** pipelines,\n","\n","on the same datasets, using a DecompX-based toxicity selection rule.\n","\n"]},{"cell_type":"code","source":["#@title Mount Drive, Imports & locate XDetox\n","from google.colab import drive; drive.mount('/content/drive')\n","\n","import os, glob, re, sys, json, shutil, math\n","import numpy as np\n","import pandas as pd\n","from tqdm.auto import tqdm\n","from pathlib import Path\n","from subprocess import run, PIPE\n","import torch\n","import nltk\n","from typing import List\n","\n","# Try My Drive\n","candidate = \"/content/drive/MyDrive/w266 - Project/XDetox\"\n","print(\"Try MyDrive:\", candidate, \"->\", os.path.isdir(candidate))\n","\n","XDETOX_DIR = candidate\n","print(\"Using XDETOX_DIR:\", XDETOX_DIR)\n","assert os.path.isdir(XDETOX_DIR), f\"XDETOX_DIR does not exist: {XDETOX_DIR}\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kfBoQTrjtynY","executionInfo":{"status":"ok","timestamp":1764545497901,"user_tz":480,"elapsed":34618,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"e6279c8e-7786-457e-cda6-5a68445ab6b3"},"id":"kfBoQTrjtynY","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Try MyDrive: /content/drive/MyDrive/w266 - Project/XDetox -> True\n","Using XDETOX_DIR: /content/drive/MyDrive/w266 - Project/XDetox\n"]}]},{"cell_type":"code","source":["#@title Runtime setup (paths, cache, GPU)\n","HF_CACHE = os.path.join(XDETOX_DIR, \"cache\")\n","os.makedirs(HF_CACHE, exist_ok=True)\n","os.environ[\"TRANSFORMERS_CACHE\"] = HF_CACHE\n","\n","if XDETOX_DIR not in sys.path:\n","    sys.path.append(XDETOX_DIR)\n","\n","print(\"XDETOX_DIR:\", XDETOX_DIR)\n","print(\"TRANSFORMERS_CACHE:\", HF_CACHE)\n","print(\"CUDA available:\", torch.cuda.is_available())\n","if torch.cuda.is_available():\n","    print(\"GPU:\", torch.cuda.get_device_name(0))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ITPlTNBtzQx","executionInfo":{"status":"ok","timestamp":1764545498531,"user_tz":480,"elapsed":633,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"0748982e-bf0b-4c36-da65-89dcba08d3e3"},"id":"7ITPlTNBtzQx","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["XDETOX_DIR: /content/drive/MyDrive/w266 - Project/XDetox\n","TRANSFORMERS_CACHE: /content/drive/MyDrive/w266 - Project/XDetox/cache\n","CUDA available: True\n","GPU: NVIDIA A100-SXM4-80GB\n"]}]},{"cell_type":"code","source":["#@title Verify XDetox repo layout\n","for d in [\"rewrite\", \"evaluation\", \"datasets\"]:\n","    assert os.path.isdir(os.path.join(XDETOX_DIR, d)), f\"Missing folder: {d}\"\n","print(\"Repo folders OK.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MEy2TGYetzIb","executionInfo":{"status":"ok","timestamp":1764545498584,"user_tz":480,"elapsed":16,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"2efd54dc-e9a3-4164-f9de-db7b5111e343"},"id":"MEy2TGYetzIb","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Repo folders OK.\n"]}]},{"cell_type":"code","source":["#@title Install dependencies (restart runtime if major errors)\n","!pip -q install --upgrade pip setuptools wheel\n","!pip -q install \"transformers==4.41.2\" \"tokenizers==0.19.1\" \\\n","                \"datasets==2.19.0\" \"evaluate==0.4.1\" \\\n","                \"sacrebleu==2.4.1\" sacremoses ftfy nltk matplotlib pandas jedi \\\n","                sentencepiece\n","!pip -q install bert-score"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GeTzwxVDtzNn","executionInfo":{"status":"ok","timestamp":1764545521124,"user_tz":480,"elapsed":22538,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"1f0cc218-e538-4d46-ad94-09c65ba27d66"},"id":"GeTzwxVDtzNn","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.3.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["#@title Imports from transformers / rewrite\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from rewrite.mask_orig import Masker as Masker_single\n","from rewrite import rewrite_example as rx\n","import argparse as _argparse"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tfnuR2YVCmW9","executionInfo":{"status":"ok","timestamp":1764545534028,"user_tz":480,"elapsed":12900,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"9544c1b2-ddcd-44ff-af74-a9bb6358b78e"},"id":"tfnuR2YVCmW9","execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["#@title NLTK data\n","nltk.download(\"punkt\", quiet=True)\n","try:\n","    nltk.download(\"punkt_tab\", quiet=True)\n","except Exception:\n","    pass\n","print(\"NLTK ready\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y0Up7SKstzK9","executionInfo":{"status":"ok","timestamp":1764545534428,"user_tz":480,"elapsed":420,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"1237d064-2ad9-425a-d05f-3fc2aed1288a"},"id":"y0Up7SKstzK9","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["NLTK ready\n"]}]},{"cell_type":"code","source":["#@title Data configs\n","data_configs = {\n","    \"microagressions_val\": {\n","        \"data_path\": \"./datasets/microagressions/val.csv\",\n","    },\n","    \"microagressions_test\": {\n","        \"data_path\": \"./datasets/microagressions/test.csv\",\n","    },\n","    \"sbf_val\": {\n","        \"data_path\": \"./datasets/sbf/sbfdev.csv\",\n","    },\n","    \"sbf_test\": {\n","        \"data_path\": \"./datasets/sbf/sbftst.csv\",\n","    },\n","    \"dynabench_val\": {\n","        \"data_path\": \"./datasets/dynabench/db_dev.csv\",\n","    },\n","    \"dynabench_test\": {\n","        \"data_path\": \"./datasets/dynabench/db_test.csv\",\n","    },\n","    \"jigsaw_toxic\": {\n","        \"data_path\": \"./datasets/jigsaw_full_30/test_10k_toxic.txt\",\n","    },\n","    \"paradetox\": {\n","        \"data_path\": \"./datasets/paradetox/test_toxic_parallel.txt\",\n","    },\n","    \"appdia_original\": {\n","        \"data_path\": \"./datasets/appdia/original-annotated-data/original-test.tsv\",\n","    },\n","    \"appdia_discourse\": {\n","        \"data_path\": \"./datasets/appdia/discourse-augmented-data/discourse-test.tsv\",\n","    }\n","}\n","print(\"Datasets:\", \", \".join(data_configs.keys()))\n","\n","REPO = XDETOX_DIR"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7nBku39IuAgb","executionInfo":{"status":"ok","timestamp":1764545534477,"user_tz":480,"elapsed":48,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"9fa7fe11-7936-42e4-c4ee-95febc3bf717"},"id":"7nBku39IuAgb","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Datasets: microagressions_val, microagressions_test, sbf_val, sbf_test, dynabench_val, dynabench_test, jigsaw_toxic, paradetox, appdia_original, appdia_discourse\n"]}]},{"cell_type":"code","source":["#@title Helpers: subset data\n","def _abs_repo_path(rel: str) -> str:\n","    return os.path.join(REPO, rel.lstrip(\"./\"))\n","\n","def _ensure_dir(p: str):\n","    Path(p).mkdir(parents=True, exist_ok=True)\n","\n","def _subset_for_data_type(data_type, data_path, n, out_dir):\n","    \"\"\"\n","    Create a small subset file matching rewrite_example.get_data().\n","    Returns the new subset path (or original path if n is None/<=0).\n","    \"\"\"\n","    if n is None or n <= 0:\n","        return data_path\n","\n","    src = _abs_repo_path(data_path)\n","    _ensure_dir(out_dir)\n","\n","    if \"microagressions\" in data_path:\n","        df = pd.read_csv(src)\n","        sub = df.head(n)\n","        out = os.path.join(out_dir, os.path.basename(src))\n","        sub.to_csv(out, index=False)\n","        return out\n","\n","    if \"sbf\" in data_path:\n","        df = pd.read_csv(src)\n","        sub = df.head(n)\n","        out = os.path.join(out_dir, os.path.basename(src))\n","        sub.to_csv(out, index=False)\n","        return out\n","\n","    if \"dynabench\" in data_path:\n","        df = pd.read_csv(src)\n","        sub = df.head(n)\n","        out = os.path.join(out_dir, os.path.basename(src))\n","        sub.to_csv(out, index=False)\n","        return out\n","\n","    if any(k in data_path for k in [\"paradetox\", \"jigsaw\"]):\n","        if data_path.endswith(\".txt\"):\n","            with open(src, \"r\") as f:\n","                lines = [s.rstrip(\"\\n\") for s in f.readlines()]\n","            out = os.path.join(out_dir, os.path.basename(src))\n","            with open(out, \"w\") as g:\n","                for s in lines[:n]:\n","                    g.write(s + \"\\n\")\n","            return out\n","        elif data_path.endswith(\".csv\"):\n","            df = pd.read_csv(src).head(n)\n","            out = os.path.join(out_dir, os.path.basename(src))\n","            df.to_csv(out, index=False)\n","            return out\n","\n","    if \"appdia\" in data_path:\n","        df = pd.read_csv(src, sep=\"\\t\").head(n)\n","        out = os.path.join(out_dir, os.path.basename(src))\n","        df.to_csv(out, sep=\"\\t\", index=False)\n","        return out\n","\n","    out = os.path.join(out_dir, os.path.basename(src))\n","    shutil.copy(src, out)\n","    return out"],"metadata":{"id":"ToytrY0SuAjr","executionInfo":{"status":"ok","timestamp":1764545534486,"user_tz":480,"elapsed":5,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"ToytrY0SuAjr","execution_count":8,"outputs":[]},{"cell_type":"code","source":["#@title DecompX helpers for masking and reranking\n","\n","def _decompx_mask_texts(texts: List[str],\n","                        threshold: float = 0.20,\n","                        batch_size: int = 16) -> List[str]:\n","    \"\"\"\n","    Run DecompX Masker on a list of texts and return masked versions.\n","    \"\"\"\n","    if not texts:\n","        return []\n","\n","    masker = Masker_single()\n","    masked_all = []\n","    for i in tqdm(range(0, len(texts), batch_size),\n","                  desc=\"DecompX masking\", leave=False):\n","        batch = texts[i:i + batch_size]\n","        batch_out = masker.process_text(sentence=batch, threshold=threshold)\n","        masked_all.extend(batch_out)\n","    masker.release_model()\n","\n","    cleaned = [\n","        m.replace(\"<s>\", \"\").replace(\"</s>\", \"\").strip()\n","        for m in masked_all\n","    ]\n","    return cleaned\n","\n","def _decompx_toxicity_scores(texts: List[str],\n","                             threshold: float = 0.20,\n","                             batch_size: int = 16) -> np.ndarray:\n","    \"\"\"\n","    Approximate DecompX toxicity for texts by:\n","      score = (# of <mask> tokens DecompX inserts) / (# tokens)\n","\n","    Lower score => less DecompX-toxic.\n","    \"\"\"\n","    if not texts:\n","        return np.zeros((0,), dtype=float)\n","\n","    masked = _decompx_mask_texts(texts, threshold=threshold, batch_size=batch_size)\n","    scores = []\n","    for m in masked:\n","        num_masks = len(re.findall(r\"<mask>\", m))\n","        tokens = m.split()\n","        length = max(len(tokens), 1)\n","        scores.append(num_masks / length)\n","    return np.asarray(scores, dtype=float)\n","\n","def rerank_candidates_decompx(\n","    sources: List[str],\n","    candidates: List[List[str]],\n","    threshold: float = 0.20,\n","    batch_size_mask: int = 16,\n","):\n","    \"\"\"\n","    DecompX-based reranking (XDetox-style):\n","\n","    For each candidate sentence s_j:\n","      - apply DecompX\n","      - compute a toxicity score (here approximated by #masks / length)\n","      - select the candidate with MINIMUM score.\n","\n","    Returns:\n","      best_idx: np.ndarray (N,) with chosen candidate index per source\n","      details: dict with 'score' matrix shape [N, C]\n","    \"\"\"\n","    N = len(sources)\n","    assert len(candidates) == N, \"candidates length mismatch\"\n","\n","    if N == 0:\n","        return np.array([], dtype=int), {}\n","\n","    C_list = [len(c) for c in candidates]\n","    assert len(set(C_list)) == 1, \"All inputs must have same num_candidates\"\n","    C = C_list[0]\n","    if C == 0:\n","        raise ValueError(\"num_candidates must be >= 1\")\n","\n","    flat_cands = []\n","    flat_src_idx = []\n","    for i, cand_list in enumerate(candidates):\n","        for cand in cand_list:\n","            flat_cands.append(cand)\n","            flat_src_idx.append(i)\n","    flat_src_idx = np.array(flat_src_idx, dtype=int)\n","\n","    scores = _decompx_toxicity_scores(\n","        flat_cands,\n","        threshold=threshold,\n","        batch_size=batch_size_mask,\n","    )  # [N*C]\n","\n","    scores2 = scores.reshape(N, C)\n","    best_idx = np.argmin(scores2, axis=1)\n","\n","    details = {\n","        \"score\": scores2,\n","    }\n","    return best_idx, details\n"],"metadata":{"id":"PDRoBhdM8461","executionInfo":{"status":"ok","timestamp":1764545534516,"user_tz":480,"elapsed":25,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"PDRoBhdM8461","execution_count":9,"outputs":[]},{"cell_type":"code","source":["#@title Evaluation helpers (evaluate_all.py with MeaningBERT + toxicity)\n","def _parse_run_folder_name(folder_name):\n","    \"\"\"\n","    For this pipeline we simply treat all subfolders as valid runs.\n","    \"\"\"\n","    return True\n","\n","def _eval_with_toxicity(base_path, overwrite_eval=False, skip_ref=False,\n","                        tox_threshold=0.5, tox_batch_size=32):\n","    \"\"\"\n","    Call evaluation.evaluate_all on each gen folder under base_path.\n","    \"\"\"\n","    import sys as _sys, os as _os\n","    for folder in os.listdir(base_path):\n","        gen_dir = os.path.join(base_path, folder)\n","        if not os.path.isdir(gen_dir) or not _parse_run_folder_name(folder):\n","            continue\n","        orig_path = os.path.join(gen_dir, \"orig.txt\")\n","        gen_path  = os.path.join(gen_dir, \"gen.txt\")\n","        out_stats = os.path.join(gen_dir, \"gen_stats.txt\")\n","        if not (os.path.exists(orig_path) and os.path.exists(gen_path)):\n","            continue\n","        if os.path.exists(out_stats) and not overwrite_eval:\n","            continue\n","\n","        env = os.environ.copy()\n","        env[\"PYTHONPATH\"] = REPO + (\":\" + env.get(\"PYTHONPATH\",\"\") if env.get(\"PYTHONPATH\") else \"\")\n","        cmd = [\n","            _sys.executable, \"-m\", \"evaluation.evaluate_all\",\n","            \"--orig_path\", orig_path,\n","            \"--gen_path\",  gen_path,\n","            \"--tox_threshold\", str(tox_threshold),\n","            \"--tox_batch_size\", str(tox_batch_size),\n","        ]\n","        if skip_ref:\n","            cmd.append(\"--skip_ref\")\n","        print(\"Eval:\", \" \".join(cmd))\n","        res = run(cmd, cwd=REPO, env=env, stdout=PIPE, stderr=PIPE, text=True)\n","        if res.returncode != 0:\n","            print(res.stdout)\n","            print(res.stderr)\n","            res.check_returncode()\n","\n","def _safe_float(x):\n","    try:\n","        return float(x)\n","    except Exception:\n","        return float('nan')\n","\n","def _read_stats_file(path):\n","    out = {}\n","    with open(path, \"r\") as f:\n","        for line in f:\n","            if \":\" not in line:\n","                continue\n","            k, v = line.strip().split(\": \", 1)\n","            k = k.replace(\"(skipped)\", \"\").strip().lower()\n","            out[k] = _safe_float(v)\n","    return out\n","\n","def _aggregate_eval_csv(output_folder, data_type, base_out_dir):\n","    \"\"\"\n","    Aggregate eval metrics for DecompX-masking + LLM-infilling + DecompX-reranking.\n","\n","    Layout (absolute base_out_dir):\n","      base_out_dir/\n","        └── {data_type}/\n","            └── DecompX_LLM{thresh}/\n","                └── {run_folder}/\n","                    └── gen_stats.txt\n","    \"\"\"\n","    rows = []\n","\n","    root = os.path.join(base_out_dir, data_type)\n","    if not os.path.isdir(root):\n","        print(\"No evaluation directory found:\", root)\n","        return\n","\n","    for mask_dir in os.listdir(root):\n","        if not mask_dir.startswith(\"DecompX_LLM\"):\n","            continue\n","        thresh_str = mask_dir.replace(\"DecompX_LLM\", \"\")\n","        try:\n","            threshold = float(thresh_str)\n","        except Exception:\n","            threshold = np.nan\n","\n","        base_path = os.path.join(root, mask_dir)\n","        for folder in os.listdir(base_path):\n","            gen_dir = os.path.join(base_path, folder)\n","            stats_path = os.path.join(gen_dir, \"gen_stats.txt\")\n","            if not os.path.exists(stats_path):\n","                continue\n","            s = _read_stats_file(stats_path)\n","            rows.append({\n","                \"threshold\":        threshold,\n","                \"folder\":           folder,\n","                \"bertscore\":        s.get(\"bertscore\", np.nan),\n","                \"meaningbert\":      s.get(\"meaningbert\", np.nan),\n","                \"bleu4\":            s.get(\"bleu4\", np.nan),\n","                \"perplexity_gen\":   s.get(\"perplexity gen\", np.nan),\n","                \"perplexity_orig\":  s.get(\"perplexity orig\", np.nan),\n","                \"toxicity_gen\":     s.get(\"toxicity gen\", np.nan),\n","                \"toxicity_orig\":    s.get(\"toxicity orig\", np.nan),\n","            })\n","\n","    if rows:\n","        cols = [\n","            \"threshold\", \"folder\",\n","            \"bertscore\", \"meaningbert\", \"bleu4\",\n","            \"perplexity_gen\", \"perplexity_orig\",\n","            \"toxicity_gen\", \"toxicity_orig\",\n","        ]\n","        df = pd.DataFrame(rows)\n","        df = df[cols]\n","        out_csv = os.path.join(base_out_dir, data_type, f\"{data_type}.csv\")\n","        df.to_csv(out_csv, index=False)\n","        print(\"Wrote summary CSV:\", out_csv)\n","    else:\n","        print(\"No evaluation files found to summarize.\")"],"metadata":{"id":"u-7I09Uvqb8c","executionInfo":{"status":"ok","timestamp":1764545534527,"user_tz":480,"elapsed":6,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"u-7I09Uvqb8c","execution_count":10,"outputs":[]},{"cell_type":"code","source":["#@title LLM infilling (Mistral-7B-Instruct) for masked sentences\n","USE_LLM_GPU = True\n","DEVICE_LLM = torch.device(\"cuda\" if USE_LLM_GPU and torch.cuda.is_available() else \"cpu\")\n","print(\"LLM infiller device:\", DEVICE_LLM)\n","\n","LLM_MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n","_LLM_TOKENIZER = None\n","_LLM_MODEL = None\n","\n","INFILL_SYSTEM_PROMPT = \"\"\"You are a helpful assistant trained to make toxic or offensive sentences\n","more polite and respectful by INFILLING <mask> tokens in a masked sentence.\n","\n","You are given:\n","- Toxic Sentence: the original sentence, which may contain offensive language.\n","- Masked Sentence: the same sentence, but all toxic spans are replaced by <mask>.\n","\n","Your rules:\n","1. Only replace <mask> tokens in the Masked Sentence with polite, non-toxic alternatives.\n","2. Do NOT change any other words or punctuation from the Masked Sentence,\n","   except for small grammar fixes needed after infilling.\n","3. Preserve the original meaning and intent of the Toxic Sentence as much as possible.\n","4. Keep the same language as the Toxic Sentence.\n","5. If the Masked Sentence has no <mask> tokens, return it unchanged.\n","\n","Output rules (very strict):\n","- Return ONLY the final detoxified sentence inside ONE pair of square brackets, like:\n","  [You are such a rude person, nobody wants to hear your opinion.]\n","- Do NOT print anything before or after the brackets.\n","- Do NOT add explanations, comments, or extra lines.\n","- Do NOT include additional '[' or ']' characters inside the sentence.\n","\"\"\"\n","\n","INFILL_FEW_SHOT = \"\"\"Toxic Sentence: You're such a stupid idiot, nobody wants to hear your crap.\n","Masked Sentence: You're such a <mask>, nobody wants to hear your <mask>.\n","Step 1 - Decide polite replacements for <mask>: \"rude person\", \"opinion\"\n","Step 2 - Insert them into the Masked Sentence, keeping all other tokens:\n","You're such a rude person, nobody wants to hear your opinion.\n","Final Output: [You're such a rude person, nobody wants to hear your opinion.]\"\"\"\n","\n","def _lazy_load_llm_infiller():\n","    global _LLM_MODEL, _LLM_TOKENIZER\n","    if _LLM_MODEL is not None and _LLM_TOKENIZER is not None:\n","        return\n","    print(f\"Loading LLM infiller: {LLM_MODEL_NAME} on {DEVICE_LLM} ...\")\n","    _LLM_TOKENIZER = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n","    _LLM_MODEL = AutoModelForCausalLM.from_pretrained(\n","        LLM_MODEL_NAME,\n","        torch_dtype=torch.float16 if DEVICE_LLM.type == \"cuda\" else torch.float32,\n","        device_map=None,\n","    ).to(DEVICE_LLM)\n","    _LLM_MODEL.eval()\n","    print(\"LLM infiller loaded.\")\n","\n","def _extract_bracket_content(text: str) -> str:\n","    \"\"\"\n","    Extract content inside the first [ ... ] block.\n","    If missing ']', take everything after '['.\n","    If no '[', return the whole string.\n","    \"\"\"\n","    text = text.strip()\n","    m = re.search(r\"\\[([^\\]]*)\\]\", text, flags=re.DOTALL)\n","    if m:\n","        return m.group(1).strip()\n","    if \"[\" in text:\n","        return text.split(\"[\", 1)[1].strip()\n","    return text\n","\n","def _cleanup_llm_output(s: str) -> str:\n","    \"\"\"\n","    Remove outer brackets if still present and normalize whitespace.\n","    \"\"\"\n","    s = s.strip()\n","    if s.startswith(\"[\") and s.endswith(\"]\") and len(s) > 2:\n","        s = s[1:-1].strip()\n","    else:\n","        if s.startswith(\"[\"):\n","            s = s[1:].strip()\n","        if s.endswith(\"]\"):\n","            s = s[:-1].strip()\n","    s = re.sub(r\"\\s+\", \" \", s).strip()\n","    return s\n","\n","@torch.no_grad()\n","def llm_infill_candidates(\n","    toxic_sentences: List[str],\n","    masked_sentences: List[str],\n","    num_candidates: int = 3,\n","    temperature: float = 0.7,\n","    top_p: float = 0.95,\n","    max_new_tokens: int = 64,\n","    sample: bool = True,\n",") -> List[List[str]]:\n","    \"\"\"\n","    For each (toxic, masked) pair, generate `num_candidates` detoxified candidates\n","    by infilling <mask> tokens with Mistral-7B-Instruct.\n","\n","    Returns: list of length N; each element is a list of length num_candidates.\n","    \"\"\"\n","    _lazy_load_llm_infiller()\n","    assert len(toxic_sentences) == len(masked_sentences), \"length mismatch\"\n","\n","    all_candidates: List[List[str]] = []\n","\n","    for idx in tqdm(range(len(toxic_sentences)), desc=\"LLM infilling\", leave=False):\n","        toxic = toxic_sentences[idx]\n","        masked = masked_sentences[idx]\n","\n","        messages = [\n","            {\n","                \"role\": \"system\",\n","                \"content\": INFILL_SYSTEM_PROMPT + \"\\n\\nBelow is an example:\\n\" + INFILL_FEW_SHOT,\n","            },\n","            {\n","                \"role\": \"user\",\n","                \"content\": (\n","                    f\"Toxic Sentence: {toxic}\\n\"\n","                    f\"Masked Sentence: {masked}\\n\"\n","                    \"Final Output:\"\n","                ),\n","            },\n","        ]\n","        try:\n","            prompt = _LLM_TOKENIZER.apply_chat_template(\n","                messages,\n","                tokenize=False,\n","                add_generation_prompt=True,\n","            )\n","        except Exception:\n","            prompt = (\n","                INFILL_SYSTEM_PROMPT\n","                + \"\\n\\nExample:\\n\"\n","                + INFILL_FEW_SHOT\n","                + \"\\n\\nToxic Sentence: \"\n","                + toxic\n","                + \"\\nMasked Sentence: \"\n","                + masked\n","                + \"\\nFinal Output:\"\n","            )\n","\n","        inputs = _LLM_TOKENIZER(prompt, return_tensors=\"pt\").to(DEVICE_LLM)\n","        gen = _LLM_MODEL.generate(\n","            **inputs,\n","            max_new_tokens=max_new_tokens,\n","            do_sample=sample,\n","            temperature=temperature if sample else 0.0,\n","            top_p=top_p,\n","            num_return_sequences=num_candidates,\n","            pad_token_id=_LLM_TOKENIZER.eos_token_id,\n","        )\n","        input_len = inputs[\"input_ids\"].shape[1]\n","\n","        cands_for_this = []\n","        for k in range(num_candidates):\n","            gen_text = _LLM_TOKENIZER.decode(\n","                gen[k][input_len:], skip_special_tokens=True\n","            )\n","            detox = _extract_bracket_content(gen_text)\n","            detox = _cleanup_llm_output(detox)\n","            if not detox:\n","                detox = masked  # fallback\n","            cands_for_this.append(detox)\n","\n","        all_candidates.append(cands_for_this)\n","\n","    return all_candidates\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nG355GLlqcp-","executionInfo":{"status":"ok","timestamp":1764545534552,"user_tz":480,"elapsed":21,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"b4b702e3-6317-40ff-f361-acddee65dfff"},"id":"nG355GLlqcp-","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["LLM infiller device: cuda\n"]}]},{"cell_type":"code","source":["#@title Masking + LLM infilling + DecompX reranking\n","\n","def _bool2str(x: bool) -> str:\n","    return \"T\" if x else \"F\"\n","\n","def _build_llm_run_folder_name(\n","    temperature: float,\n","    sample: bool,\n","    top_p: float,\n","    max_new_tokens: int,\n","    num_candidates: int,\n","):\n","    return (\n","        \"llmtemp\" + str(temperature) +\n","        \"_sample\" + _bool2str(sample) +\n","        \"_topp\" + str(top_p) +\n","        \"_maxnew\" + str(max_new_tokens) +\n","        \"_ncand\" + str(num_candidates)\n","    )\n","\n","def _run_decompx_masking_llm_infill_and_decompx_reranking_for_threshold(\n","    data_type,\n","    subset_path,\n","    thresh,\n","    base_out_rel,\n","    mask_batch_size,\n","    llm_sample,\n","    llm_temperature,\n","    llm_top_p,\n","    max_new_tokens,\n","    num_candidates,\n","    decompx_threshold,\n","    overwrite_gen=False,\n","    inputs=None,\n","    rerank_batch_size: int = 16,\n","    echo: bool = False,\n","):\n","    \"\"\"\n","    For one DecompX threshold:\n","      - Mask with DecompX (on raw toxic inputs).\n","      - Use Mistral LLM to infill <mask> tokens and generate num_candidates per input.\n","      - Rerank candidates with DecompX-based toxicity (lowest score wins).\n","      - Save orig.txt / gen.txt under:\n","          data/model_outputs/{output_folder}/{data_type}/DecompX_LLM{thresh}/{run_folder}/\n","    \"\"\"\n","    # Load inputs if not provided\n","    if inputs is None:\n","        args_data = _argparse.Namespace(data_type=data_type, data_path=subset_path)\n","        inputs = rx.get_data(args_data)\n","    print(f\"#inputs at thresh={thresh}: {len(inputs)}\")\n","\n","    # Paths\n","    mask_dir = f\"DecompX_LLM{abs(thresh):g}\" if thresh != 0 else \"DecompX_LLM0.0\"\n","    cur_rel = os.path.join(base_out_rel, data_type, mask_dir)\n","    cur_abs = os.path.join(REPO, cur_rel)\n","    _ensure_dir(cur_abs)\n","\n","    masked_file = os.path.join(cur_abs, \"masked_inputs.txt\")\n","\n","    # DecompX masking (reuse if exists)\n","    if not os.path.exists(masked_file):\n","        print(f\"Running DecompX masking (threshold={thresh:.2f}) to create masked_inputs.txt ...\")\n","        decoded_mask_inputs = _decompx_mask_texts(\n","            inputs, threshold=thresh, batch_size=mask_batch_size\n","        )\n","        with open(masked_file, \"w\") as f:\n","            for d in decoded_mask_inputs:\n","                f.write(re.sub(r\"\\s+\", \" \", d).strip() + \"\\n\")\n","    else:\n","        with open(masked_file, \"r\") as f:\n","            decoded_mask_inputs = [s.strip() for s in f.readlines()]\n","        print(\"Reusing existing masked_inputs.txt\")\n","\n","    assert len(decoded_mask_inputs) == len(inputs), \"Masked vs inputs mismatch\"\n","\n","    if echo:\n","        print(\"\\n[echo] Example masked inputs (first up to 3):\")\n","        for i, m in enumerate(decoded_mask_inputs[:3]):\n","            print(f\"  masked[{i}]: {m}\")\n","\n","    # Run folder for this LLM configuration\n","    run_folder = _build_llm_run_folder_name(\n","        llm_temperature, llm_sample, llm_top_p, max_new_tokens, num_candidates\n","    )\n","    final_abs = os.path.join(cur_abs, run_folder)\n","    gen_txt = os.path.join(final_abs, \"gen.txt\")\n","    orig_txt = os.path.join(final_abs, \"orig.txt\")\n","\n","    if os.path.exists(gen_txt) and not overwrite_gen:\n","        print(\"Generation already exists at:\", gen_txt, \"— skipping generation.\")\n","        _ensure_dir(final_abs)\n","        with open(gen_txt, \"r\") as f:\n","            best_generations = [s.strip() for s in f.readlines()]\n","        return inputs, decoded_mask_inputs, best_generations, final_abs\n","\n","    _ensure_dir(final_abs)\n","\n","    # LLM infilling: generate candidates\n","    print(f\"LLM infilling: generating {num_candidates} candidates per input (sampling={llm_sample})\")\n","    all_candidates = llm_infill_candidates(\n","        toxic_sentences=inputs,\n","        masked_sentences=decoded_mask_inputs,\n","        num_candidates=num_candidates,\n","        temperature=llm_temperature,\n","        top_p=llm_top_p,\n","        max_new_tokens=max_new_tokens,\n","        sample=llm_sample,\n","    )\n","\n","    # DecompX-based reranking\n","    print(f\"DecompX reranking of LLM candidates (threshold={decompx_threshold:.2f}) ...\")\n","    best_idx, details = rerank_candidates_decompx(\n","        sources=inputs,\n","        candidates=all_candidates,\n","        threshold=decompx_threshold,\n","        batch_size_mask=rerank_batch_size,\n","    )\n","    best_generations = [\n","        all_candidates[i][best_idx[i]] for i in range(len(inputs))\n","    ]\n","\n","    if echo:\n","        print(\"\\n[echo] Example detoxified outputs (first up to 3):\")\n","        for i, g in enumerate(best_generations[:3]):\n","            print(f\"  detox[{i}]: {g}\")\n","\n","    # Save orig + chosen gen\n","    with open(orig_txt, \"w\") as f:\n","        for l in inputs:\n","            f.write(re.sub(r\"\\s+\", \" \", l).strip() + \"\\n\")\n","    with open(gen_txt, \"w\") as f:\n","        for l in best_generations:\n","            f.write(re.sub(r\"\\s+\", \" \", l).strip() + \"\\n\")\n","\n","    print(\"Saved:\", orig_txt)\n","    print(\"Saved:\", gen_txt)\n","\n","    return inputs, decoded_mask_inputs, best_generations, final_abs"],"metadata":{"id":"U5oOUWRYuA6E","executionInfo":{"status":"ok","timestamp":1764545534609,"user_tz":480,"elapsed":15,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"U5oOUWRYuA6E","execution_count":12,"outputs":[]},{"cell_type":"code","source":["#@title `detoxify()` — DecompX masking + LLM infilling + DecompX reranking + optional eval\n","\n","def detoxify(\n","    data_type: str = \"paradetox\",\n","    output_folder: str = \"colab_run_decompx_mask_llm_decompx\",\n","    thresholds = (0.20,),\n","    echo: bool = False,\n","    num_examples: int = 100,        # None = full dataset\n","    overwrite_gen: bool = False,\n","    run_eval: bool = False,\n","    overwrite_eval: bool = False,\n","    skip_ref_eval: bool = False,\n","    # DecompX params\n","    mask_batch_size: int = 10,\n","    rerank_batch_size: int = 16,\n","    decompx_rerank_threshold: float = None,  # if None, use each t\n","    # LLM infilling params\n","    llm_sample: bool = True,\n","    llm_temperature: float = 0.7,\n","    llm_top_p: float = 0.95,\n","    max_new_tokens: int = 64,\n","    num_candidates: int = 3,\n","):\n","    \"\"\"\n","    Run XDetox with:\n","      - DecompX masking (token-level) on raw toxic inputs,\n","      - LLM infilling (Mistral-7B-Instruct) to fill <mask> tokens,\n","      - DecompX-based reranking over LLM candidates,\n","      - optional evaluation via evaluation.evaluate_all.\n","\n","    Outputs are stored under:\n","      data/model_outputs/{output_folder}/{data_type}/DecompX_LLM{thresh}/{run_folder}/\n","    \"\"\"\n","    assert data_type in data_configs, f\"Unknown data_type: {data_type}\"\n","    cfg = data_configs[data_type].copy()\n","\n","    if num_candidates < 1:\n","        raise ValueError(\"num_candidates must be >= 1\")\n","\n","    base_out_rel = os.path.join(\"data\", \"model_outputs\", output_folder)\n","    base_out_abs = os.path.join(REPO, base_out_rel)\n","    _ensure_dir(base_out_abs)\n","\n","    # subset path (file)\n","    original_data_path = cfg[\"data_path\"]\n","    subset_dir = os.path.join(REPO, \"datasets\", \"_subsets\", data_type)\n","    _ensure_dir(subset_dir)\n","    subset_path = _subset_for_data_type(\n","        data_type, original_data_path, num_examples, subset_dir\n","    )\n","\n","    # Load inputs once for echo and reuse\n","    args_data = _argparse.Namespace(data_type=data_type, data_path=subset_path)\n","    inputs = rx.get_data(args_data)\n","    num_inputs = len(inputs)\n","\n","    if echo:\n","        print(\"=\" * 80)\n","        print(f\"[echo] Dataset: {data_type}\")\n","        print(f\"[echo] Subset path: {subset_path}\")\n","        print(f\"[echo] Output base: {base_out_abs}\")\n","        print(f\"[echo] Number of examples to detoxify: {num_inputs}\")\n","        print(f\"[echo] Thresholds (DecompX masking): {', '.join(f'{t:.2f}' for t in thresholds)}\")\n","        print(f\"[echo] LLM: temperature={llm_temperature}, top_p={llm_top_p}, \"\n","              f\"sample={llm_sample}, max_new_tokens={max_new_tokens}\")\n","        print(f\"[echo] num_candidates per input: {num_candidates}\")\n","        print(\"\\n[echo] Example inputs (first up to 3):\")\n","        for i, s in enumerate(inputs[:3]):\n","            print(f\"  input[{i}]: {s}\")\n","        print(\"=\" * 80)\n","\n","    # Run for each DecompX masking threshold\n","    last_run_dir = None\n","    for t in thresholds:\n","        print(\"=\" * 60)\n","        print(f\"DecompX masking threshold = {t:.2f}\")\n","        effective_rerank_t = decompx_rerank_threshold if decompx_rerank_threshold is not None else t\n","\n","        inputs, masked_inputs, best_generations, run_dir = \\\n","            _run_decompx_masking_llm_infill_and_decompx_reranking_for_threshold(\n","                data_type=data_type,\n","                subset_path=subset_path,\n","                thresh=t,\n","                base_out_rel=base_out_rel,\n","                mask_batch_size=mask_batch_size,\n","                llm_sample=llm_sample,\n","                llm_temperature=llm_temperature,\n","                llm_top_p=llm_top_p,\n","                max_new_tokens=max_new_tokens,\n","                num_candidates=num_candidates,\n","                decompx_threshold=effective_rerank_t,\n","                overwrite_gen=overwrite_gen,\n","                inputs=inputs,\n","                rerank_batch_size=rerank_batch_size,\n","                echo=echo,\n","            )\n","        last_run_dir = run_dir\n","\n","        if run_eval:\n","            mask_dir = f\"DecompX_LLM{abs(t):g}\" if t != 0 else \"DecompX_LLM0.0\"\n","            base_path = os.path.join(base_out_abs, data_type, mask_dir)\n","            _eval_with_toxicity(\n","                base_path,\n","                overwrite_eval=overwrite_eval,\n","                skip_ref=skip_ref_eval,\n","                tox_threshold=0.5,\n","                tox_batch_size=32,\n","            )\n","\n","            if echo:\n","                run_folder = os.path.basename(run_dir)\n","                stats_path = os.path.join(base_path, run_folder, \"gen_stats.txt\")\n","                if os.path.exists(stats_path):\n","                    stats = _read_stats_file(stats_path)\n","                    print(\"\\n[echo] Evaluation metrics for this run \"\n","                          f\"(t={t:.2f}):\")\n","                    metric_keys = [\n","                        (\"bertscore\",        \"BERTScore\"),\n","                        (\"meaningbert\",      \"MeaningBERT\"),\n","                        (\"bleu4\",            \"BLEU-4\"),\n","                        (\"perplexity gen\",   \"Perplexity (gen)\"),\n","                        (\"perplexity orig\",  \"Perplexity (orig)\"),\n","                        (\"toxicity gen\",     \"Toxicity (gen)\"),\n","                        (\"toxicity orig\",    \"Toxicity (orig)\"),\n","                    ]\n","                    for key, label in metric_keys:\n","                        val = stats.get(key, None)\n","                        if isinstance(val, float) and math.isnan(val):\n","                            continue\n","                        if val is None:\n","                            continue\n","                        print(f\"  {label}: {val:.4f}\")\n","                else:\n","                    print(f\"[echo] gen_stats.txt not found at {stats_path}\")\n","\n","    # Summarize across thresholds\n","    if run_eval:\n","        _aggregate_eval_csv(\n","            output_folder,\n","            data_type,\n","            os.path.join(REPO, \"data\", \"model_outputs\", output_folder),\n","        )\n"],"metadata":{"id":"oqBLx5OSuA72","executionInfo":{"status":"ok","timestamp":1764545534638,"user_tz":480,"elapsed":26,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"oqBLx5OSuA72","execution_count":13,"outputs":[]},{"cell_type":"code","source":["#@title Example run — paradetox, DecompX masking + LLM infilling + DecompX reranking\n","\n","# Example (small subset; adjust as needed):\n","# detoxify(\n","#     data_type=\"paradetox\",\n","#     output_folder=\"colab_run_decompx_mask_llm_decompx_demo_50_examples\",\n","#     thresholds=(0.20,),\n","#     echo=True,\n","#     num_examples=50,\n","#     overwrite_gen=True,\n","#     run_eval=True,\n","#     overwrite_eval=True,\n","#     skip_ref_eval=False,\n","#     mask_batch_size=8,\n","#     rerank_batch_size=16,\n","#     llm_sample=True,\n","#     llm_temperature=0.7,\n","#     llm_top_p=0.95,\n","#     max_new_tokens=64,\n","#     num_candidates=10,\n","# )"],"metadata":{"id":"u5LlySYquA9g","collapsed":true,"executionInfo":{"status":"ok","timestamp":1764545534648,"user_tz":480,"elapsed":5,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"u5LlySYquA9g","execution_count":14,"outputs":[]},{"cell_type":"code","source":["detoxify(\n","    data_type=\"paradetox\",\n","    output_folder=\"XDetox_w_DecompX-Masking_LLM-Infilling_DecompX-Reranking_Pipeline\",\n","    thresholds=(0.20,),\n","    echo=True,\n","    num_examples=1000,\n","    overwrite_gen=True,\n","    run_eval=True,\n","    overwrite_eval=True,\n","    skip_ref_eval=False,\n","    mask_batch_size=8,\n","    rerank_batch_size=16,\n","    llm_sample=True,\n","    llm_temperature=0.7,\n","    llm_top_p=0.95,\n","    max_new_tokens=64,\n","    num_candidates=10,\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["71da2c25a0bf47078c758ed59ef4bc28","ccebb6db12a0418a90189c29c1d960ef","bde93d01ab5b4eb2b216a7953b41931e","59af0aa5311644deb97845669ba73a52","1a6df16002b54f0f9bded2f156637ce0","b43394c03fd84a4380ae84b1cb1523da","ceb67f9b82b54d319e120c0d309d2885","34e0316d7c9b4b3d86dcf7035e06d1c1","efcb58a662224f108c8c1638744e0fe7","30d9e7d8a6ff4c349468d234c5b147c2","74634886ae894a5483a65ca0f7cea548","35c5a71dc5b44c1aa3ce08c1836a32be","faf9af58e8ee496abc7869284d273e30","be2f24f320ce4b81b6eea28bb479287d","4c14692fc6b64f3db7401b350fda7698","955ae2dc83ed4e36b3a6eef238ee6d5e","b47186463c4f4331abd72607498ceec4","6156ce5bc4114a5cb027106a1e6e843f","f456ec232c1649f39607bd025882110e","6bb27cf92319458293255e789a938d75","310b5fb84f36421a8e207324e3c0faf4","f30d9f931ad84451b4a08d9631d42959","efe6108d8690407cab868dc5a9b90141","addc396bf10947af8613fe51d219a50e","a23db48efb224909b3d5a2df332c55d1","059c133b7ee04684b53fe7cc118eb09f","7dd6f07942bf490dbfce7f878bab36ee","91ee2ae926274595a9918a52e832468e","9a0c7eac7abe4461b2ba236e8165cb67","dedf18d9aafa4d92ac0c28b91b7a2fdc","90f3abd167cb4abeb386f7aad213a300","cd97eb140cf242bb9680f1702d3544e1","0db2cf8137fe409985a72b9432385a30","ea1c62620985487f86b5eca38c5d8712","e486adb41aaf4526831716b6b655fd7d","466759ae0d4a411cbf5fa7eb55b5a1f0","60fb26d0acf841f1bb76520a8be4847a","905dfb8f2b734625b3aac9114e283d79","09fa8278eafc4c28a2c0724caf12f764","725dd073f0574d51a2cd58e54426267a","5dab7203ab364959a0f1237c24c7ffb4","d94273b08da242739a30f6012e5dc58d","102b8e7750754cb1bd42633201e2af62","4c3b1ab8a8e9467295fec8bdedbd686e","f3f9f1d51bcf4b009becaaaa03d62a13","f4710f9eb9e54742a5a071227848048a","8ef9e9f677964c44ba7c0213696dd774","49127e0052dd4b62800c755801a65b78","76ff9cb7208c47459f2ad8ea5d6ca642","7908ea2adb4a4866a240736b0ab19fd1","5a745e7cd0574c488b00c8c459c74f2a","397038c370fc46fc876a559dee2cbae8","12fc2ace10614a23ac3769468516997c","6b680ab8f92846b7829a973d5374b811","f360eff002c848549ba42229210e6b09"]},"id":"O2S9fX-adatU","executionInfo":{"status":"ok","timestamp":1764548221756,"user_tz":480,"elapsed":2687104,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}},"outputId":"53624e95-d426-4739-84bd-4f43a1d062bd"},"id":"O2S9fX-adatU","execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","[echo] Dataset: paradetox\n","[echo] Subset path: /content/drive/MyDrive/w266 - Project/XDetox/datasets/_subsets/paradetox/test_toxic_parallel.txt\n","[echo] Output base: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_DecompX-Masking_LLM-Infilling_DecompX-Reranking_Pipeline\n","[echo] Number of examples to detoxify: 671\n","[echo] Thresholds (DecompX masking): 0.20\n","[echo] LLM: temperature=0.7, top_p=0.95, sample=True, max_new_tokens=64\n","[echo] num_candidates per input: 10\n","\n","[echo] Example inputs (first up to 3):\n","  input[0]: . or the loud ass one - thousand ton beast roaring towards you howling its horn .\n","  input[1]: mandated  and \" right fucking now \" would be good .\n","  input[2]: * neither * of my coworkers gave a shit when it came time to ditch mitch . ugh .\n","================================================================================\n","============================================================\n","DecompX masking threshold = 0.20\n","#inputs at thresh=0.2: 671\n","Running DecompX masking (threshold=0.20) to create masked_inputs.txt ...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n","Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n","You are not authenticated with the Hugging Face Hub in this notebook.\n","If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n","  warnings.warn(\n","Some weights of the model checkpoint at s-nlp/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"display_data","data":{"text/plain":["DecompX masking:   0%|          | 0/84 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71da2c25a0bf47078c758ed59ef4bc28"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:1052: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","[echo] Example masked inputs (first up to 3):\n","  masked[0]: . or the loud <mask> one - thousand ton beast roaring towards you howling its horn .\n","  masked[1]: mandated  and \" right <mask> now \" would be good .\n","  masked[2]: * neither * of my coworkers gave a <mask> when it came time to ditch mitch . ugh .\n","LLM infilling: generating 10 candidates per input (sampling=True)\n","Loading LLM infiller: mistralai/Mistral-7B-Instruct-v0.2 on cuda ...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35c5a71dc5b44c1aa3ce08c1836a32be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efe6108d8690407cab868dc5a9b90141"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["LLM infiller loaded.\n"]},{"output_type":"display_data","data":{"text/plain":["LLM infilling:   0%|          | 0/671 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea1c62620985487f86b5eca38c5d8712"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["DecompX reranking of LLM candidates (threshold=0.20) ...\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at s-nlp/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"display_data","data":{"text/plain":["DecompX masking:   0%|          | 0/420 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3f9f1d51bcf4b009becaaaa03d62a13"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:1052: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","[echo] Example detoxified outputs (first up to 3):\n","  detox[0]: or the loud person one - thousand ton beast roaring towards you howling its horn.\n","  detox[1]: mandated and \" right now \" would be good .\n","  detox[2]: neither of my coworkers showed interest when it came time to ditch mitch . ugh .\n","Saved: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_DecompX-Masking_LLM-Infilling_DecompX-Reranking_Pipeline/paradetox/DecompX_LLM0.2/llmtemp0.7_sampleT_topp0.95_maxnew64_ncand10/orig.txt\n","Saved: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_DecompX-Masking_LLM-Infilling_DecompX-Reranking_Pipeline/paradetox/DecompX_LLM0.2/llmtemp0.7_sampleT_topp0.95_maxnew64_ncand10/gen.txt\n","Eval: /usr/bin/python3 -m evaluation.evaluate_all --orig_path /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_DecompX-Masking_LLM-Infilling_DecompX-Reranking_Pipeline/paradetox/DecompX_LLM0.2/llmtemp0.7_sampleT_topp0.95_maxnew64_ncand10/orig.txt --gen_path /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_DecompX-Masking_LLM-Infilling_DecompX-Reranking_Pipeline/paradetox/DecompX_LLM0.2/llmtemp0.7_sampleT_topp0.95_maxnew64_ncand10/gen.txt --tox_threshold 0.5 --tox_batch_size 32\n","\n","[echo] Evaluation metrics for this run (t=0.20):\n","  BERTScore: 0.9384\n","  MeaningBERT: 66.1640\n","  BLEU-4: 82.8620\n","  Perplexity (gen): 200.2900\n","  Perplexity (orig): 273.7500\n","  Toxicity (gen): 0.1707\n","  Toxicity (orig): 0.9253\n","Wrote summary CSV: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_DecompX-Masking_LLM-Infilling_DecompX-Reranking_Pipeline/paradetox/paradetox.csv\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"VgZRbEBED8w4","executionInfo":{"status":"ok","timestamp":1764548221765,"user_tz":480,"elapsed":14,"user":{"displayName":"Kent Bourgoing","userId":"01773768369839516808"}}},"id":"VgZRbEBED8w4","execution_count":15,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"A100","machine_shape":"hm"},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"71da2c25a0bf47078c758ed59ef4bc28":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ccebb6db12a0418a90189c29c1d960ef","IPY_MODEL_bde93d01ab5b4eb2b216a7953b41931e","IPY_MODEL_59af0aa5311644deb97845669ba73a52"],"layout":"IPY_MODEL_1a6df16002b54f0f9bded2f156637ce0"}},"ccebb6db12a0418a90189c29c1d960ef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b43394c03fd84a4380ae84b1cb1523da","placeholder":"​","style":"IPY_MODEL_ceb67f9b82b54d319e120c0d309d2885","value":"DecompX masking:  99%"}},"bde93d01ab5b4eb2b216a7953b41931e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_34e0316d7c9b4b3d86dcf7035e06d1c1","max":84,"min":0,"orientation":"horizontal","style":"IPY_MODEL_efcb58a662224f108c8c1638744e0fe7","value":84}},"59af0aa5311644deb97845669ba73a52":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_30d9e7d8a6ff4c349468d234c5b147c2","placeholder":"​","style":"IPY_MODEL_74634886ae894a5483a65ca0f7cea548","value":" 83/84 [00:06&lt;00:00, 14.83it/s]"}},"1a6df16002b54f0f9bded2f156637ce0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"b43394c03fd84a4380ae84b1cb1523da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ceb67f9b82b54d319e120c0d309d2885":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"34e0316d7c9b4b3d86dcf7035e06d1c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efcb58a662224f108c8c1638744e0fe7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"30d9e7d8a6ff4c349468d234c5b147c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74634886ae894a5483a65ca0f7cea548":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"35c5a71dc5b44c1aa3ce08c1836a32be":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_faf9af58e8ee496abc7869284d273e30","IPY_MODEL_be2f24f320ce4b81b6eea28bb479287d","IPY_MODEL_4c14692fc6b64f3db7401b350fda7698"],"layout":"IPY_MODEL_955ae2dc83ed4e36b3a6eef238ee6d5e"}},"faf9af58e8ee496abc7869284d273e30":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b47186463c4f4331abd72607498ceec4","placeholder":"​","style":"IPY_MODEL_6156ce5bc4114a5cb027106a1e6e843f","value":"Downloading shards: 100%"}},"be2f24f320ce4b81b6eea28bb479287d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f456ec232c1649f39607bd025882110e","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6bb27cf92319458293255e789a938d75","value":3}},"4c14692fc6b64f3db7401b350fda7698":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_310b5fb84f36421a8e207324e3c0faf4","placeholder":"​","style":"IPY_MODEL_f30d9f931ad84451b4a08d9631d42959","value":" 3/3 [00:02&lt;00:00,  1.40it/s]"}},"955ae2dc83ed4e36b3a6eef238ee6d5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b47186463c4f4331abd72607498ceec4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6156ce5bc4114a5cb027106a1e6e843f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f456ec232c1649f39607bd025882110e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bb27cf92319458293255e789a938d75":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"310b5fb84f36421a8e207324e3c0faf4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f30d9f931ad84451b4a08d9631d42959":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"efe6108d8690407cab868dc5a9b90141":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_addc396bf10947af8613fe51d219a50e","IPY_MODEL_a23db48efb224909b3d5a2df332c55d1","IPY_MODEL_059c133b7ee04684b53fe7cc118eb09f"],"layout":"IPY_MODEL_7dd6f07942bf490dbfce7f878bab36ee"}},"addc396bf10947af8613fe51d219a50e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91ee2ae926274595a9918a52e832468e","placeholder":"​","style":"IPY_MODEL_9a0c7eac7abe4461b2ba236e8165cb67","value":"Loading checkpoint shards: 100%"}},"a23db48efb224909b3d5a2df332c55d1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dedf18d9aafa4d92ac0c28b91b7a2fdc","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_90f3abd167cb4abeb386f7aad213a300","value":3}},"059c133b7ee04684b53fe7cc118eb09f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd97eb140cf242bb9680f1702d3544e1","placeholder":"​","style":"IPY_MODEL_0db2cf8137fe409985a72b9432385a30","value":" 3/3 [03:55&lt;00:00, 74.97s/it]"}},"7dd6f07942bf490dbfce7f878bab36ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91ee2ae926274595a9918a52e832468e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a0c7eac7abe4461b2ba236e8165cb67":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dedf18d9aafa4d92ac0c28b91b7a2fdc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90f3abd167cb4abeb386f7aad213a300":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cd97eb140cf242bb9680f1702d3544e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0db2cf8137fe409985a72b9432385a30":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ea1c62620985487f86b5eca38c5d8712":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e486adb41aaf4526831716b6b655fd7d","IPY_MODEL_466759ae0d4a411cbf5fa7eb55b5a1f0","IPY_MODEL_60fb26d0acf841f1bb76520a8be4847a"],"layout":"IPY_MODEL_905dfb8f2b734625b3aac9114e283d79"}},"e486adb41aaf4526831716b6b655fd7d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_09fa8278eafc4c28a2c0724caf12f764","placeholder":"​","style":"IPY_MODEL_725dd073f0574d51a2cd58e54426267a","value":"LLM infilling: 100%"}},"466759ae0d4a411cbf5fa7eb55b5a1f0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_5dab7203ab364959a0f1237c24c7ffb4","max":671,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d94273b08da242739a30f6012e5dc58d","value":671}},"60fb26d0acf841f1bb76520a8be4847a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_102b8e7750754cb1bd42633201e2af62","placeholder":"​","style":"IPY_MODEL_4c3b1ab8a8e9467295fec8bdedbd686e","value":" 671/671 [31:47&lt;00:00,  2.90s/it]"}},"905dfb8f2b734625b3aac9114e283d79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"09fa8278eafc4c28a2c0724caf12f764":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"725dd073f0574d51a2cd58e54426267a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5dab7203ab364959a0f1237c24c7ffb4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d94273b08da242739a30f6012e5dc58d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"102b8e7750754cb1bd42633201e2af62":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c3b1ab8a8e9467295fec8bdedbd686e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f3f9f1d51bcf4b009becaaaa03d62a13":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f4710f9eb9e54742a5a071227848048a","IPY_MODEL_8ef9e9f677964c44ba7c0213696dd774","IPY_MODEL_49127e0052dd4b62800c755801a65b78"],"layout":"IPY_MODEL_76ff9cb7208c47459f2ad8ea5d6ca642"}},"f4710f9eb9e54742a5a071227848048a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7908ea2adb4a4866a240736b0ab19fd1","placeholder":"​","style":"IPY_MODEL_5a745e7cd0574c488b00c8c459c74f2a","value":"DecompX masking: 100%"}},"8ef9e9f677964c44ba7c0213696dd774":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_397038c370fc46fc876a559dee2cbae8","max":420,"min":0,"orientation":"horizontal","style":"IPY_MODEL_12fc2ace10614a23ac3769468516997c","value":420}},"49127e0052dd4b62800c755801a65b78":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b680ab8f92846b7829a973d5374b811","placeholder":"​","style":"IPY_MODEL_f360eff002c848549ba42229210e6b09","value":" 420/420 [00:47&lt;00:00, 15.25it/s]"}},"76ff9cb7208c47459f2ad8ea5d6ca642":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"7908ea2adb4a4866a240736b0ab19fd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a745e7cd0574c488b00c8c459c74f2a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"397038c370fc46fc876a559dee2cbae8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12fc2ace10614a23ac3769468516997c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6b680ab8f92846b7829a973d5374b811":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f360eff002c848549ba42229210e6b09":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}